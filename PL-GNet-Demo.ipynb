{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:26:58.711273Z",
     "start_time": "2019-07-27T02:26:54.918034Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "#from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "from compute_mcc import *\n",
    "#import scipy.io as sio\n",
    "import math\n",
    "import h5py\n",
    "#from compute_mcc import compute_mcc,metrics,_fast_hist,label_accuracy_score\n",
    "from hilbert import hilbertCurve\n",
    "#from compute_IoU import compute_precision,bb_IoU\n",
    "# sys.path.append('.')\n",
    "import os,sys\n",
    "from scipy import signal\n",
    "import time\n",
    "import skimage\n",
    "import skimage.io, skimage.transform\n",
    "from skimage.transform import resize\n",
    "from skimage.util import view_as_windows\n",
    "import scipy.misc\n",
    "import scipy.io as sio\n",
    "from skimage import img_as_uint\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas\n",
    "import glob \n",
    "import datetime\n",
    "from skimage.color import rgb2ycbcr \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pywt\n",
    "# import cv2\n",
    "import re\n",
    "from PIL import Image\n",
    "import os\n",
    "import bisect\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fileters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:27:03.687328Z",
     "start_time": "2019-07-27T02:27:03.670281Z"
    }
   },
   "outputs": [],
   "source": [
    "Bayar_Kernel = np.array([\n",
    "    [[0,0,0,0,0],[0,(1.0/4)*-1,(1.0/4)*2,(1.0/4)*-1,0],[0,(1.0/4)*2,(1.0/4)*-4,(1.0/4)*2,0],[0,(1.0/4)*-1,(1.0/4)*2,(1.0/4)*-1,0],[0,0,0,0,0]],\n",
    "    [[(1.0/12)*-1,(1.0/12)*2,(1.0/12)*-2,(1.0/12)*2,(1.0/12)*-1],[(1.0/12)*2,(1.0/12)*-6,(1.0/12)*8,(1.0/12)*-6,(1.0/12)*2],[(1.0/12)*-2,(1.0/12)*8,(1.0/12)*-12,(1.0/12)*8,(1.0/12)*-2],[(1.0/12)*2,(1.0/12)*-6,(1.0/12)*8,(1.0/12)*-6,(1.0/12)*2],[(1.0/12)*-1,(1.0/12)*2,(1.0/12)*-2,(1.0/12)*2,(1.0/12)*-1]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,(1.0/2)*1,(1.0/2)*-2,(1.0/2)*1,0],[0,0,0,0,0],[0,0,0,0,0]]\n",
    "])\n",
    "Bayar_Kernel = np.vstack((Bayar_Kernel, Bayar_Kernel, Bayar_Kernel)).reshape(3, 3, 5, 5).transpose(2,3,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:27:05.839826Z",
     "start_time": "2019-07-27T02:27:05.826926Z"
    }
   },
   "outputs": [],
   "source": [
    "SRM_Kernel = np.array([\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,1,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,1,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,1,0,0],[0,0,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,0,0],[0,0,1,0,0],[0,0,0,0,0]],\n",
    "])\n",
    "SRM_Kernel = np.vstack((SRM_Kernel, SRM_Kernel, SRM_Kernel)).reshape(3, 4, 5, 5).transpose(2,3,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:07:11.500898Z",
     "start_time": "2019-06-10T02:07:11.310420Z"
    }
   },
   "outputs": [],
   "source": [
    "SRM_Kernel = np.array([\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,1,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,1,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,1,0,0],[0,0,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,0,0],[0,0,1,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,1,0],[0,0,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,0,0],[0,0,0,1,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,1,0,0,0],[0,0,-1,0,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,0,-1,0,0],[0,1,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,1,-2,1,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,1,0,0],[0,0,-2,0,0],[0,0,1,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,1,0,0,0],[0,0,-2,0,0],[0,0,0,1,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,1,0],[0,0,-2,0,0],[0,1,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,1,-3,3,-1],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[-1,3,-3,1,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,-1,0,0],[0,0,3,0,0],[0,0,-3,0,0],[0,0,1,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,1,0,0],[0,0,-3,0,0],[0,0,3,0,0],[0,0,-1,0,0]],\n",
    "    [[0,0,0,0,-1],[0,0,0,3,0],[0,0,-3,0,0],[0,1,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,1,0,0,0],[0,0,-3,0,0],[0,0,0,3,0],[0,0,0,0,-1]],\n",
    "    [[-1,0,0,0,0],[0,3,0,0,0],[0,0,-3,0,0],[0,0,0,1,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,1,0],[0,0,-3,0,0],[0,3,0,0,0],[-1,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,-1,2,-1,0],[0,2,-4,2,0],[0,-1,2,-1,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,2,-1,0],[0,0,-4,2,0],[0,0,2,-1,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,-1,2,0,0],[0,2,-4,0,0],[0,-1,2,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,-1,2,-1,0],[0,2,-4,2,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[0,2,-4,2,0],[0,-1,2,-1,0],[0,0,0,0,0]],\n",
    "    [[-1,2,-2,2,-1],[2,-6,8,-6,2],[-2,8,-12,8,-2],[2,-6,8,-6,2],[-1,2,-2,2,-1]],\n",
    "    [[0,0,-2,2,-1],[0,0,8,-6,2],[0,0,-12,8,-2],[0,0,8,-6,2],[0,0,-2,2,-1]],\n",
    "    [[-1,2,-2,0,0],[2,-6,8,0,0],[-2,8,-12,0,0],[2,-6,8,0,0],[-1,2,-2,0,0]],\n",
    "    [[-1,2,-2,2,-1],[2,-6,8,-6,2],[-2,8,-12,8,-2],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "    [[0,0,0,0,0],[0,0,0,0,0],[-2,8,-12,8,-2],[2,-6,8,-6,2],[-1,2,-2,2,-1]]\n",
    "])\n",
    "SRM_Kernels = np.vstack((SRM_Kernel, SRM_Kernel, SRM_Kernel)).reshape(3, 30, 5, 5).transpose(2,3,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:47:41.012255Z",
     "start_time": "2019-07-27T09:47:40.897291Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63]\n",
      "[ 0  1 14 15 16 19 20 21  3  2 13 12 17 18 23 22  4  7  8 11 30 29 24 25\n",
      "  5  6  9 10 31 28 27 26 58 57 54 53 32 35 36 37 59 56 55 52 33 34 39 38\n",
      " 60 61 50 51 46 45 40 41 63 62 49 48 47 44 43 42]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "log_device_placement = True\n",
    "# Parameters\n",
    "lr = 0.00003\n",
    "training_iters = 50000000\n",
    "batch_size = 30\n",
    "display_step = 10\n",
    "nb_nontamp_img=16960\n",
    "nb_tamp_img=68355\n",
    "nbFilter=32\n",
    "\n",
    "\n",
    "# LSTM network parameters\n",
    "n_input = 240 # data input (img shape: 64x64)\n",
    "n_steps = 64 # timesteps\n",
    "nBlock=int(math.sqrt(n_steps))\n",
    "n_hidden = 64# hidden layer num of features\n",
    "nStride=int(math.sqrt(n_hidden))\n",
    "# other parameters\n",
    "imSize=256\n",
    "# Network Parameters\n",
    "n_classes = 2 # manipulated vs unmanipulated\n",
    "mx=127.0\n",
    "\n",
    "# tf Graph input\n",
    "input_layer = tf.placeholder(\"float\", [None, imSize,imSize,3])\n",
    "y= tf.placeholder(\"float\", [2,None, imSize,imSize])\n",
    "freqFeat=tf.placeholder(\"float\", [None, 248,248,3])\n",
    "# freqFeat=tf.placeholder(\"float\", [None, 64,240])\n",
    "# freqFeat=tf.placeholder(\"float\", [None, 256,256,3])\n",
    "filter = tf.Variable(tf.random_normal([5,5,3,9]))\n",
    "ratio=15.0 #tf.placeholder(\"float\",[1])\n",
    "#out_rnn=tf.placeholder(\"float\", [None, 128,128,3])\n",
    "# W1 = tf.get_variable('W1', [5,5,3, 10], tf.float32, xavier_initializer())\n",
    "# b1 = tf.Variable(tf.random_normal([5,5,3]))\n",
    "\n",
    "############################################################################\n",
    "#total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = 2\n",
    "upsample_factor=16\n",
    "beta=.01\n",
    "outSize=16\n",
    "############################################################################\n",
    "seq = np.linspace(0,63,64).astype(int)\n",
    "order3 = hilbertCurve(3)\n",
    "order3 = np.reshape(order3,(64))\n",
    "print(seq)\n",
    "print(order3)\n",
    "hilbert_ind = np.lexsort((seq,order3))\n",
    "actual_ind=np.lexsort((seq,hilbert_ind))\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([64,64,nbFilter]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([nbFilter]))\n",
    "}\n",
    "\n",
    "atrous_fil = tf.Variable(tf.random_normal([3, 3, 256,256]),name = 'atrous_fil')\n",
    "atrous_fil_1 =tf.Variable(tf.random_normal([3, 3, 256,256]),name ='atrous_fil_1')\n",
    "atrous_fil_2 =tf.Variable(tf.random_normal([3, 3, 256,256]),name ='atrous_fil_2')\n",
    "\n",
    "\n",
    "atrous_fil1 = tf.Variable(tf.random_normal([3, 3, 256,256]),name ='atrous_fil1')\n",
    "atrous_fil2 = tf.Variable(tf.random_normal([3, 3, 256,256]),name ='atrous_fil2')\n",
    "atrous_fil3 = tf.Variable(tf.random_normal([3, 3, 256,256]),name ='atrous_fil3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T11:35:11.609630Z",
     "start_time": "2019-07-22T11:35:11.564295Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "log_device_placement = True\n",
    "lr = 0.00003\n",
    "training_iters = 50000000\n",
    "batch_size = 30\n",
    "display_step = 10\n",
    "nb_nontamp_img=16960\n",
    "nb_tamp_img=68355\n",
    "nbFilter=32\n",
    "\n",
    "\n",
    "# LSTM network parameters\n",
    "n_input = 240 # data input (img shape: 64x64)\n",
    "n_steps = 64 # timesteps\n",
    "nBlock=int(math.sqrt(n_steps))\n",
    "n_hidden = 64# hidden layer num of features\n",
    "nStride=int(math.sqrt(n_hidden))\n",
    "# other parameters\n",
    "imSize=256\n",
    "# Network Parameters\n",
    "n_classes = 2 # manipulated vs unmanipulated\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "input_layer = tf.placeholder(\"float\", [None, imSize,imSize,3])\n",
    "y= tf.placeholder(\"float\", [2,None, imSize,imSize])\n",
    "freqFeat=tf.placeholder(\"float\", [None, 64,240])\n",
    "ratio=15.0 #tf.placeholder(\"float\",[1])\n",
    "#out_rnn=tf.placeholder(\"float\", [None, 128,128,3])\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = 2\n",
    "upsample_factor=16\n",
    "n_classes=2\n",
    "beta=.01\n",
    "outSize=16\n",
    "############################################################################\n",
    "seq = np.linspace(0,63,64).astype(int)\n",
    "order3 = hilbertCurve(3)\n",
    "order3 = np.reshape(order3,(64))\n",
    "hilbert_ind = np.lexsort((seq,order3))\n",
    "actual_ind=np.lexsort((seq,hilbert_ind))\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([64,64,nbFilter]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([nbFilter]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base-model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T11:35:37.142854Z",
     "start_time": "2019-07-22T11:35:21.470928Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_out3: (64, ?, 32)\n",
      "lstm_out3: (30, 8, 8, 32)\n",
      "joint_out: (30, 16, 16, 64)\n",
      "upsampled_layer4: (30, 64, 64, 64)\n",
      "upsampled_layer4: (30, 64, 64, 2)\n",
      "upsampled_layer5: (30, 256, 256, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = slim.conv2d(input_layer,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
    "        layer1 =resUnit(layer1,1,nbFilter)\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "        layer2=slim.max_pool2d(layer1, [2, 2], scope='pool_'+str(1))\n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        layer2 = slim.conv2d(layer2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        layer2 =resUnit(layer2,2,2*nbFilter)\n",
    "        layer2 = tf.nn.relu(layer2)\n",
    "        layer3=slim.max_pool2d(layer2, [2, 2], scope='pool_'+str(2))\n",
    "        # layer3: resblock, input size(64,64) \n",
    "        layer3 = slim.conv2d(layer3,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        layer3 =resUnit(layer3,3,4*nbFilter)\n",
    "        layer3 = tf.nn.relu(layer3)\n",
    "        layer4=slim.max_pool2d(layer3, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        layer4 = slim.conv2d(layer4,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        layer4 =resUnit(layer4,4,8*nbFilter)\n",
    "        layer4 = tf.nn.relu(layer4)\n",
    "        layer4=slim.max_pool2d(layer4, [2, 2], scope='pool_'+str(4))\n",
    "        # end of layer4: resblock, input size(16,16)\n",
    "\n",
    "        # lstm network \n",
    "        patches=tf.transpose(freqFeat,[1,0,2])\n",
    "        patches=tf.gather(patches,hilbert_ind)\n",
    "        patches=tf.transpose(patches,[1,0,2])         \n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        print('lstm_out3:',lstm_out.shape)\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        print('lstm_out3:',lstm_out.shape)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer4,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        print('joint_out:',joint_out.shape)\n",
    "        \n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        print('upsampled_layer4:',upsampled_layer4.shape)\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(4))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        print('upsampled_layer4:',upsampled_layer4.shape)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        print('upsampled_layer5:',upsampled_layer5.shape)\n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG_AR_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T02:56:52.701876Z",
     "start_time": "2019-06-18T02:56:31.482370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('concat:', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(13)]))\n",
      "('Pool_1:', TensorShape([Dimension(None), Dimension(128), Dimension(128), Dimension(32)]))\n",
      "('Pool_2:', TensorShape([Dimension(None), Dimension(64), Dimension(64), Dimension(64)]))\n",
      "('Pool_3:', TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(128)]))\n",
      "('layer10:', TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(256)]))\n",
      "('layer13:', TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(256)]))\n",
      "('layer6_shape:', TensorShape([Dimension(None), Dimension(248), Dimension(248), Dimension(1)]))\n",
      "('layer7_shape:', TensorShape([Dimension(None), Dimension(1), Dimension(248), Dimension(248)]))\n",
      "('y_list_shape2:', 8)\n",
      "('xy_list_shape2:', 8)\n",
      "('xy_shape:', 64)\n",
      "('xy_shape2:', TensorShape([Dimension(None), Dimension(64), Dimension(31), Dimension(31)]))\n",
      "('patches_shape2:', TensorShape([Dimension(None), Dimension(64), Dimension(961)]))\n",
      "('lstm_out:', TensorShape([Dimension(30), Dimension(16), Dimension(16), Dimension(32)]))\n",
      "('top:', TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(32)]))\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        \n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        \n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "        Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "        Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(Tan_1,atrous_fil,rate =3,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.tanh(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =5,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.tanh(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =7,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.tanh(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         Pool_4 = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(13))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(14))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(17))\n",
    "        layer55 = tf.nn.relu(layer55)\n",
    "        layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "        layer66 = tf.nn.relu(layer66)\n",
    "        print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "        print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        y_list = tf.split(layer77,8,axis=3)\n",
    "        print('y_list_shape2:',len(y_list))\n",
    "        xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "        print('xy_list_shape2:',len(xy_list))\n",
    "        xy = [item for items in xy_list for item in items]\n",
    "        print('xy_shape:',len(xy))\n",
    "#         xy = torch.cat(xy,1)\n",
    "        xy = tf.concat(xy,1)\n",
    "        print('xy_shape2:',xy.shape)\n",
    "#         patches = xy.view(-1,64,64)#\n",
    "        patches = tf.reshape(xy,(-1,64,31*31))\n",
    "        print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "#         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "#         patches=tf.gather(patches,hilbert_ind)\n",
    "#         patches=tf.transpose(patches,[1,0,2])\n",
    "#         print('patches:',patches.shape)\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG_AR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2,layer3], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        \n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        \n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        \n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "        Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "        Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(Tan_1,atrous_fil,rate =3,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =5,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =7,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "        layer55 = tf.nn.relu(layer55)\n",
    "        layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(19))\n",
    "        layer66 = tf.nn.relu(layer66)\n",
    "        print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "        print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        y_list = tf.split(layer77,8,axis=3)\n",
    "        print('y_list_shape2:',len(y_list))\n",
    "        xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "        print('xy_list_shape2:',len(xy_list))\n",
    "        xy = [item for items in xy_list for item in items]\n",
    "        print('xy_shape:',len(xy))\n",
    "#         xy = torch.cat(xy,1)\n",
    "        xy = tf.concat(xy,1)\n",
    "        print('xy_shape2:',xy.shape)\n",
    "#         patches = xy.view(-1,64,64)#\n",
    "        patches = tf.reshape(xy,(-1,64,31*31))\n",
    "        print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "#         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "#         patches=tf.gather(patches,hilbert_ind)\n",
    "#         patches=tf.transpose(patches,[1,0,2])\n",
    "#         print('patches:',patches.shape)\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_AR_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T05:08:47.275480Z",
     "start_time": "2019-06-19T05:08:24.080397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: (?, 256, 256, 16)\n",
      "Pool_1: (?, 128, 128, 32)\n",
      "Pool_2: (?, 64, 64, 64)\n",
      "Pool_3: (?, 32, 32, 128)\n",
      "layer10: (?, 16, 16, 256)\n",
      "layer6_shape: (?, 248, 248, 1)\n",
      "layer7_shape: (?, 1, 248, 248)\n",
      "y_list_shape2: 8\n",
      "xy_list_shape2: 8\n",
      "xy_shape: 64\n",
      "xy_shape2: (?, 64, 31, 31)\n",
      "patches_shape2: (?, 64, 961)\n",
      "lstm_out: (30, 16, 16, 32)\n",
      "top: (?, 16, 16, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2,layer3], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        \n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        \n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "        Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "        Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(Tan_1,atrous_fil,rate =3,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =5,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =7,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "        output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "#         layer12 = tf.nn.relu(layer10)\n",
    "#         layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "#         print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "        layer55 = tf.nn.relu(layer55)\n",
    "        layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(19))\n",
    "        layer66 = tf.nn.relu(layer66)\n",
    "        print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "        print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        y_list = tf.split(layer77,8,axis=3)\n",
    "        print('y_list_shape2:',len(y_list))\n",
    "        xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "        print('xy_list_shape2:',len(xy_list))\n",
    "        xy = [item for items in xy_list for item in items]\n",
    "        print('xy_shape:',len(xy))\n",
    "#         xy = torch.cat(xy,1)\n",
    "        xy = tf.concat(xy,1)\n",
    "        print('xy_shape2:',xy.shape)\n",
    "#         patches = xy.view(-1,64,64)#\n",
    "        patches = tf.reshape(xy,(-1,64,31*31))\n",
    "        print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "#         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "#         patches=tf.gather(patches,hilbert_ind)\n",
    "#         patches=tf.transpose(patches,[1,0,2])\n",
    "#         print('patches:',patches.shape)\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer10,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_AR_4 _LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T14:05:27.238381Z",
     "start_time": "2019-06-20T14:05:08.998679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: (?, 256, 256, 16)\n",
      "Pool_1: (?, 128, 128, 32)\n",
      "Pool_2: (?, 64, 64, 64)\n",
      "Pool_3: (?, 32, 32, 128)\n",
      "layer10: (?, 32, 32, 256)\n",
      "layer13: (?, 16, 16, 256)\n",
      "layer6_shape: (?, 256, 256, 1)\n",
      "layer7_shape: (?, 1, 256, 256)\n",
      "y_list_shape2: 8\n",
      "xy_list_shape2: 8\n",
      "xy_shape: 64\n",
      "xy_shape2: (?, 64, 32, 32)\n",
      "patches_shape2: (?, 64, 1024)\n",
      "lstm_out: (30, 16, 16, 32)\n",
      "top: (?, 16, 16, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2,layer3], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        \n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        \n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        \n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "        Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "        Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(Tan_1,atrous_fil,rate =3,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =5,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =7,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "        layer55 = tf.nn.relu(layer55)\n",
    "        layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(19))\n",
    "        layer66 = tf.nn.relu(layer66)\n",
    "        print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "        print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        y_list = tf.split(layer77,8,axis=3)\n",
    "        print('y_list_shape2:',len(y_list))\n",
    "        xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "        print('xy_list_shape2:',len(xy_list))\n",
    "        xy = [item for items in xy_list for item in items]\n",
    "        print('xy_shape:',len(xy))\n",
    "#         xy = torch.cat(xy,1)\n",
    "        xy = tf.concat(xy,1)\n",
    "        print('xy_shape2:',xy.shape)\n",
    "#         patches = xy.view(-1,64,64)#\n",
    "        patches = tf.reshape(xy,(-1,64,32*32))\n",
    "        print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "#         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "#         patches=tf.gather(patches,hilbert_ind)\n",
    "#         patches=tf.transpose(patches,[1,0,2])\n",
    "#         print('patches:',patches.shape)\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PL-GNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:48:02.511969Z",
     "start_time": "2019-07-27T09:47:46.805610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: (?, 256, 256, 16)\n",
      "Pool_1: (?, 128, 128, 32)\n",
      "Pool_2: (?, 64, 64, 64)\n",
      "Pool_3: (?, 32, 32, 128)\n",
      "ReLu_9: (?, 32, 32, 256)\n",
      "layer10: (?, 32, 32, 256)\n",
      "layer13: (?, 16, 16, 256)\n",
      "layer6_shape: (?, 248, 248, 1)\n",
      "layer7_shape: (?, 1, 248, 248)\n",
      "y_list_shape2: 8\n",
      "xy_list_shape2: 8\n",
      "xy_shape: 64\n",
      "xy_shape2: (?, 64, 31, 31)\n",
      "patches_shape2: (?, 64, 961)\n",
      "lstm_out1: (64, ?, 32)\n",
      "lstm_out2: (?, 64, 32)\n",
      "lstm_out3: (30, 8, 8, 32)\n",
      "lstm_out4: (30, 16, 16, 32)\n",
      "top: (?, 16, 16, 32)\n",
      "joint_out: (30, 16, 16, 64)\n",
      "upsampled_logits_shape: (30, 256, 256, 2)\n",
      "flat_pred_shape: (1966080, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2,layer3], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        \n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        \n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        print('ReLu_9:',ReLu_9.shape)\n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "#         Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "#         Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(ReLu_9,atrous_fil,rate =2,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =2,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =2,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "        layer55 = tf.nn.relu(layer55)\n",
    "        layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(19))\n",
    "        layer66 = tf.nn.relu(layer66)\n",
    "        print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "        print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "        y_list = tf.split(layer77,8,axis=3)\n",
    "        print('y_list_shape2:',len(y_list))\n",
    "        xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "        print('xy_list_shape2:',len(xy_list))\n",
    "        xy = [item for items in xy_list for item in items]\n",
    "        print('xy_shape:',len(xy))\n",
    "#         xy = torch.cat(xy,1)\n",
    "        xy = tf.concat(xy,1)\n",
    "        print('xy_shape2:',xy.shape)\n",
    "#         patches = xy.view(-1,64,64)#\n",
    "        patches = tf.reshape(xy,(-1,64,31*31))\n",
    "        print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "#         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "#         patches=tf.gather(patches,hilbert_ind)\n",
    "#         patches=tf.transpose(patches,[1,0,2])\n",
    "#         print('patches:',patches.shape)\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        print('lstm_out1:',lstm_out.shape)\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        print('lstm_out2:',lstm_out.shape)\n",
    "        \n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        print('lstm_out3:',lstm_out.shape)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out4:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        print('joint_out:',joint_out.shape)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "    print('upsampled_logits_shape:',upsampled_logits.shape)\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    print('flat_pred_shape:',flat_pred.shape)\n",
    "    \n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGAR_no_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T07:04:52.247601Z",
     "start_time": "2019-07-25T07:04:50.389675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: (?, 256, 256, 16)\n",
      "Pool_1: (?, 128, 128, 32)\n",
      "Pool_2: (?, 64, 64, 64)\n",
      "Pool_3: (?, 32, 32, 128)\n",
      "ReLu_9: (?, 32, 32, 256)\n",
      "layer10: (?, 32, 32, 256)\n",
      "layer13: (?, 16, 16, 256)\n",
      "top: (?, 16, 16, 64)\n",
      "upsampled_logits_shape: (30, 256, 256, 2)\n",
      "flat_pred_shape: (1966080, 2)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2,layer3], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        \n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        \n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        print('ReLu_9:',ReLu_9.shape)\n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "#         Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "#         Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(ReLu_9,atrous_fil,rate =2,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =2,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =2,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "#         # lstm network \n",
    "#         layer55 = slim.conv2d(freqFeat,16,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(18))\n",
    "#         layer55 = tf.nn.relu(layer55)\n",
    "#         layer66 = slim.conv2d(layer55,1,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(19))\n",
    "#         layer66 = tf.nn.relu(layer66)\n",
    "#         print('layer6_shape:',layer66.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "#         layer77=tf.transpose(layer66,[0,3,1,2])\n",
    "#         print('layer7_shape:',layer77.shape)  # ( bs, 256, 256, 1)\n",
    "        \n",
    "#         y_list = tf.split(layer77,8,axis=3)\n",
    "#         print('y_list_shape2:',len(y_list))\n",
    "#         xy_list = [tf.split(x,8,axis =2) for x in y_list] ##\n",
    "#         print('xy_list_shape2:',len(xy_list))\n",
    "#         xy = [item for items in xy_list for item in items]\n",
    "#         print('xy_shape:',len(xy))\n",
    "# #         xy = torch.cat(xy,1)\n",
    "#         xy = tf.concat(xy,1)\n",
    "#         print('xy_shape2:',xy.shape)\n",
    "# #         patches = xy.view(-1,64,64)#\n",
    "#         patches = tf.reshape(xy,(-1,64,31*31))\n",
    "#         print('patches_shape2:',patches.shape)\n",
    "        \n",
    "        \n",
    "# #         patches=tf.transpose(freqFeat,[1,0,2])\n",
    "# #         patches=tf.gather(patches,hilbert_ind)\n",
    "# #         patches=tf.transpose(patches,[1,0,2])\n",
    "# #         print('patches:',patches.shape)\n",
    "#         # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "#         xCell=tf.unstack(patches, n_steps, 1)\n",
    "#         # 2 stacked layers\n",
    "#         stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "#         out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "#         # organizing the lstm output\n",
    "#         out=tf.gather(out,actual_ind)\n",
    "#         # convert to lstm output (64,batchSize,nbFilter)\n",
    "#         lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "#         print('lstm_out1:',lstm_out.shape)\n",
    "#         lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "#         print('lstm_out2:',lstm_out.shape)\n",
    "        \n",
    "#         # convert to size(batchSize, 8,8, nbFilter)\n",
    "#         lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "#         # perform batch normalization and activiation\n",
    "#         lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "#         lstm_out=tf.nn.relu(lstm_out)\n",
    "#         print('lstm_out3:',lstm_out.shape)\n",
    "#         # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "#         temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "#         uShape1=tf.shape(temp)\n",
    "#         upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "#         upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "#         lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "#         print('lstm_out4:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,2*nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "#         joint_out=tf.concat([top,lstm_out],3)\n",
    "#         print('joint_out:',joint_out.shape)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(top, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "    print('upsampled_logits_shape:',upsampled_logits.shape)\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    print('flat_pred_shape:',flat_pred.shape)\n",
    "    \n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_AR+original-feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T13:29:43.529434Z",
     "start_time": "2019-06-23T13:29:16.352405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: (?, 256, 256, 12)\n",
      "Pool_1: (?, 128, 128, 32)\n",
      "Pool_2: (?, 64, 64, 64)\n",
      "Pool_3: (?, 32, 32, 128)\n",
      "layer10: (?, 32, 32, 256)\n",
      "layer13: (?, 16, 16, 256)\n",
      "lstm_out: (30, 16, 16, 32)\n",
      "top: (?, 16, 16, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "\n",
    "    def conv_mask_gt(z): \n",
    "        # Get ones for each class instead of a number -- we need that\n",
    "        # for cross-entropy loss later on. Sometimes the groundtruth\n",
    "        # masks have values other than 1 and 0. \n",
    "#         class_labels_tensor = (z==1)\n",
    "#         background_labels_tensor = (z==0)\n",
    "        \n",
    "        class_labels_tensor = (z==1)\n",
    "        background_labels_tensor = (z==0)\n",
    "        # Convert the boolean values into floats -- so that\n",
    "        # computations in cross-entropy loss is correct\n",
    "        bit_mask_class = np.float32(class_labels_tensor)\n",
    "        bit_mask_background = np.float32(background_labels_tensor)\n",
    "        combined_mask=[]\n",
    "        combined_mask.append(bit_mask_background)\n",
    "        combined_mask.append(bit_mask_class)\n",
    "        #combined_mask = tf.concat(concat_dim=3, values=[bit_mask_background,bit_mask_class])\t\t\n",
    "\n",
    "        # Lets reshape our input so that it becomes suitable for \n",
    "        # tf.softmax_cross_entropy_with_logits with [batch_size, num_classes]\n",
    "        #flat_labels = tf.reshape(tensor=combined_mask, shape=(-1, 2))\t\n",
    "        return combined_mask#flat_labels\n",
    "\n",
    "    def get_kernel_size(factor):\n",
    "        #Find the kernel size given the desired factor of upsampling.\n",
    "        return 2 * factor - factor % 2\n",
    "\n",
    "    def upsample_filt(size):\n",
    "        \"\"\"\n",
    "        Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "        \"\"\"\n",
    "        factor = (size + 1) // 2\n",
    "        if size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:size, :size]\n",
    "        return (1 - abs(og[0] - center) / factor) * \\\n",
    "            (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "    def bilinear_upsample_weights(factor, number_of_classes):\n",
    "        \"\"\"\n",
    "        Create weights matrix for transposed convolution with bilinear filter\n",
    "        initialization.\n",
    "        \"\"\"    \n",
    "        filter_size = get_kernel_size(factor)\n",
    "\n",
    "        weights = np.zeros((filter_size,filter_size,number_of_classes,number_of_classes), dtype=np.float32)    \n",
    "        upsample_kernel = upsample_filt(filter_size)    \n",
    "        for i in range(number_of_classes):        \n",
    "            weights[:, :, i, i] = upsample_kernel    \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def resUnit(input_layer,i,nbF):\n",
    "        with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        #input_layer=tf.reshape(input_layer,[-1,64,64,3])\n",
    "            part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "            part2 = tf.nn.relu(part1)\n",
    "            part3 = slim.conv2d(part2,nbF,[3,3],activation_fn=None)\n",
    "            part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "            part5 = tf.nn.relu(part4)\n",
    "            part6 = slim.conv2d(part5,nbF,[3,3],activation_fn=None)\t\n",
    "            output = input_layer + part6\n",
    "            return output\n",
    "\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    def segNet(input_layer,bSize,freqFeat,weights,biases):\n",
    "        \n",
    "        # layer1: resblock, input size(256,256)\n",
    "        layer1 = tf.nn.conv2d(input_layer, Bayar_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out' )\n",
    "        layer2 = tf.nn.conv2d(input_layer, filter, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "#         layer3 = tf.nn.conv2d(input_layer, SRM_Kernel, strides=[1,1,1,1],padding ='SAME',name = 'SRM_out1' )\n",
    "        concat = tf.concat([layer1, layer2], axis=3, name='concat')\n",
    "        print('concat:',concat.shape)\n",
    "        \n",
    "        Conv_1 = slim.conv2d(concat,nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(1))\n",
    "        ReLu_1 = tf.nn.relu(Conv_1)\n",
    "        Pool_1 = slim.max_pool2d(ReLu_1, [2, 2], scope='pool_'+str(1))\n",
    "        print('Pool_1:',Pool_1.shape)\n",
    "        \n",
    "        # layer2: resblock, input size(128,128)   \n",
    "        Conv_2 = slim.conv2d(Pool_1,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(2))\n",
    "        ReLu_2 = tf.nn.relu(Conv_2)\n",
    "        Conv_3 = slim.conv2d(ReLu_2,2*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(3))\n",
    "        ReLu_3 = tf.nn.relu(Conv_3)\n",
    "        Pool_2 = slim.max_pool2d(ReLu_3, [2, 2], scope='pool_'+str(2))\n",
    "        print('Pool_2:',Pool_2.shape)\n",
    "        \n",
    "        # layer3: resblock, input size(64,64) \n",
    "        Conv_4 = slim.conv2d(Pool_2,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(4))\n",
    "        ReLu_4 = tf.nn.relu(Conv_4)\n",
    "        Conv_5 = slim.conv2d(ReLu_4,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(5))\n",
    "        ReLu_5 = tf.nn.relu(Conv_5)\n",
    "        Conv_6 = slim.conv2d(ReLu_5,4*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(6))\n",
    "        ReLu_6 = tf.nn.relu(Conv_6)\n",
    "        Pool_3 = slim.max_pool2d(ReLu_6, [2, 2], scope='pool_'+str(3))\n",
    "        # layer4: resblock, input size(32,32) \n",
    "        print('Pool_3:',Pool_3.shape)\n",
    "        \n",
    "        Conv_7 = slim.conv2d(Pool_3,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(7))\n",
    "        ReLu_7= tf.nn.relu(Conv_7)\n",
    "        Conv_8 = slim.conv2d(ReLu_7,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(8))\n",
    "        ReLu_8 = tf.nn.relu(Conv_8)\n",
    "        Conv_9 = slim.conv2d(ReLu_8,8*nbFilter,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(9))\n",
    "        ReLu_9 = tf.nn.relu(Conv_9)\n",
    "        \n",
    "        # layer4: resblock, input size(32,32) \n",
    "#         print('Pool_4:',Pool_4.shape)\n",
    "        \n",
    "#         Conv_10_AC = slim.conv2d(ReLu_9,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(10))\n",
    "#         Tan_1 = tf.nn.relu(Conv_10_AC)\n",
    "        \n",
    "        Conv_11_AC = tf.nn.atrous_conv2d(ReLu_9,atrous_fil,rate =2,padding = 'SAME',name = 'conv_'+str(11))\n",
    "        BN_2 = slim.batch_norm(Conv_11_AC,activation_fn=None)\n",
    "        Tan_2 = tf.nn.relu(BN_2)\n",
    "        Conv_12_AC = tf.nn.atrous_conv2d(Tan_2,atrous_fil_1,rate =2,padding = 'SAME',name = 'conv_'+str(12))\n",
    "        BN_3 = slim.batch_norm(Conv_12_AC,activation_fn=None)\n",
    "        Tan_3 = tf.nn.relu(BN_3)\n",
    "        Conv_13_AC = tf.nn.atrous_conv2d(Tan_3,atrous_fil_2,rate =2,padding = 'SAME',name = 'conv_'+str(13))\n",
    "        BN_4 = slim.batch_norm(Conv_13_AC,activation_fn=None)\n",
    "        Tan_4 = tf.nn.relu(BN_4)\n",
    "        output = ReLu_9 + Tan_4\n",
    "#         output = slim.max_pool2d(output, [2, 2], scope='pool_'+str(4))\n",
    "        \n",
    "#         layer55 = tf.nn.relu(output)\n",
    "       \n",
    "        layer6 = slim.conv2d(output,8*nbFilter,[1,1],normalizer_fn=slim.batch_norm,scope='conv_'+str(14))\n",
    "        layer6 = tf.nn.relu(layer6)\n",
    "       \n",
    "                                      \n",
    "        layer7 = tf.nn.atrous_conv2d(output,atrous_fil1,rate = 3,padding = 'SAME',name='conv_'+str(15))\n",
    "        layer7 = tf.nn.relu(layer7)\n",
    "        \n",
    "                                      \n",
    "        layer8 = tf.nn.atrous_conv2d(output,atrous_fil2,rate = 5,padding = 'SAME',name='conv_'+str(16))\n",
    "        layer8 = tf.nn.relu(layer8)\n",
    "        \n",
    "                                      \n",
    "        layer9 = tf.nn.atrous_conv2d(output,atrous_fil3,rate = 7,padding = 'SAME',name='conv_'+str(17))\n",
    "        layer9 = tf.nn.relu(layer9)\n",
    "       \n",
    "        \n",
    "        layer10 = layer6+layer7+layer8+layer9\n",
    "        print('layer10:',layer10.shape)\n",
    "        \n",
    "        layer12 = tf.nn.relu(layer10)\n",
    "        layer13 = slim.max_pool2d(layer12, [2, 2], scope='pool_'+str(4))\n",
    "        print('layer13:',layer13.shape)\n",
    "     \n",
    "        # lstm network \n",
    "        \n",
    "         # lstm network \n",
    "        patches=tf.transpose(freqFeat,[1,0,2])\n",
    "        patches=tf.gather(patches,hilbert_ind)\n",
    "        patches=tf.transpose(patches,[1,0,2])         \n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        xCell=tf.unstack(patches, n_steps, 1)\n",
    "        # 2 stacked layers\n",
    "        stacked_lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(rnn.BasicLSTMCell(n_hidden),output_keep_prob=0.9) for _ in range(2)] )\n",
    "        out, state = rnn.static_rnn(stacked_lstm_cell, xCell, dtype=tf.float32)\n",
    "        # organizing the lstm output\n",
    "        out=tf.gather(out,actual_ind)\n",
    "        # convert to lstm output (64,batchSize,nbFilter)\n",
    "        lstm_out=tf.matmul(out,weights['out'])+biases['out']\n",
    "        lstm_out=tf.transpose(lstm_out,[1,0,2])\n",
    "        # convert to size(batchSize, 8,8, nbFilter)\n",
    "        lstm_out=tf.reshape(lstm_out,[bSize,8,8,nbFilter])\n",
    "        # perform batch normalization and activiation\n",
    "        lstm_out=slim.batch_norm(lstm_out,activation_fn=None)\n",
    "        lstm_out=tf.nn.relu(lstm_out)\n",
    "        # upsample lstm output to (batchSize, 16,16, nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize,outSize,nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(2, nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        lstm_out = tf.nn.conv2d_transpose(lstm_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 2, 2, 1])\n",
    "        print('lstm_out:',lstm_out.shape)\n",
    "        # reduce the filter size to nbFilter for layer4\n",
    "        top = slim.conv2d(layer13,nbFilter,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_top')\n",
    "        top = tf.nn.relu(top)\n",
    "        print('top:',top.shape)\n",
    "        # concatenate both lstm features and image features\n",
    "        joint_out=tf.concat([top,lstm_out],3)\n",
    "        # perform upsampling (batchSize, 64,64, 2*nbFilter)\n",
    "        temp=tf.random_normal([bSize,outSize*4,outSize*4,2*nbFilter])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4, 2*nbFilter)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer4 = tf.nn.conv2d_transpose(joint_out, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \t\n",
    "        # reduce filter sizes\t\n",
    "        upsampled_layer4 = slim.conv2d(upsampled_layer4,2,[1,1], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(12))\n",
    "        upsampled_layer4=slim.batch_norm(upsampled_layer4,activation_fn=None)\n",
    "        upsampled_layer4=tf.nn.relu(upsampled_layer4)\n",
    "        # upsampling to (batchSize, 256,256, nbClasses)\n",
    "        temp=tf.random_normal([bSize,outSize*16,outSize*16,2])\n",
    "        uShape1=tf.shape(temp)\n",
    "        upsample_filter_np = bilinear_upsample_weights(4,2)\n",
    "        upsample_filter_tensor = tf.constant(upsample_filter_np)\n",
    "        upsampled_layer5 = tf.nn.conv2d_transpose(upsampled_layer4, upsample_filter_tensor,output_shape=uShape1,strides=[1, 4, 4, 1]) \n",
    "        #upsampled_layer5=slim.batch_norm(upsampled_layer5,activation_fn=None)\n",
    "        #upsampled_layer5 = slim.conv2d(upsampled_layer5,2,[3,3], normalizer_fn=slim.batch_norm, activation_fn=None, scope='conv_'+str(5))\n",
    "        #upsampled_layer5=tf.nn.relu(upsampled_layer5)\n",
    "\n",
    "\n",
    "        return upsampled_layer5\n",
    "\n",
    "\n",
    "    y1=tf.transpose(y,[1,2,3,0])\n",
    "    upsampled_logits=segNet(input_layer,batch_size,freqFeat,weights,biases)\n",
    "\n",
    "\n",
    "    flat_pred=tf.reshape(upsampled_logits,(-1,n_classes))\n",
    "    flat_y=tf.reshape(y1,(-1,n_classes))\n",
    "\n",
    "    #loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_pred,labels=flat_y))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(flat_y,flat_pred, 1.0))\n",
    "\n",
    "    #all_weights  = tf.trainable_variables()\n",
    "    #regLoss = tf.add_n([ tf.nn.l2_loss(v) for v in all_weights ]) * beta\n",
    "    #loss = 0.75*loss1+loss2\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    update = trainer.minimize(loss)\n",
    "    #update2 = trainer.minimize(loss2)\n",
    "\n",
    "    probabilities=tf.nn.softmax(flat_pred)\n",
    "    correct_pred=tf.equal(tf.argmax(probabilities,1),tf.argmax(flat_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    y_actual=tf.argmax(flat_y,1)\n",
    "    y_pred=tf.argmax(flat_pred,1)\n",
    "\n",
    "    mask_actual= tf.argmax(y1,3)\n",
    "    mask_pred=tf.argmax(upsampled_logits,3)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.allow_soft_placement=True\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:00:16.526055Z",
     "start_time": "2019-06-15T14:00:16.514184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "a = np.arange(12).reshape(2,3,2)\n",
    "b = np.arange(24).reshape(2,3,4)\n",
    "\n",
    "c = concatenate((a,b),axis=2)\n",
    "\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T05:06:56.454775Z",
     "start_time": "2019-06-19T04:50:47.105526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img.....OK\n",
      "Lab....OK\n",
      "freq1...OK\n",
      "Img_NC16_img.shape= (798, 256, 256, 3)\n",
      "Img_NC16.shape= (758, 256, 256, 3)\n",
      "freq4.shape= (266, 248, 248, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading Synthesized data\n",
    "# freq1 = np.load('../dataset_npy/synthesized_imgs/Synthesized_imgS_feat.npy')\n",
    "\n",
    "Img = np.load('../dataset_npy/synthesized_imgs/Synthesized_imgS.npy')\n",
    "print('Img.....OK')\n",
    "Lab = np.load('../dataset_npy/synthesized_imgs/Synthesized_labelS.npy')\n",
    "print('Lab....OK')\n",
    "freq1 = np.load('../dataset_npy/synthesized_imgs/Synthesized_imgS_cohere_feat.npy')\n",
    "print('freq1...OK')\n",
    "\n",
    "# loading IEEE IFS data\n",
    "\n",
    "#######----train\n",
    "# freq2 = np.load('../dataset_npy/IFS/train/IFS_train_img_feat.npy')\n",
    "# print ('freq2.shape=',np.shape(freq2))\n",
    "# nI = np.load('../dataset_npy/IFS/train/IFS_train_img.npy')\n",
    "# print ('nI.shape=',np.shape(nI))\n",
    "# nL = np.load('../dataset_npy/IFS/train/IFS_train_label.npy')\n",
    "# print ('nL.shape=',np.shape(nL))\n",
    "\n",
    "# # validation set\n",
    "# dx=nI[-batch_size:]\n",
    "# #dx=np.float32(dx)\n",
    "# dx1=freq2[-batch_size:]\n",
    "# dy=nL[-batch_size:]\n",
    "# # separate out the training set from val set\n",
    "# nI=nI[:-batch_size]\n",
    "# nL=nL[:-batch_size]\n",
    "# freq2 = freq2[:-batch_size]\n",
    "# print ('nI.shape=',np.shape(nI))\n",
    "# print ('dx.shape=',np.shape(dx)) \n",
    "\n",
    "#####------test\n",
    "# freq3 = np.load('../dataset_npy/IFS/test/IFS_test_img_feat.npy')\n",
    "# print ('freq3.shape=',np.shape(freq3))\n",
    "# nc17_img = np.load('../dataset_npy/IFS/test/IFS_test_img.npy')\n",
    "# nb_nc17_img=np.shape(nc17_img)[0]\n",
    "# print ('nc17_img.shape=',np.shape(nc17_img))\n",
    "# nc17_lab = np.load('../dataset_npy/IFS/test/IFS_test_label.npy')\n",
    "# print ('nc17_lab.shape=',np.shape(nc17_lab))\n",
    "\n",
    "\n",
    "# loading NC_16 data\n",
    "\n",
    "Img_NC16 = np.load('../dataset_npy/NC_16/train/NC16_train_img.npy')\n",
    "Img_NC16_img=np.shape(Img_NC16)\n",
    "print ('Img_NC16_img.shape=',Img_NC16_img)\n",
    "Lab_NC16 = np.load('../dataset_npy/NC_16/train/NC16_train_label.npy')\n",
    "# feat_nc16 = np.load('../dataset_npy/NC_16/train/NC16_train_img_feat.npy')\n",
    "feat_nc16 = np.load('../dataset_npy/NC_16/train/NC16_train_img_cohere_feat.npy')\n",
    "\n",
    "vx=Img_NC16[-40:]\n",
    "vx1=feat_nc16[-40:]\n",
    "vy=Lab_NC16[-40:]\n",
    "\n",
    "\n",
    "Img_NC16 = Img_NC16[:-40]\n",
    "Lab_NC16 = Lab_NC16[:-40]\n",
    "feat_nc16 = feat_nc16[:-40]\n",
    "print('Img_NC16.shape=',Img_NC16.shape)\n",
    "\n",
    "tx = np.load('../dataset_npy/NC_16/test/NC16_test_img.npy')\n",
    "ty = np.load('../dataset_npy/NC_16/test/NC16_test_label.npy')\n",
    "# freq4 = np.load('../dataset_npy/NC_16/test/NC16_test_img_feat.npy')\n",
    "freq4 = np.load('../dataset_npy/NC_16/test/NC16_test_img_cohere_feat.npy')\n",
    "print('freq4.shape=',freq4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T07:20:54.248225Z",
     "start_time": "2019-07-25T07:04:58.956227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img.....OK\n",
      "Lab....OK\n",
      "freq1...OK\n",
      "img_shape: (67179, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "Img = np.load('../dataset_npy/synthesized_imgs/Synthesized_imgS.npy')\n",
    "print('Img.....OK')\n",
    "Lab = np.load('../dataset_npy/synthesized_imgs/Synthesized_labelS.npy')\n",
    "print('Lab....OK')\n",
    "freq1 = np.load('../dataset_npy/synthesized_imgs/Synthesized_imgS_cohere_feat.npy')\n",
    "print('freq1...OK')\n",
    "print('img_shape:',Img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:27:42.841765Z",
     "start_time": "2019-07-27T02:27:42.835306Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def next_batch(size, batch_size, batch_ratio=1.0):\n",
    "    import random\n",
    "    rand = list(range(size))\n",
    "    while True:\n",
    "        random.shuffle(rand)\n",
    "        for i in range(int(batch_ratio * size // batch_size)):\n",
    "            yield rand[i*batch_size:(i+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:27:45.366737Z",
     "start_time": "2019-07-27T02:27:45.353800Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_pos_neg(y_actual, y_hat):\n",
    "    TP = 0; FP = 0;TN = 0; FN = 0\n",
    "#     print('y_hat_shape',y_hat.shape)\n",
    "#     print('y_actual_shape',y_actual.shape)\n",
    "#     print('y_hat',y_hat)\n",
    "#     print('y_actual',y_actual)\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return TP,FP,TN,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T02:27:48.982260Z",
     "start_time": "2019-07-27T02:27:48.968498Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def metrics(TP,FP,TN,FN):\n",
    "    a=TP+FP\n",
    "    b=TP+FN\n",
    "    c=TN+FP\n",
    "    d=TN+FN\n",
    "    mcc=((TP*TN)-(FP*FN))/(math.sqrt(float(a*b*c*d)+0.0001))\n",
    "    F1=(2*TP)/float(2*TP+FP+FN+.0000001)\n",
    "    precision=TP/float(TP+FP+.0000001)\n",
    "    #recall=TP/float(TP+FN+.0000001)\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T22:55:02.572293Z",
     "start_time": "2019-05-17T14:08:35.536893Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_epoch=', 23.0)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 22:25:27 | Step:  4198(  1/ 23) LR:0.000030 TLoss:0.701346 VLoss:0.743232 VAcc:0.545825\n",
      "('learn_step=', 4198)\n",
      "('TP =', 610135)\n",
      "('FP = ', 8702156)\n",
      "('TN = ', 6904691)\n",
      "('FN = ', 560234)\n",
      "('prec = ', 0.065519322796076)\n",
      "('test_accuracy = ', 0.4479186534881592)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 22:42:34 | Step:  8396(  2/ 23) LR:0.000030 TLoss:0.691650 VLoss:0.736005 VAcc:0.552769\n",
      "('learn_step=', 8396)\n",
      "('TP =', 618166)\n",
      "('FP = ', 8292456)\n",
      "('TN = ', 7314391)\n",
      "('FN = ', 552203)\n",
      "('prec = ', 0.06937405716458324)\n",
      "('test_accuracy = ', 0.47281742095947266)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 22:59:29 | Step: 12594(  3/ 23) LR:0.000030 TLoss:0.690206 VLoss:0.732762 VAcc:0.557689\n",
      "('learn_step=', 12594)\n",
      "('TP =', 602830)\n",
      "('FP = ', 8391745)\n",
      "('TN = ', 7215102)\n",
      "('FN = ', 567539)\n",
      "('prec = ', 0.06702151018808485)\n",
      "('test_accuracy = ', 0.46598565578460693)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 23:17:42 | Step: 16792(  4/ 23) LR:0.000030 TLoss:0.688642 VLoss:0.733762 VAcc:0.530961\n",
      "('learn_step=', 16792)\n",
      "('TP =', 549201)\n",
      "('FP = ', 8184205)\n",
      "('TN = ', 7422642)\n",
      "('FN = ', 621168)\n",
      "('prec = ', 0.06288508744469153)\n",
      "('test_accuracy = ', 0.4751591682434082)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 23:38:17 | Step: 20990(  5/ 23) LR:0.000030 TLoss:0.687022 VLoss:0.748108 VAcc:0.570225\n",
      "('learn_step=', 20990)\n",
      "('TP =', 459180)\n",
      "('FP = ', 6983475)\n",
      "('TN = ', 8623372)\n",
      "('FN = ', 711189)\n",
      "('prec = ', 0.06169572551730449)\n",
      "('test_accuracy = ', 0.5413627624511719)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-17 23:59:43 | Step: 25188(  6/ 23) LR:0.000030 TLoss:0.685428 VLoss:0.745175 VAcc:0.558443\n",
      "('learn_step=', 25188)\n",
      "('TP =', 460415)\n",
      "('FP = ', 7133033)\n",
      "('TN = ', 8473814)\n",
      "('FN = ', 709954)\n",
      "('prec = ', 0.06063319324765165)\n",
      "('test_accuracy = ', 0.5325220227241516)\n",
      "WARNING:tensorflow:From /home/shizenan/anaconda3/envs/forgery_localization_HLED1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 00:22:00 | Step: 29386(  7/ 23) LR:0.000030 TLoss:0.683714 VLoss:0.738424 VAcc:0.544298\n",
      "('learn_step=', 29386)\n",
      "('TP =', 549967)\n",
      "('FP = ', 7797794)\n",
      "('TN = ', 7809053)\n",
      "('FN = ', 620402)\n",
      "('prec = ', 0.06588197721520699)\n",
      "('test_accuracy = ', 0.4982372522354126)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 00:44:35 | Step: 33584(  8/ 23) LR:0.000030 TLoss:0.682046 VLoss:0.762663 VAcc:0.561167\n",
      "('learn_step=', 33584)\n",
      "('TP =', 439460)\n",
      "('FP = ', 6738669)\n",
      "('TN = ', 8868178)\n",
      "('FN = ', 730909)\n",
      "('prec = ', 0.061222081687302345)\n",
      "('test_accuracy = ', 0.5547787547111511)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 01:07:20 | Step: 37782(  9/ 23) LR:0.000030 TLoss:0.680114 VLoss:0.753467 VAcc:0.539544\n",
      "('learn_step=', 37782)\n",
      "('TP =', 504640)\n",
      "('FP = ', 7074437)\n",
      "('TN = ', 8532410)\n",
      "('FN = ', 665729)\n",
      "('prec = ', 0.0665833055924875)\n",
      "('test_accuracy = ', 0.538650393486023)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 01:30:06 | Step: 41980( 10/ 23) LR:0.000030 TLoss:0.678104 VLoss:0.755282 VAcc:0.559403\n",
      "('learn_step=', 41980)\n",
      "('TP =', 505919)\n",
      "('FP = ', 6983421)\n",
      "('TN = ', 8623426)\n",
      "('FN = ', 664450)\n",
      "('prec = ', 0.06755188040601619)\n",
      "('test_accuracy = ', 0.5441527962684631)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 01:52:54 | Step: 46178( 11/ 23) LR:0.000030 TLoss:0.675537 VLoss:0.769251 VAcc:0.578586\n",
      "('learn_step=', 46178)\n",
      "('TP =', 515590)\n",
      "('FP = ', 6907605)\n",
      "('TN = ', 8699242)\n",
      "('FN = ', 654779)\n",
      "('prec = ', 0.06945661537922594)\n",
      "('test_accuracy = ', 0.5492468476295471)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 02:16:21 | Step: 50376( 12/ 23) LR:0.000030 TLoss:0.671688 VLoss:0.771617 VAcc:0.545954\n",
      "('learn_step=', 50376)\n",
      "('TP =', 536540)\n",
      "('FP = ', 7139907)\n",
      "('TN = ', 8466940)\n",
      "('FN = ', 633829)\n",
      "('prec = ', 0.06989431438789234)\n",
      "('test_accuracy = ', 0.5366494655609131)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 02:39:16 | Step: 54574( 13/ 23) LR:0.000030 TLoss:0.667106 VLoss:0.764844 VAcc:0.500414\n",
      "('learn_step=', 54574)\n",
      "('TP =', 545990)\n",
      "('FP = ', 7919701)\n",
      "('TN = ', 7687146)\n",
      "('FN = ', 624379)\n",
      "('prec = ', 0.06449443996951855)\n",
      "('test_accuracy = ', 0.49073326587677)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 03:02:33 | Step: 58772( 14/ 23) LR:0.000030 TLoss:0.661610 VLoss:0.771277 VAcc:0.510539\n",
      "('learn_step=', 58772)\n",
      "('TP =', 561380)\n",
      "('FP = ', 8102829)\n",
      "('TN = ', 7504018)\n",
      "('FN = ', 608989)\n",
      "('prec = ', 0.06479298918112357)\n",
      "('test_accuracy = ', 0.48073530197143555)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 03:25:35 | Step: 62970( 15/ 23) LR:0.000030 TLoss:0.655538 VLoss:0.764635 VAcc:0.445779\n",
      "('learn_step=', 62970)\n",
      "('TP =', 633961)\n",
      "('FP = ', 8885474)\n",
      "('TN = ', 6721373)\n",
      "('FN = ', 536408)\n",
      "('prec = ', 0.06659649443480556)\n",
      "('test_accuracy = ', 0.43841302394866943)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 03:48:40 | Step: 67168( 16/ 23) LR:0.000030 TLoss:0.653104 VLoss:0.762368 VAcc:0.418214\n",
      "('learn_step=', 67168)\n",
      "('TP =', 673334)\n",
      "('FP = ', 9090280)\n",
      "('TN = ', 6516567)\n",
      "('FN = ', 497035)\n",
      "('prec = ', 0.06896360302650156)\n",
      "('test_accuracy = ', 0.4285517930984497)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 04:12:02 | Step: 71366( 17/ 23) LR:0.000030 TLoss:0.651585 VLoss:0.769475 VAcc:0.419911\n",
      "('learn_step=', 71366)\n",
      "('TP =', 720601)\n",
      "('FP = ', 9319957)\n",
      "('TN = ', 6286890)\n",
      "('FN = ', 449768)\n",
      "('prec = ', 0.07176901921187974)\n",
      "('test_accuracy = ', 0.4176793098449707)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 04:35:13 | Step: 75564( 18/ 23) LR:0.000030 TLoss:0.650242 VLoss:0.783950 VAcc:0.401752\n",
      "('learn_step=', 75564)\n",
      "('TP =', 727037)\n",
      "('FP = ', 9468411)\n",
      "('TN = ', 6138436)\n",
      "('FN = ', 443332)\n",
      "('prec = ', 0.07130996107282317)\n",
      "('test_accuracy = ', 0.4092141389846802)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 04:58:41 | Step: 79762( 19/ 23) LR:0.000030 TLoss:0.649134 VLoss:0.776112 VAcc:0.405991\n",
      "('learn_step=', 79762)\n",
      "('TP =', 765037)\n",
      "('FP = ', 9632706)\n",
      "('TN = ', 5974141)\n",
      "('FN = ', 405332)\n",
      "('prec = ', 0.07357721767118043)\n",
      "('test_accuracy = ', 0.4016863703727722)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 05:21:59 | Step: 83960( 20/ 23) LR:0.000030 TLoss:0.648202 VLoss:0.789723 VAcc:0.399072\n",
      "('learn_step=', 83960)\n",
      "('TP =', 770395)\n",
      "('FP = ', 9744892)\n",
      "('TN = ', 5861955)\n",
      "('FN = ', 399974)\n",
      "('prec = ', 0.07326428655727539)\n",
      "('test_accuracy = ', 0.39531904458999634)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 05:45:13 | Step: 88158( 21/ 23) LR:0.000030 TLoss:0.647554 VLoss:0.787791 VAcc:0.405639\n",
      "('learn_step=', 88158)\n",
      "('TP =', 764481)\n",
      "('FP = ', 9879789)\n",
      "('TN = ', 5727058)\n",
      "('FN = ', 405888)\n",
      "('prec = ', 0.07182089518585988)\n",
      "('test_accuracy = ', 0.3869258761405945)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 06:08:30 | Step: 92356( 22/ 23) LR:0.000030 TLoss:0.646874 VLoss:0.789129 VAcc:0.376520\n",
      "('learn_step=', 92356)\n",
      "('TP =', 792613)\n",
      "('FP = ', 9962533)\n",
      "('TN = ', 5644314)\n",
      "('FN = ', 377756)\n",
      "('prec = ', 0.07369616367829805)\n",
      "('test_accuracy = ', 0.3836711049079895)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 06:31:48 | Step: 96554( 23/ 23) LR:0.000030 TLoss:0.646226 VLoss:0.780521 VAcc:0.376026\n",
      "('learn_step=', 96554)\n",
      "('TP =', 807482)\n",
      "('FP = ', 10121817)\n",
      "('TN = ', 5485030)\n",
      "('FN = ', 362887)\n",
      "('prec = ', 0.07388232310233186)\n",
      "('test_accuracy = ', 0.37506306171417236)\n",
      "('train_batches=', 4198)\n",
      "4197()\n",
      "('vali_batches=', 2)\n",
      "()\n",
      "05-18 06:54:49 | Step:100752( 24/ 23) LR:0.000030 TLoss:0.645696 VLoss:0.779885 VAcc:0.362674\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 100000\n",
    "batch_size = 32\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "    vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "        print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "            \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            \n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        vali_loss = 0\n",
    "        vali_accuracy = 0\n",
    "        vali_batches = vx.shape[0]//batch_size\n",
    "        print()\n",
    "        \n",
    "        print('vali_batches=',vali_batches)\n",
    "        for j in range(vali_batches):\n",
    "#             print('\\rj: %d'%j)\n",
    "            select = next(vnb)\n",
    "            batch_X = vx[select]\n",
    "            batch_Y  = vy[select]\n",
    "            batch_X1 = vx1[select]\n",
    "            batch_X=np.multiply(batch_X,1.0/mx)\n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_Y))\n",
    "\n",
    "            vali_loss += sess.run(loss, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "            vali_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "        vali_loss /= vali_batches\n",
    "        vali_accuracy /= vali_batches\n",
    "        \n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TLoss:%.6f VLoss:%f VAcc:%f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, vali_loss, vali_accuracy))\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        print('learn_step=',learn_step)\n",
    "        if learn_step % int(show_freq * train_batches)==0:\n",
    "            \n",
    "            \n",
    "            TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "            #TP1=0;FP1=0\n",
    "            num_images=batch_size\n",
    "            n_chunks=np.shape(tx)[0]//batch_size\n",
    "            tAcc=np.zeros(n_chunks)\n",
    "\n",
    "            for chunk in range(0,n_chunks):               \n",
    "                tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx1_batch=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=conv_mask_gt(ty_batch)\n",
    "                tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "                a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "                TP+=a; FP+=b;TN+=c; FN+=d\n",
    "                \n",
    "            print(\"TP =\",TP)\n",
    "            print(\"FP = \",FP)\n",
    "            print(\"TN = \",TN)\n",
    "            print(\"FN = \",FN)\n",
    "            \n",
    "            prec=metrics(TP,FP,TN,FN)            \n",
    "            test_accuracy=np.mean(tAcc)\n",
    "            print(\"prec = \",prec)\n",
    "            print(\"test_accuracy = \",test_accuracy)\n",
    "           \n",
    "            if prec > 0.45 :\n",
    "                best_prec = prec\n",
    "                save_path=saver.save(sess,'../model_shi/final_model_nist.ckpt')\n",
    "                print (\"Best Model Found on NC16...\")\n",
    "                print ( \"prec = \"+str(prec) + \", acc = \"+ str(test_accuracy))\n",
    "\n",
    "            saver.save(sess, '../model_shi/modelS%d.ckpt' % learn_step)\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_shi/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T02:32:07.513816Z",
     "start_time": "2019-05-22T11:16:56.302635Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_epoch=', 44.0)\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 19:36:51 | Step:  2239(  1/ 44) LR:0.000030 TLoss:0.640677 VLoss:0.619777 VAcc:0.630257\n",
      "('learn_step=', 2239)\n",
      " | TP：435630 FP:4741920 TN:9642551 FN:908539----prec:0.084138-----Tacc:0.640754\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 19:57:30 | Step:  4478(  2/ 44) LR:0.000030 TLoss:0.599521 VLoss:0.604033 VAcc:0.704309\n",
      "('learn_step=', 4478)\n",
      " | TP：365305 FP:2874600 TN:11509871 FN:978864----prec:0.112752-----Tacc:0.755005\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 20:17:39 | Step:  6717(  3/ 44) LR:0.000030 TLoss:0.587363 VLoss:0.600613 VAcc:0.740475\n",
      "('learn_step=', 6717)\n",
      " | TP：120352 FP:961628 TN:13422843 FN:1223817----prec:0.111233-----Tacc:0.861054\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 20:37:57 | Step:  8956(  4/ 44) LR:0.000030 TLoss:0.579335 VLoss:0.587010 VAcc:0.788289\n",
      "('learn_step=', 8956)\n",
      " | TP：50273 FP:406819 TN:13977652 FN:1293896----prec:0.109984-----Tacc:0.891876\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 20:58:08 | Step: 11195(  5/ 44) LR:0.000030 TLoss:0.571834 VLoss:0.582455 VAcc:0.828929\n",
      "('learn_step=', 11195)\n",
      " | TP：15512 FP:60383 TN:14324088 FN:1328657----prec:0.204388-----Tacc:0.911687\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 21:18:16 | Step: 13434(  6/ 44) LR:0.000030 TLoss:0.564413 VLoss:0.576473 VAcc:0.928456\n",
      "('learn_step=', 13434)\n",
      " | TP：30686 FP:305509 TN:14078962 FN:1313483----prec:0.091274-----Tacc:0.897068\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 21:38:26 | Step: 15673(  7/ 44) LR:0.000030 TLoss:0.557302 VLoss:0.554537 VAcc:0.968017\n",
      "('learn_step=', 15673)\n",
      " | TP：39465 FP:353115 TN:14031356 FN:1304704----prec:0.100527-----Tacc:0.894599\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 21:58:56 | Step: 17912(  8/ 44) LR:0.000030 TLoss:0.550237 VLoss:0.564869 VAcc:0.909618\n",
      "('learn_step=', 17912)\n",
      " | TP：181160 FP:1216274 TN:13168197 FN:1163009----prec:0.129638-----Tacc:0.848729\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 22:19:11 | Step: 20151(  9/ 44) LR:0.000030 TLoss:0.543354 VLoss:0.560345 VAcc:0.918359\n",
      "('learn_step=', 20151)\n",
      " | TP：34705 FP:135547 TN:14248924 FN:1309464----prec:0.203845-----Tacc:0.908129\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 22:39:31 | Step: 22390( 10/ 44) LR:0.000030 TLoss:0.536553 VLoss:0.541852 VAcc:0.857627\n",
      "('learn_step=', 22390)\n",
      " | TP：98888 FP:480603 TN:13903868 FN:1245281----prec:0.170646-----Tacc:0.890272\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 23:00:00 | Step: 24629( 11/ 44) LR:0.000030 TLoss:0.529966 VLoss:0.541593 VAcc:0.920754\n",
      "('learn_step=', 24629)\n",
      " | TP：144089 FP:525674 TN:13858797 FN:1200080----prec:0.215134-----Tacc:0.890280\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 23:20:11 | Step: 26868( 12/ 44) LR:0.000030 TLoss:0.523487 VLoss:0.529707 VAcc:0.949010\n",
      "('learn_step=', 26868)\n",
      " | TP：93626 FP:273363 TN:14111108 FN:1250543----prec:0.255119-----Tacc:0.903113\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-22 23:40:55 | Step: 29107( 13/ 44) LR:0.000030 TLoss:0.517303 VLoss:0.533421 VAcc:0.918287\n",
      "('learn_step=', 29107)\n",
      " | TP：108768 FP:252104 TN:14132367 FN:1235401----prec:0.301403-----Tacc:0.905427\n",
      "Best Model Found on NC16...\n",
      "prec = 0.301403267641, acc = 0.905426986515522\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 00:01:13 | Step: 31346( 14/ 44) LR:0.000030 TLoss:0.511145 VLoss:0.527957 VAcc:0.930460\n",
      "('learn_step=', 31346)\n",
      " | TP：79177 FP:188704 TN:14195767 FN:1264992----prec:0.295568-----Tacc:0.907576\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 00:21:41 | Step: 33585( 15/ 44) LR:0.000030 TLoss:0.505339 VLoss:0.530060 VAcc:0.928494\n",
      "('learn_step=', 33585)\n",
      " | TP：100492 FP:181594 TN:14202877 FN:1243677----prec:0.356246-----Tacc:0.909384\n",
      "Best Model Found on NC16...\n",
      "prec = 0.356245967542, acc = 0.9093837812542915\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 00:42:00 | Step: 35824( 16/ 44) LR:0.000030 TLoss:0.499740 VLoss:0.503389 VAcc:0.956965\n",
      "('learn_step=', 35824)\n",
      " | TP：104624 FP:170524 TN:14213947 FN:1239545----prec:0.380246-----Tacc:0.910350\n",
      "Best Model Found on NC16...\n",
      "prec = 0.380246267463, acc = 0.9103502184152603\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 01:02:24 | Step: 38063( 17/ 44) LR:0.000030 TLoss:0.494269 VLoss:0.506280 VAcc:0.944327\n",
      "('learn_step=', 38063)\n",
      " | TP：108387 FP:201914 TN:14182557 FN:1235782----prec:0.349296-----Tacc:0.908594\n",
      "Best Model Found on NC16...\n",
      "prec = 0.349296328404, acc = 0.9085937589406967\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 01:22:47 | Step: 40302( 18/ 44) LR:0.000030 TLoss:0.489049 VLoss:0.496802 VAcc:0.952597\n",
      "('learn_step=', 40302)\n",
      " | TP：81344 FP:154526 TN:14229945 FN:1262825----prec:0.344868-----Tacc:0.909887\n",
      "Best Model Found on NC16...\n",
      "prec = 0.344867935727, acc = 0.909887321293354\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 01:43:07 | Step: 42541( 19/ 44) LR:0.000030 TLoss:0.484047 VLoss:0.496216 VAcc:0.942306\n",
      "('learn_step=', 42541)\n",
      " | TP：105083 FP:239199 TN:14145272 FN:1239086----prec:0.305224-----Tacc:0.906013\n",
      "Best Model Found on NC16...\n",
      "prec = 0.305223624819, acc = 0.9060134291648865\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 02:03:30 | Step: 44780( 20/ 44) LR:0.000030 TLoss:0.479319 VLoss:0.499491 VAcc:0.939545\n",
      "('learn_step=', 44780)\n",
      " | TP：110159 FP:213210 TN:14171261 FN:1234010----prec:0.340660-----Tacc:0.907989\n",
      "Best Model Found on NC16...\n",
      "prec = 0.340660360146, acc = 0.9079891294240952\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 02:23:49 | Step: 47019( 21/ 44) LR:0.000030 TLoss:0.474958 VLoss:0.496270 VAcc:0.941751\n",
      "('learn_step=', 47019)\n",
      " | TP：94269 FP:144734 TN:14239737 FN:1249900----prec:0.394426-----Tacc:0.911332\n",
      "Best Model Found on NC16...\n",
      "prec = 0.394426011389, acc = 0.9113315567374229\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 02:44:13 | Step: 49258( 22/ 44) LR:0.000030 TLoss:0.470371 VLoss:0.495888 VAcc:0.927571\n",
      "('learn_step=', 49258)\n",
      " | TP：131618 FP:290251 TN:14094220 FN:1212551----prec:0.311988-----Tacc:0.904454\n",
      "Best Model Found on NC16...\n",
      "prec = 0.311987844568, acc = 0.904454417526722\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 03:04:47 | Step: 51497( 23/ 44) LR:0.000030 TLoss:0.466264 VLoss:0.517795 VAcc:0.877853\n",
      "('learn_step=', 51497)\n",
      " | TP：100331 FP:268077 TN:14116394 FN:1243838----prec:0.272337-----Tacc:0.903875\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 03:24:59 | Step: 53736( 24/ 44) LR:0.000030 TLoss:0.462369 VLoss:0.497618 VAcc:0.928888\n",
      "('learn_step=', 53736)\n",
      " | TP：116836 FP:276151 TN:14108320 FN:1227333----prec:0.297302-----Tacc:0.904411\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 03:45:02 | Step: 55975( 25/ 44) LR:0.000030 TLoss:0.458590 VLoss:0.485449 VAcc:0.927058\n",
      "('learn_step=', 55975)\n",
      " | TP：108173 FP:257966 TN:14126505 FN:1235996----prec:0.295442-----Tacc:0.905016\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 04:05:28 | Step: 58214( 26/ 44) LR:0.000030 TLoss:0.454939 VLoss:0.494298 VAcc:0.926013\n",
      "('learn_step=', 58214)\n",
      " | TP：145269 FP:315220 TN:14069251 FN:1198900----prec:0.315467-----Tacc:0.903735\n",
      "Best Model Found on NC16...\n",
      "prec = 0.315466818968, acc = 0.9037348479032516\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 04:25:43 | Step: 60453( 27/ 44) LR:0.000030 TLoss:0.451437 VLoss:0.490129 VAcc:0.921606\n",
      "('learn_step=', 60453)\n",
      " | TP：100211 FP:204847 TN:14179624 FN:1243958----prec:0.328498-----Tacc:0.907887\n",
      "Best Model Found on NC16...\n",
      "prec = 0.32849818723, acc = 0.9078874513506889\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 04:46:20 | Step: 62692( 28/ 44) LR:0.000030 TLoss:0.448244 VLoss:0.479467 VAcc:0.939068\n",
      "('learn_step=', 62692)\n",
      " | TP：100772 FP:170160 TN:14214311 FN:1243397----prec:0.371946-----Tacc:0.910128\n",
      "Best Model Found on NC16...\n",
      "prec = 0.371945728079, acc = 0.9101284667849541\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 05:06:26 | Step: 64931( 29/ 44) LR:0.000030 TLoss:0.445107 VLoss:0.487323 VAcc:0.917270\n",
      "('learn_step=', 64931)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | TP：102378 FP:250686 TN:14133785 FN:1241791----prec:0.289970-----Tacc:0.905111\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 05:26:52 | Step: 67170( 30/ 44) LR:0.000030 TLoss:0.442226 VLoss:0.483823 VAcc:0.928511\n",
      "('learn_step=', 67170)\n",
      " | TP：113299 FP:193323 TN:14191148 FN:1230870----prec:0.369507-----Tacc:0.909452\n",
      "Best Model Found on NC16...\n",
      "prec = 0.369507080379, acc = 0.9094522446393967\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 05:47:07 | Step: 69409( 31/ 44) LR:0.000030 TLoss:0.439532 VLoss:0.476985 VAcc:0.927225\n",
      "('learn_step=', 69409)\n",
      " | TP：116520 FP:253567 TN:14130904 FN:1227649----prec:0.314845-----Tacc:0.905827\n",
      "Best Model Found on NC16...\n",
      "prec = 0.314844887824, acc = 0.9058268219232559\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 06:07:25 | Step: 71648( 32/ 44) LR:0.000030 TLoss:0.436895 VLoss:0.476548 VAcc:0.928157\n",
      "('learn_step=', 71648)\n",
      " | TP：125797 FP:223103 TN:14161368 FN:1218372----prec:0.360553-----Tacc:0.908354\n",
      "Best Model Found on NC16...\n",
      "prec = 0.360553167096, acc = 0.9083535447716713\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 06:27:39 | Step: 73887( 33/ 44) LR:0.000030 TLoss:0.434370 VLoss:0.489050 VAcc:0.927250\n",
      "('learn_step=', 73887)\n",
      " | TP：99937 FP:168548 TN:14215923 FN:1244232----prec:0.372226-----Tacc:0.910178\n",
      "Best Model Found on NC16...\n",
      "prec = 0.372225636441, acc = 0.9101778566837311\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 06:48:13 | Step: 76126( 34/ 44) LR:0.000030 TLoss:0.432071 VLoss:0.468495 VAcc:0.935670\n",
      "('learn_step=', 76126)\n",
      " | TP：82847 FP:159303 TN:14225168 FN:1261322----prec:0.342131-----Tacc:0.909679\n",
      "Best Model Found on NC16...\n",
      "prec = 0.342130910592, acc = 0.9096790924668312\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 07:08:37 | Step: 78365( 35/ 44) LR:0.000030 TLoss:0.429726 VLoss:0.483861 VAcc:0.928297\n",
      "('learn_step=', 78365)\n",
      " | TP：105756 FP:175059 TN:14209412 FN:1238413----prec:0.376604-----Tacc:0.910134\n",
      "Best Model Found on NC16...\n",
      "prec = 0.376603813899, acc = 0.9101338684558868\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 07:29:16 | Step: 80604( 36/ 44) LR:0.000030 TLoss:0.427720 VLoss:0.455182 VAcc:0.951359\n",
      "('learn_step=', 80604)\n",
      " | TP：70180 FP:87394 TN:14297077 FN:1273989----prec:0.445378-----Tacc:0.913446\n",
      "Best Model Found on NC16...\n",
      "prec = 0.445378044601, acc = 0.9134455919265747\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 07:49:23 | Step: 82843( 37/ 44) LR:0.000030 TLoss:0.425779 VLoss:0.481855 VAcc:0.919344\n",
      "('learn_step=', 82843)\n",
      " | TP：96133 FP:205546 TN:14178925 FN:1248036----prec:0.318660-----Tacc:0.907584\n",
      "Best Model Found on NC16...\n",
      "prec = 0.318659900092, acc = 0.9075837433338165\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 08:09:49 | Step: 85082( 38/ 44) LR:0.000030 TLoss:0.424092 VLoss:0.475480 VAcc:0.929716\n",
      "('learn_step=', 85082)\n",
      " | TP：91941 FP:195709 TN:14188762 FN:1252228----prec:0.319628-----Tacc:0.907943\n",
      "Best Model Found on NC16...\n",
      "prec = 0.319628020163, acc = 0.9079426378011703\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 08:30:13 | Step: 87321( 39/ 44) LR:0.000030 TLoss:0.422228 VLoss:0.479902 VAcc:0.923741\n",
      "('learn_step=', 87321)\n",
      " | TP：89746 FP:168920 TN:14215551 FN:1254423----prec:0.346957-----Tacc:0.909506\n",
      "Best Model Found on NC16...\n",
      "prec = 0.346957079786, acc = 0.9095062911510468\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 08:50:26 | Step: 89560( 40/ 44) LR:0.000030 TLoss:0.420782 VLoss:0.472968 VAcc:0.935309\n",
      "('learn_step=', 89560)\n",
      " | TP：75456 FP:113854 TN:14270617 FN:1268713----prec:0.398584-----Tacc:0.912099\n",
      "Best Model Found on NC16...\n",
      "prec = 0.398584332576, acc = 0.9120987504720688\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 09:10:28 | Step: 91799( 41/ 44) LR:0.000030 TLoss:0.419335 VLoss:0.479631 VAcc:0.928107\n",
      "('learn_step=', 91799)\n",
      " | TP：75541 FP:112625 TN:14271846 FN:1268628----prec:0.401459-----Tacc:0.912182\n",
      "Best Model Found on NC16...\n",
      "prec = 0.401459349723, acc = 0.9121823012828827\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 09:30:51 | Step: 94038( 42/ 44) LR:0.000030 TLoss:0.418118 VLoss:0.448692 VAcc:0.949464\n",
      "('learn_step=', 94038)\n",
      " | TP：86476 FP:105241 TN:14279230 FN:1257693----prec:0.451061-----Tacc:0.913347\n",
      "Best Model Found on NC16...\n",
      "prec = 0.451060677978, acc = 0.913346990942955\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 09:51:17 | Step: 96277( 43/ 44) LR:0.000030 TLoss:0.417177 VLoss:0.463581 VAcc:0.935536\n",
      "('learn_step=', 96277)\n",
      " | TP：92195 FP:139101 TN:14245370 FN:1251974----prec:0.398602-----Tacc:0.911558\n",
      "Best Model Found on NC16...\n",
      "prec = 0.398601791643, acc = 0.9115578308701515\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 10:11:38 | Step: 98516( 44/ 44) LR:0.000030 TLoss:0.416071 VLoss:0.479943 VAcc:0.928194\n",
      "('learn_step=', 98516)\n",
      " | TP：79688 FP:103480 TN:14280991 FN:1264481----prec:0.435054-----Tacc:0.913027\n",
      "Best Model Found on NC16...\n",
      "prec = 0.435054157931, acc = 0.9130273833870888\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "()\n",
      "05-23 10:31:56 | Step:100755( 45/ 44) LR:0.000030 TLoss:0.415247 VLoss:0.478583 VAcc:0.931666\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 100000\n",
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "    vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "        print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "            \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            \n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        vali_loss = 0\n",
    "        vali_accuracy = 0\n",
    "        vali_batches = vx.shape[0]//batch_size\n",
    "        print()\n",
    "        \n",
    "        print('vali_batches=',vali_batches)\n",
    "        for j in range(vali_batches):\n",
    "#             print('\\rj: %d'%j)\n",
    "            select = next(vnb)\n",
    "            batch_X = vx[select]\n",
    "            batch_Y  = vy[select]\n",
    "            batch_X1 = vx1[select]\n",
    "            batch_X=np.multiply(batch_X,1.0/mx)\n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_Y))\n",
    "\n",
    "            vali_loss += sess.run(loss, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "            vali_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "        vali_loss /= vali_batches\n",
    "        vali_accuracy /= vali_batches\n",
    "        \n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TLoss:%.6f VLoss:%f VAcc:%f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, vali_loss, vali_accuracy))\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        print('learn_step=',learn_step)\n",
    "        if learn_step % int(show_freq * train_batches)==0:\n",
    "            \n",
    "            \n",
    "            TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "            #TP1=0;FP1=0\n",
    "            num_images=batch_size\n",
    "            n_chunks=np.shape(tx)[0]//batch_size\n",
    "            tAcc=np.zeros(n_chunks)\n",
    "\n",
    "            for chunk in range(0,n_chunks):               \n",
    "                tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx1_batch=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=conv_mask_gt(ty_batch)\n",
    "                tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "                a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "                TP+=a; FP+=b;TN+=c; FN+=d\n",
    "            \n",
    "            prec=metrics(TP,FP,TN,FN)            \n",
    "            test_accuracy=np.mean(tAcc)\n",
    "       \n",
    "            print(' | TP：%d FP:%d TN:%d FN:%d----prec:%.6f-----Tacc:%.6f' % (TP, FP, TN, FN, prec, test_accuracy))\n",
    "            \n",
    "            if prec > 0.30 :\n",
    "                best_prec = prec\n",
    "                save_path=saver.save(sess,'../model_s_gaijin1/final_model_nist.ckpt')\n",
    "                print (\"Best Model Found on NC16...\")\n",
    "                print ( \"prec = \"+str(prec) + \", acc = \"+ str(test_accuracy))\n",
    "\n",
    "            saver.save(sess, '../model_s_gaijin1/modelS%d.ckpt' % learn_step)\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_s_gaijin1/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-06T14:12:15.699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_epoch=', 44.0)\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-06 22:37:29 | Step:  2239(  1/ 44) LR:0.000030 TLoss:0.628372 VLoss:0.619009 VAcc:0.748359\n",
      " | TP：348867 FP:3787789 TN:10596682 FN:995302----prec:0.084336-----Tacc:0.695900\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-06 23:03:00 | Step:  4478(  2/ 44) LR:0.000030 TLoss:0.598614 VLoss:0.605903 VAcc:0.909326\n",
      " | TP：144419 FP:1097300 TN:13287171 FN:1199750----prec:0.116306-----Tacc:0.853960\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-06 23:28:21 | Step:  6717(  3/ 44) LR:0.000030 TLoss:0.588858 VLoss:0.598874 VAcc:0.928887\n",
      " | TP：107507 FP:531094 TN:13853377 FN:1236662----prec:0.168348-----Tacc:0.887610\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-06 23:54:19 | Step:  8956(  4/ 44) LR:0.000030 TLoss:0.580286 VLoss:0.590473 VAcc:0.925045\n",
      " | TP：141039 FP:485421 TN:13899050 FN:1203130----prec:0.225136-----Tacc:0.892645\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 00:20:35 | Step: 11195(  5/ 44) LR:0.000030 TLoss:0.572080 VLoss:0.582843 VAcc:0.930822\n",
      " | TP：122930 FP:453021 TN:13931450 FN:1221239----prec:0.213438-----Tacc:0.893553\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 00:46:12 | Step: 13434(  6/ 44) LR:0.000030 TLoss:0.563994 VLoss:0.576191 VAcc:0.932351\n",
      " | TP：134451 FP:350698 TN:14033773 FN:1209718----prec:0.277133-----Tacc:0.900792\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 01:11:42 | Step: 15673(  7/ 44) LR:0.000030 TLoss:0.556265 VLoss:0.570793 VAcc:0.926855\n",
      " | TP：150874 FP:421480 TN:13962991 FN:1193295----prec:0.263603-----Tacc:0.897335\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 01:37:39 | Step: 17912(  8/ 44) LR:0.000030 TLoss:0.548740 VLoss:0.566043 VAcc:0.927866\n",
      " | TP：126976 FP:351458 TN:14033013 FN:1217193----prec:0.265399-----Tacc:0.900268\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 02:03:19 | Step: 20151(  9/ 44) LR:0.000030 TLoss:0.541571 VLoss:0.558649 VAcc:0.928639\n",
      " | TP：155616 FP:378650 TN:14005821 FN:1188553----prec:0.291271-----Tacc:0.900360\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 02:29:17 | Step: 22390( 10/ 44) LR:0.000030 TLoss:0.534589 VLoss:0.554977 VAcc:0.927302\n",
      " | TP：147541 FP:352040 TN:14032431 FN:1196628----prec:0.295329-----Tacc:0.901538\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 02:54:51 | Step: 24629( 11/ 44) LR:0.000030 TLoss:0.527836 VLoss:0.546804 VAcc:0.930631\n",
      " | TP：164227 FP:325823 TN:14058648 FN:1179942----prec:0.335123-----Tacc:0.904265\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 03:20:47 | Step: 26868( 12/ 44) LR:0.000030 TLoss:0.521329 VLoss:0.543391 VAcc:0.927664\n",
      " | TP：184245 FP:400317 TN:13984154 FN:1159924----prec:0.315185-----Tacc:0.900803\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 03:45:58 | Step: 29107( 13/ 44) LR:0.000030 TLoss:0.514989 VLoss:0.537481 VAcc:0.930753\n",
      " | TP：167137 FP:344576 TN:14039895 FN:1177032----prec:0.326623-----Tacc:0.903259\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 04:11:38 | Step: 31346( 14/ 44) LR:0.000030 TLoss:0.508913 VLoss:0.532893 VAcc:0.930110\n",
      " | TP：165557 FP:364457 TN:14020014 FN:1178612----prec:0.312363-----Tacc:0.901894\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 04:37:33 | Step: 33585( 15/ 44) LR:0.000030 TLoss:0.503052 VLoss:0.528784 VAcc:0.929669\n",
      " | TP：170158 FP:393088 TN:13991383 FN:1174011----prec:0.302102-----Tacc:0.900367\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 05:03:29 | Step: 35824( 16/ 44) LR:0.000030 TLoss:0.497398 VLoss:0.523838 VAcc:0.929247\n",
      " | TP：184949 FP:386905 TN:13997566 FN:1159220----prec:0.323420-----Tacc:0.901700\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 05:28:52 | Step: 38063( 17/ 44) LR:0.000030 TLoss:0.491988 VLoss:0.518735 VAcc:0.929355\n",
      " | TP：168271 FP:384199 TN:14000272 FN:1175898----prec:0.304579-----Tacc:0.900812\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 05:54:46 | Step: 40302( 18/ 44) LR:0.000030 TLoss:0.486777 VLoss:0.512873 VAcc:0.933220\n",
      " | TP：133967 FP:278031 TN:14106440 FN:1210202----prec:0.325164-----Tacc:0.905381\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 06:20:59 | Step: 42541( 19/ 44) LR:0.000030 TLoss:0.481806 VLoss:0.508452 VAcc:0.931656\n",
      " | TP：158600 FP:341141 TN:14043330 FN:1185569----prec:0.317364-----Tacc:0.902934\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 06:46:21 | Step: 44780( 20/ 44) LR:0.000030 TLoss:0.477042 VLoss:0.505562 VAcc:0.932261\n",
      " | TP：160624 FP:301414 TN:14083057 FN:1183545----prec:0.347642-----Tacc:0.905589\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 07:11:47 | Step: 47019( 21/ 44) LR:0.000030 TLoss:0.472537 VLoss:0.502571 VAcc:0.933438\n",
      " | TP：156287 FP:265709 TN:14118762 FN:1187882----prec:0.370352-----Tacc:0.907583\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 07:37:36 | Step: 49258( 22/ 44) LR:0.000030 TLoss:0.468200 VLoss:0.501034 VAcc:0.925905\n",
      " | TP：196388 FP:439043 TN:13945428 FN:1147781----prec:0.309063-----Tacc:0.899112\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 08:03:16 | Step: 51497( 23/ 44) LR:0.000030 TLoss:0.464038 VLoss:0.496133 VAcc:0.930314\n",
      " | TP：179588 FP:386931 TN:13997540 FN:1164581----prec:0.317003-----Tacc:0.901358\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 08:28:49 | Step: 53736( 24/ 44) LR:0.000030 TLoss:0.460125 VLoss:0.493715 VAcc:0.926507\n",
      " | TP：176874 FP:340864 TN:14043607 FN:1167295----prec:0.341628-----Tacc:0.904114\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 08:54:49 | Step: 55975( 25/ 44) LR:0.000030 TLoss:0.456429 VLoss:0.493644 VAcc:0.925739\n",
      " | TP：174423 FP:393196 TN:13991275 FN:1169746----prec:0.307289-----Tacc:0.900631\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 09:19:57 | Step: 58214( 26/ 44) LR:0.000030 TLoss:0.452770 VLoss:0.498731 VAcc:0.918452\n",
      " | TP：201599 FP:501199 TN:13883272 FN:1142570----prec:0.286852-----Tacc:0.895492\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 09:45:19 | Step: 60453( 27/ 44) LR:0.000030 TLoss:0.449430 VLoss:0.491652 VAcc:0.928203\n",
      " | TP：167914 FP:345349 TN:14039122 FN:1176255----prec:0.327150-----Tacc:0.903259\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 10:11:52 | Step: 62692( 28/ 44) LR:0.000030 TLoss:0.446176 VLoss:0.491860 VAcc:0.925385\n",
      " | TP：198926 FP:419688 TN:13964783 FN:1145243----prec:0.321567-----Tacc:0.900504\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 10:37:29 | Step: 64931( 29/ 44) LR:0.000030 TLoss:0.443206 VLoss:0.493176 VAcc:0.921030\n",
      " | TP：194511 FP:464846 TN:13919625 FN:1149658----prec:0.295001-----Tacc:0.897353\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 11:03:25 | Step: 67170( 30/ 44) LR:0.000030 TLoss:0.440337 VLoss:0.487171 VAcc:0.927225\n",
      " | TP：188173 FP:400727 TN:13983744 FN:1155996----prec:0.319533-----Tacc:0.901026\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 11:29:09 | Step: 69409( 31/ 44) LR:0.000030 TLoss:0.437671 VLoss:0.486489 VAcc:0.924653\n",
      " | TP：216034 FP:450642 TN:13933829 FN:1128135----prec:0.324046-----Tacc:0.899624\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 11:55:29 | Step: 71648( 32/ 44) LR:0.000030 TLoss:0.435149 VLoss:0.488320 VAcc:0.921261\n",
      " | TP：207307 FP:483201 TN:13901270 FN:1136862----prec:0.300224-----Tacc:0.896999\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 12:21:21 | Step: 73887( 33/ 44) LR:0.000030 TLoss:0.432783 VLoss:0.483820 VAcc:0.923603\n",
      " | TP：194887 FP:459345 TN:13925126 FN:1149282----prec:0.297887-----Tacc:0.897726\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 12:47:03 | Step: 76126( 34/ 44) LR:0.000030 TLoss:0.430547 VLoss:0.484396 VAcc:0.921652\n",
      " | TP：185674 FP:406412 TN:13978059 FN:1158495----prec:0.313593-----Tacc:0.900506\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 13:12:51 | Step: 78365( 35/ 44) LR:0.000030 TLoss:0.428460 VLoss:0.481564 VAcc:0.926273\n",
      " | TP：202854 FP:418399 TN:13966072 FN:1141315----prec:0.326524-----Tacc:0.900836\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 13:38:55 | Step: 80604( 36/ 44) LR:0.000030 TLoss:0.426589 VLoss:0.481941 VAcc:0.921980\n",
      " | TP：190707 FP:433835 TN:13950636 FN:1153462----prec:0.305355-----Tacc:0.899082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 14:04:57 | Step: 82843( 37/ 44) LR:0.000030 TLoss:0.424850 VLoss:0.483029 VAcc:0.921375\n",
      " | TP：207146 FP:451539 TN:13932932 FN:1137023----prec:0.314484-----Tacc:0.899002\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 14:30:49 | Step: 85082( 38/ 44) LR:0.000030 TLoss:0.423161 VLoss:0.477301 VAcc:0.926505\n",
      " | TP：199552 FP:449842 TN:13934629 FN:1144617----prec:0.307290-----Tacc:0.898627\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 14:56:45 | Step: 87321( 39/ 44) LR:0.000030 TLoss:0.421657 VLoss:0.479705 VAcc:0.923668\n",
      " | TP：188483 FP:488914 TN:13895557 FN:1155686----prec:0.278246-----Tacc:0.895439\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 15:22:49 | Step: 89560( 40/ 44) LR:0.000030 TLoss:0.420375 VLoss:0.479050 VAcc:0.922830\n",
      " | TP：180416 FP:457331 TN:13927140 FN:1163753----prec:0.282896-----Tacc:0.896934\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 15:47:57 | Step: 91799( 41/ 44) LR:0.000030 TLoss:0.419166 VLoss:0.476770 VAcc:0.924866\n",
      " | TP：186850 FP:421287 TN:13963184 FN:1157319----prec:0.307250-----Tacc:0.899635\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 16:13:43 | Step: 94038( 42/ 44) LR:0.000030 TLoss:0.418126 VLoss:0.477070 VAcc:0.924680\n",
      " | TP：185323 FP:424411 TN:13960060 FN:1158846----prec:0.303941-----Tacc:0.899339\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 16:39:29 | Step: 96277( 43/ 44) LR:0.000030 TLoss:0.417165 VLoss:0.478163 VAcc:0.924721\n",
      " | TP：194533 FP:472846 TN:13911625 FN:1149636----prec:0.291488-----Tacc:0.896845\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 17:05:28 | Step: 98516( 44/ 44) LR:0.000030 TLoss:0.416333 VLoss:0.474248 VAcc:0.926965\n",
      " | TP：166433 FP:447707 TN:13936764 FN:1177736----prec:0.271002-----Tacc:0.896657\n",
      "('train_batches=', 2239)\n",
      "2238()\n",
      "('vali_batches=', 1)\n",
      "06-07 17:30:39 | Step:100755( 45/ 44) LR:0.000030 TLoss:0.415644 VLoss:0.477273 VAcc:0.925225\n",
      " | TP：194445 FP:458752 TN:13925719 FN:1149724----prec:0.297682-----Tacc:0.897736\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 100000\n",
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "    vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_batches = int(Lab.shape[0] // batch_size)\n",
    "        print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = Img[select]\n",
    "        \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            \n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        vali_loss = 0\n",
    "        vali_accuracy = 0\n",
    "        vali_batches = vx.shape[0]//batch_size\n",
    "        print()\n",
    "        \n",
    "        print('vali_batches=',vali_batches)\n",
    "        for j in range(vali_batches):\n",
    "#             print('\\rj: %d'%j,  end='', flush=True)\n",
    "            select = next(vnb)\n",
    "            batch_X = vx[select]\n",
    "            batch_Y  = vy[select]\n",
    "            batch_X1 = vx[select]\n",
    "            batch_X=np.multiply(batch_X,1.0/mx)\n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_Y))\n",
    "\n",
    "            vali_loss += sess.run(loss, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "            vali_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "        vali_loss /= vali_batches\n",
    "        vali_accuracy /= vali_batches\n",
    "        \n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        \n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TLoss:%.6f VLoss:%f VAcc:%f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, vali_loss, vali_accuracy))\n",
    "        \n",
    "        \n",
    "        if learn_step % train_batches==0:\n",
    "            \n",
    "            \n",
    "            TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "            #TP1=0;FP1=0\n",
    "            num_images=batch_size\n",
    "            n_chunks=np.shape(tx)[0]//batch_size\n",
    "            tAcc=np.zeros(n_chunks)\n",
    "\n",
    "            for chunk in range(0,n_chunks):               \n",
    "                tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx1_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=conv_mask_gt(ty_batch)\n",
    "                tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "                a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "                TP+=a; FP+=b;TN+=c; FN+=d\n",
    "                 \n",
    "            prec=metrics(TP,FP,TN,FN)            \n",
    "            test_accuracy=np.mean(tAcc)\n",
    "\n",
    "            print(' | TP：%d FP:%d TN:%d FN:%d----prec:%.6f-----Tacc:%.6f' % (TP, FP, TN, FN, prec, test_accuracy))\n",
    "            \n",
    "            if prec > 0.45 :\n",
    "                best_prec = prec\n",
    "                save_path=saver.save(sess,'../model_gai3/final_model_nist.ckpt')\n",
    "                print (\"Best Model Found on NC16...\")\n",
    "                print ( \"prec = \"+str(prec)+\"(\"+str(best_prec)+\")\" + \", acc = \"+ str(test_accuracy))\n",
    "\n",
    "            saver.save(sess, '../model_gai3/modelS%d.ckpt' % learn_step)\n",
    "            \n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "    saver.save(sess, '../model_gai3/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PL-GNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T22:01:28.406638Z",
     "start_time": "2019-06-10T06:55:33.366584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total_epoch=', 44.0)\n",
      "223806-10 15:26:20 | Step:  2239(  1/ 44) LR:0.000030 TLoss:0.628752 VLoss:0.625361 VAcc:0.681947\n",
      "('learn_step=', 2239)\n",
      " | TP：480907 FP:3270213 TN:11114258 FN:863262----prec:0.128204-----Tacc:0.737201\n",
      "223806-10 16:02:34 | Step:  4478(  2/ 44) LR:0.000030 TLoss:0.599538 VLoss:0.612402 VAcc:0.710164\n",
      "('learn_step=', 4478)\n",
      " | TP：247650 FP:1389222 TN:12995249 FN:1096519----prec:0.151295-----Tacc:0.841962\n",
      "223806-10 16:52:07 | Step:  6717(  3/ 44) LR:0.000030 TLoss:0.592080 VLoss:0.603283 VAcc:0.723325\n",
      "('learn_step=', 6717)\n",
      " | TP：194531 FP:844816 TN:13539655 FN:1149638----prec:0.187167-----Tacc:0.873196\n",
      "223806-10 17:44:08 | Step:  8956(  4/ 44) LR:0.000030 TLoss:0.585652 VLoss:0.592854 VAcc:0.874745\n",
      "('learn_step=', 8956)\n",
      " | TP：162215 FP:571691 TN:13812780 FN:1181954----prec:0.221030-----Tacc:0.888506\n",
      "223806-10 18:36:45 | Step: 11195(  5/ 44) LR:0.000030 TLoss:0.577894 VLoss:0.586371 VAcc:0.910894\n",
      "('learn_step=', 11195)\n",
      " | TP：176918 FP:742802 TN:13641669 FN:1167251----prec:0.192361-----Tacc:0.878563\n",
      "223806-10 19:29:40 | Step: 13434(  6/ 44) LR:0.000030 TLoss:0.570309 VLoss:0.580109 VAcc:0.921548\n",
      "('learn_step=', 13434)\n",
      " | TP：171046 FP:687556 TN:13696915 FN:1173123----prec:0.199215-----Tacc:0.881701\n",
      "WARNING:tensorflow:From /home/shizenan/anaconda3/envs/forgery_localization_HLED1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "223806-10 20:22:09 | Step: 15673(  7/ 44) LR:0.000030 TLoss:0.562301 VLoss:0.573142 VAcc:0.917863\n",
      "('learn_step=', 15673)\n",
      " | TP：187358 FP:624207 TN:13760264 FN:1156811----prec:0.230860-----Tacc:0.886766\n",
      "223806-10 21:15:10 | Step: 17912(  8/ 44) LR:0.000030 TLoss:0.554861 VLoss:0.566887 VAcc:0.925267\n",
      "('learn_step=', 17912)\n",
      " | TP：172718 FP:506885 TN:13877586 FN:1171451----prec:0.254145-----Tacc:0.893294\n",
      "223806-10 22:08:07 | Step: 20151(  9/ 44) LR:0.000030 TLoss:0.545380 VLoss:0.559280 VAcc:0.924667\n",
      "('learn_step=', 20151)\n",
      " | TP：168934 FP:486196 TN:13898275 FN:1175235----prec:0.257863-----Tacc:0.894369\n",
      "223806-10 23:01:08 | Step: 22390( 10/ 44) LR:0.000030 TLoss:0.536094 VLoss:0.552951 VAcc:0.925224\n",
      "('learn_step=', 22390)\n",
      " | TP：181193 FP:400852 TN:13983619 FN:1162976----prec:0.311304-----Tacc:0.900574\n",
      "223806-10 23:54:13 | Step: 24629( 11/ 44) LR:0.000030 TLoss:0.528760 VLoss:0.547351 VAcc:0.923859\n",
      "('learn_step=', 24629)\n",
      " | TP：182802 FP:394064 TN:13990407 FN:1161367----prec:0.316888-----Tacc:0.901108\n",
      "223806-11 00:47:26 | Step: 26868( 12/ 44) LR:0.000030 TLoss:0.521845 VLoss:0.541926 VAcc:0.926993\n",
      "('learn_step=', 26868)\n",
      " | TP：183404 FP:360114 TN:14024357 FN:1160765----prec:0.337439-----Tacc:0.903306\n",
      "223806-11 01:40:55 | Step: 29107( 13/ 44) LR:0.000030 TLoss:0.515249 VLoss:0.535774 VAcc:0.917724\n",
      "('learn_step=', 29107)\n",
      " | TP：219030 FP:444742 TN:13939729 FN:1125139----prec:0.329978-----Tacc:0.900190\n",
      "223806-11 02:33:51 | Step: 31346( 14/ 44) LR:0.000030 TLoss:0.509004 VLoss:0.530137 VAcc:0.917630\n",
      "('learn_step=', 31346)\n",
      " | TP：204395 FP:487563 TN:13896908 FN:1139774----prec:0.295386-----Tacc:0.896537\n",
      "223806-11 03:27:05 | Step: 33585( 15/ 44) LR:0.000030 TLoss:0.503012 VLoss:0.524605 VAcc:0.926648\n",
      "('learn_step=', 33585)\n",
      " | TP：198039 FP:348880 TN:14035591 FN:1146130----prec:0.362099-----Tacc:0.904950\n",
      "223806-11 04:20:04 | Step: 35824( 16/ 44) LR:0.000030 TLoss:0.497183 VLoss:0.521484 VAcc:0.922032\n",
      "('learn_step=', 35824)\n",
      " | TP：212834 FP:437396 TN:13947075 FN:1131335----prec:0.327321-----Tacc:0.900263\n",
      "223806-11 05:13:29 | Step: 38063( 17/ 44) LR:0.000030 TLoss:0.491705 VLoss:0.515725 VAcc:0.926313\n",
      "('learn_step=', 38063)\n",
      " | TP：213990 FP:355808 TN:14028663 FN:1130179----prec:0.375554-----Tacc:0.905523\n",
      "223806-11 06:06:42 | Step: 40302( 18/ 44) LR:0.000030 TLoss:0.486487 VLoss:0.515099 VAcc:0.920644\n",
      "('learn_step=', 40302)\n",
      " | TP：239951 FP:467840 TN:13916631 FN:1104218----prec:0.339014-----Tacc:0.900051\n",
      "223806-11 06:59:47 | Step: 42541( 19/ 44) LR:0.000030 TLoss:0.481435 VLoss:0.506578 VAcc:0.925987\n",
      "('learn_step=', 42541)\n",
      " | TP：219598 FP:407885 TN:13976586 FN:1124571----prec:0.349966-----Tacc:0.902569\n",
      "223806-11 07:53:06 | Step: 44780( 20/ 44) LR:0.000030 TLoss:0.476735 VLoss:0.505067 VAcc:0.920979\n",
      "('learn_step=', 44780)\n",
      " | TP：230348 FP:498993 TN:13885478 FN:1113821----prec:0.315830-----Tacc:0.897460\n",
      "223806-11 08:46:25 | Step: 47019( 21/ 44) LR:0.000030 TLoss:0.472177 VLoss:0.502878 VAcc:0.921784\n",
      "('learn_step=', 47019)\n",
      " | TP：232137 FP:475662 TN:13908809 FN:1112032----prec:0.327970-----Tacc:0.899057\n",
      "223806-11 09:39:27 | Step: 49258( 22/ 44) LR:0.000030 TLoss:0.467897 VLoss:0.496268 VAcc:0.925707\n",
      "('learn_step=', 49258)\n",
      " | TP：220239 FP:386140 TN:13998331 FN:1123930----prec:0.363204-----Tacc:0.903992\n",
      "223806-11 10:32:24 | Step: 51497( 23/ 44) LR:0.000030 TLoss:0.463838 VLoss:0.497553 VAcc:0.919241\n",
      "('learn_step=', 51497)\n",
      " | TP：236126 FP:459190 TN:13925281 FN:1108043----prec:0.339595-----Tacc:0.900358\n",
      "223806-11 11:25:33 | Step: 53736( 24/ 44) LR:0.000030 TLoss:0.459966 VLoss:0.490871 VAcc:0.926203\n",
      "('learn_step=', 53736)\n",
      " | TP：226097 FP:421759 TN:13962712 FN:1118072----prec:0.348993-----Tacc:0.902100\n",
      "223806-11 12:18:41 | Step: 55975( 25/ 44) LR:0.000030 TLoss:0.456324 VLoss:0.489521 VAcc:0.918374\n",
      "('learn_step=', 55975)\n",
      " | TP：250134 FP:524248 TN:13860223 FN:1094035----prec:0.323011-----Tacc:0.897112\n",
      "223806-11 13:12:14 | Step: 58214( 26/ 44) LR:0.000030 TLoss:0.452773 VLoss:0.488693 VAcc:0.922477\n",
      "('learn_step=', 58214)\n",
      " | TP：236375 FP:416986 TN:13967485 FN:1107794----prec:0.361783-----Tacc:0.903057\n",
      "223806-11 14:05:06 | Step: 60453( 27/ 44) LR:0.000030 TLoss:0.449516 VLoss:0.487708 VAcc:0.917361\n",
      "('learn_step=', 60453)\n",
      " | TP：239507 FP:513733 TN:13870738 FN:1104662----prec:0.317969-----Tacc:0.897105\n",
      "223806-11 14:58:02 | Step: 62692( 28/ 44) LR:0.000030 TLoss:0.446453 VLoss:0.485606 VAcc:0.920999\n",
      "('learn_step=', 62692)\n",
      " | TP：239277 FP:464858 TN:13919613 FN:1104892----prec:0.339817-----Tacc:0.900198\n",
      "223806-11 15:51:19 | Step: 64931( 29/ 44) LR:0.000030 TLoss:0.443460 VLoss:0.479258 VAcc:0.926641\n",
      "('learn_step=', 64931)\n",
      " | TP：223108 FP:390351 TN:13994120 FN:1121061----prec:0.363689-----Tacc:0.903907\n",
      "223806-11 16:44:45 | Step: 67170( 30/ 44) LR:0.000030 TLoss:0.440744 VLoss:0.481291 VAcc:0.923732\n",
      "('learn_step=', 67170)\n",
      " | TP：235159 FP:470176 TN:13914295 FN:1109010----prec:0.333400-----Tacc:0.899598\n",
      "223806-11 17:37:29 | Step: 69409( 31/ 44) LR:0.000030 TLoss:0.438175 VLoss:0.478214 VAcc:0.926235\n",
      "('learn_step=', 69409)\n",
      " | TP：230986 FP:382358 TN:14002113 FN:1113183----prec:0.376601-----Tacc:0.904916\n",
      "223806-11 18:30:19 | Step: 71648( 32/ 44) LR:0.000030 TLoss:0.435761 VLoss:0.474870 VAcc:0.926504\n",
      "('learn_step=', 71648)\n",
      " | TP：229534 FP:373698 TN:14010773 FN:1114635----prec:0.380507-----Tacc:0.905374\n",
      "223806-11 19:23:23 | Step: 73887( 33/ 44) LR:0.000030 TLoss:0.433487 VLoss:0.475464 VAcc:0.922297\n",
      "('learn_step=', 73887)\n",
      " | TP：244932 FP:499000 TN:13885471 FN:1099237----prec:0.329240-----Tacc:0.898387\n",
      "223806-11 20:16:21 | Step: 76126( 34/ 44) LR:0.000030 TLoss:0.431371 VLoss:0.478381 VAcc:0.921654\n",
      "('learn_step=', 76126)\n",
      " | TP：227402 FP:502770 TN:13881701 FN:1116767----prec:0.311436-----Tacc:0.897033\n",
      "223806-11 21:09:50 | Step: 78365( 35/ 44) LR:0.000030 TLoss:0.429406 VLoss:0.475411 VAcc:0.922484\n",
      "('learn_step=', 78365)\n",
      " | TP：232383 FP:472937 TN:13911534 FN:1111786----prec:0.329472-----Tacc:0.899246\n",
      "223806-11 22:02:55 | Step: 80604( 36/ 44) LR:0.000030 TLoss:0.427672 VLoss:0.471953 VAcc:0.924985\n",
      "('learn_step=', 80604)\n",
      " | TP：227710 FP:499804 TN:13884667 FN:1116459----prec:0.312997-----Tacc:0.897241\n",
      "223806-11 22:56:08 | Step: 82843( 37/ 44) LR:0.000030 TLoss:0.425966 VLoss:0.470704 VAcc:0.930432\n",
      "('learn_step=', 82843)\n",
      " | TP：218417 FP:325553 TN:14058918 FN:1125752----prec:0.401524-----Tacc:0.907729\n",
      "223806-11 23:49:15 | Step: 85082( 38/ 44) LR:0.000030 TLoss:0.424490 VLoss:0.473437 VAcc:0.921873\n",
      "('learn_step=', 85082)\n",
      " | TP：222895 FP:497494 TN:13886977 FN:1121274----prec:0.309409-----Tacc:0.897082\n",
      "223806-12 00:42:27 | Step: 87321( 39/ 44) LR:0.000030 TLoss:0.423024 VLoss:0.469669 VAcc:0.926908\n",
      "('learn_step=', 87321)\n",
      " | TP：215372 FP:432182 TN:13952289 FN:1128797----prec:0.332593-----Tacc:0.900756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223806-12 01:35:52 | Step: 89560( 40/ 44) LR:0.000030 TLoss:0.421772 VLoss:0.466346 VAcc:0.928970\n",
      "('learn_step=', 89560)\n",
      " | TP：209637 FP:360305 TN:14024166 FN:1134532----prec:0.367822-----Tacc:0.904961\n",
      "223806-12 02:29:07 | Step: 91799( 41/ 44) LR:0.000030 TLoss:0.420680 VLoss:0.468464 VAcc:0.926008\n",
      "('learn_step=', 91799)\n",
      " | TP：216008 FP:412336 TN:13972135 FN:1128161----prec:0.343773-----Tacc:0.902058\n",
      "223806-12 03:22:08 | Step: 94038( 42/ 44) LR:0.000030 TLoss:0.419673 VLoss:0.469581 VAcc:0.927917\n",
      "('learn_step=', 94038)\n",
      " | TP：215049 FP:431283 TN:13953188 FN:1129120----prec:0.332722-----Tacc:0.900792\n",
      "223806-12 04:14:47 | Step: 96277( 43/ 44) LR:0.000030 TLoss:0.418850 VLoss:0.466866 VAcc:0.929807\n",
      "('learn_step=', 96277)\n",
      " | TP：206728 FP:327885 TN:14056586 FN:1137441----prec:0.386687-----Tacc:0.906837\n",
      "223806-12 05:08:13 | Step: 98516( 44/ 44) LR:0.000030 TLoss:0.418047 VLoss:0.469915 VAcc:0.929191\n",
      "('learn_step=', 98516)\n",
      " | TP：201134 FP:354275 TN:14030196 FN:1143035----prec:0.362137-----Tacc:0.904804\n",
      "223806-12 06:01:15 | Step:100755( 45/ 44) LR:0.000030 TLoss:0.417459 VLoss:0.466755 VAcc:0.928212\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 100000\n",
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "    vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "#         print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "#             print('batch_x1:',batch_x1.shape)\n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            \n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        vali_loss = 0\n",
    "        vali_accuracy = 0\n",
    "        vali_batches = vx.shape[0]//batch_size\n",
    "#         print()\n",
    "        \n",
    "#         print('vali_batches=',vali_batches)\n",
    "        for j in range(vali_batches):\n",
    "#             print('\\rj: %d'%j)\n",
    "            select = next(vnb)\n",
    "            batch_X = vx[select]\n",
    "            batch_Y  = vy[select]\n",
    "            batch_X1 = vx1[select]\n",
    "            batch_X=np.multiply(batch_X,1.0/mx)\n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_Y))\n",
    "\n",
    "            vali_loss += sess.run(loss, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "            vali_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "        vali_loss /= vali_batches\n",
    "        vali_accuracy /= vali_batches\n",
    "        \n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "       \n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TLoss:%.6f VLoss:%f VAcc:%f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, vali_loss, vali_accuracy))\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        print('learn_step=',learn_step)\n",
    "        if learn_step % int(show_freq * train_batches)==0:\n",
    "            \n",
    "            \n",
    "            TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "            #TP1=0;FP1=0\n",
    "            num_images=batch_size\n",
    "            n_chunks=np.shape(tx)[0]//batch_size\n",
    "            tAcc=np.zeros(n_chunks)\n",
    "\n",
    "            for chunk in range(0,n_chunks):               \n",
    "                tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx1_batch=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=conv_mask_gt(ty_batch)\n",
    "                tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "                a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "                TP+=a; FP+=b;TN+=c; FN+=d\n",
    "            \n",
    "            prec=metrics(TP,FP,TN,FN)            \n",
    "            test_accuracy=np.mean(tAcc)\n",
    "       \n",
    "            print(' | TP：%d FP:%d TN:%d FN:%d----prec:%.6f-----Tacc:%.6f' % (TP, FP, TN, FN, prec, test_accuracy))\n",
    "            \n",
    "            if prec > 0.45 :\n",
    "                best_prec = prec\n",
    "                save_path=saver.save(sess,'../model_s_gaijin3/final_model_nist.ckpt')\n",
    "                print (\"Best Model Found on NC16...\")\n",
    "                print ( \"prec = \"+str(prec) + \", acc = \"+ str(test_accuracy))\n",
    "\n",
    "            saver.save(sess, '../model_s_gaijin3/modelS%d.ckpt' % learn_step)\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_s_gaijin3/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  VGGAR_no_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-25T07:21:49.736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_epoch= 497\n",
      "2014\n",
      "learn_step= 2015\n",
      "07-25 15:58:28 | Step:  2015(  1/497) LR:0.000030 TrainLoss:0.616731 ValiLoss:0.597109 TrainAcc:0.746071\n",
      " | TP：14925079 FP:59139493 TN:360222831 FN:4148437----prec:0.201514-----Tacc:0.855654\n",
      "2014\n",
      "learn_step= 4030\n",
      "07-25 16:34:50 | Step:  4030(  2/497) LR:0.000030 TrainLoss:0.595716 ValiLoss:0.588660 TrainAcc:0.895656\n",
      " | TP：10989560 FP:15379091 TN:403983233 FN:8083956----prec:0.416766-----Tacc:0.946485\n",
      "2014\n",
      "learn_step= 6045\n",
      "07-25 17:11:29 | Step:  6045(  3/497) LR:0.000030 TrainLoss:0.588066 ValiLoss:0.581384 TrainAcc:0.949847\n",
      " | TP：10716981 FP:10080495 TN:409281829 FN:8356535----prec:0.515302-----Tacc:0.957949\n",
      "2014\n",
      "learn_step= 8060\n",
      "07-25 17:49:22 | Step:  8060(  4/497) LR:0.000030 TrainLoss:0.581102 ValiLoss:0.574486 TrainAcc:0.960646\n",
      " | TP：11008309 FP:8590087 TN:410772237 FN:8065207----prec:0.561694-----Tacc:0.962012\n",
      "Best Model Found on NC16...\n",
      "prec = 0.5616943856017576, acc = 0.9620122064924026\n",
      "2014\n",
      "learn_step= 10075\n",
      "07-25 18:30:13 | Step: 10075(  5/497) LR:0.000030 TrainLoss:0.574150 ValiLoss:0.567040 TrainAcc:0.966088\n",
      " | TP：10854546 FP:6971023 TN:412391301 FN:8218970----prec:0.608931-----Tacc:0.965354\n",
      "Best Model Found on NC16...\n",
      "prec = 0.6089312492633441, acc = 0.9653541878734468\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "2014\n",
      "learn_step= 12090\n",
      "07-25 19:12:07 | Step: 12090(  6/497) LR:0.000030 TrainLoss:0.567364 ValiLoss:0.560217 TrainAcc:0.968668\n",
      " | TP：11063488 FP:6296874 TN:413065450 FN:8010028----prec:0.637284-----Tacc:0.967368\n",
      "Best Model Found on NC16...\n",
      "prec = 0.637284406857411, acc = 0.9673682766644944\n",
      "2014\n",
      "learn_step= 14105\n",
      "07-25 19:57:44 | Step: 14105(  7/497) LR:0.000030 TrainLoss:0.560518 ValiLoss:0.553366 TrainAcc:0.971349\n",
      " | TP：11501999 FP:5780622 TN:413581702 FN:7571517----prec:0.665524-----Tacc:0.969546\n",
      "Best Model Found on NC16...\n",
      "prec = 0.6655239966206475, acc = 0.9695460392755243\n",
      "2014\n",
      "learn_step= 16120\n",
      "07-25 20:47:47 | Step: 16120(  8/497) LR:0.000030 TrainLoss:0.553756 ValiLoss:0.546156 TrainAcc:0.973260\n",
      " | TP：11795780 FP:5725249 TN:413637075 FN:7277736----prec:0.673236-----Tacc:0.970342\n",
      "Best Model Found on NC16...\n",
      "prec = 0.6732355730933345, acc = 0.9703423605371484\n",
      "2014\n",
      "learn_step= 18135\n",
      "07-25 21:42:44 | Step: 18135(  9/497) LR:0.000030 TrainLoss:0.547074 ValiLoss:0.538685 TrainAcc:0.974748\n",
      " | TP：11624778 FP:5000014 TN:414362310 FN:7448738----prec:0.699244-----Tacc:0.971606\n",
      "Best Model Found on NC16...\n",
      "prec = 0.6992435153474359, acc = 0.971606428045863\n",
      "2014\n",
      "learn_step= 20150\n",
      "07-25 22:36:36 | Step: 20150( 10/497) LR:0.000030 TrainLoss:0.540473 ValiLoss:0.532035 TrainAcc:0.976144\n",
      " | TP：11670681 FP:4291201 TN:415071123 FN:7402835----prec:0.731159-----Tacc:0.973328\n",
      "Best Model Found on NC16...\n",
      "prec = 0.731159458514975, acc = 0.9733278192746799\n",
      "2014\n",
      "learn_step= 22165\n",
      "07-25 23:31:42 | Step: 22165( 11/497) LR:0.000030 TrainLoss:0.534117 ValiLoss:0.525783 TrainAcc:0.977060\n",
      " | TP：12072340 FP:4663713 TN:414698611 FN:7001176----prec:0.721337-----Tacc:0.973394\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7213373428011925, acc = 0.9733943153389901\n",
      "2014\n",
      "learn_step= 24180\n",
      "07-26 00:26:52 | Step: 24180( 12/497) LR:0.000030 TrainLoss:0.527748 ValiLoss:0.518314 TrainAcc:0.978053\n",
      " | TP：11755972 FP:4333878 TN:415028446 FN:7317544----prec:0.730645-----Tacc:0.973425\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7306452204339957, acc = 0.9734250250953196\n",
      "2014\n",
      "learn_step= 26195\n",
      "07-26 01:20:26 | Step: 26195( 13/497) LR:0.000030 TrainLoss:0.521656 ValiLoss:0.512262 TrainAcc:0.978927\n",
      " | TP：12428795 FP:4482910 TN:414879414 FN:6644721----prec:0.734923-----Tacc:0.974620\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7349226467703833, acc = 0.9746197023733849\n",
      "2014\n",
      "learn_step= 28210\n",
      "07-26 02:15:35 | Step: 28210( 14/497) LR:0.000030 TrainLoss:0.515801 ValiLoss:0.505841 TrainAcc:0.979618\n",
      " | TP：11684158 FP:3920740 TN:415441584 FN:7389358----prec:0.748749-----Tacc:0.974204\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7487493990668779, acc = 0.9742035218953017\n",
      "2014\n",
      "learn_step= 30225\n",
      "07-26 03:10:04 | Step: 30225( 15/497) LR:0.000030 TrainLoss:0.510070 ValiLoss:0.499954 TrainAcc:0.980195\n",
      " | TP：12422138 FP:3915708 TN:415446616 FN:6651378----prec:0.760329-----Tacc:0.975898\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7603289931855107, acc = 0.9758981613300306\n",
      "2014\n",
      "learn_step= 32240\n",
      "07-26 04:04:58 | Step: 32240( 16/497) LR:0.000030 TrainLoss:0.504459 ValiLoss:0.493485 TrainAcc:0.980851\n",
      " | TP：12422087 FP:3869526 TN:415492798 FN:6651429----prec:0.762484-----Tacc:0.976003\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7624835551887909, acc = 0.9760034327549785\n",
      "2014\n",
      "learn_step= 34255\n",
      "07-26 04:58:34 | Step: 34255( 17/497) LR:0.000030 TrainLoss:0.499161 ValiLoss:0.487436 TrainAcc:0.981318\n",
      " | TP：12393105 FP:3741083 TN:415621241 FN:6680411----prec:0.768127-----Tacc:0.976230\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7681269736041209, acc = 0.9762302864292812\n",
      "2014\n",
      "learn_step= 36270\n",
      "07-26 05:53:50 | Step: 36270( 18/497) LR:0.000030 TrainLoss:0.493950 ValiLoss:0.481676 TrainAcc:0.981787\n",
      " | TP：13442516 FP:4220525 TN:415141799 FN:5631000----prec:0.761053-----Tacc:0.977530\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7610533203200923, acc = 0.9775302936677975\n",
      "2014\n",
      "learn_step= 38285\n",
      "07-26 06:49:41 | Step: 38285( 19/497) LR:0.000030 TrainLoss:0.488979 ValiLoss:0.476838 TrainAcc:0.982160\n",
      " | TP：12374927 FP:3572380 TN:415789944 FN:6698589----prec:0.775989-----Tacc:0.976574\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7759885101603626, acc = 0.9765736126578977\n",
      "2014\n",
      "learn_step= 40300\n",
      "07-26 07:44:36 | Step: 40300( 20/497) LR:0.000030 TrainLoss:0.484250 ValiLoss:0.471044 TrainAcc:0.982484\n",
      " | TP：12915459 FP:3999078 TN:415363246 FN:6158057----prec:0.763572-----Tacc:0.976833\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7635715361289477, acc = 0.9768332678640904\n",
      "2014\n",
      "learn_step= 42315\n",
      "07-26 08:39:02 | Step: 42315( 21/497) LR:0.000030 TrainLoss:0.479536 ValiLoss:0.466476 TrainAcc:0.982902\n",
      " | TP：12405024 FP:3347131 TN:416015193 FN:6668492----prec:0.787513-----Tacc:0.977156\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7875128196745094, acc = 0.9771560248772659\n",
      "2014\n",
      "learn_step= 44330\n",
      "07-26 09:34:38 | Step: 44330( 22/497) LR:0.000030 TrainLoss:0.475225 ValiLoss:0.460663 TrainAcc:0.983228\n",
      " | TP：12909127 FP:3310008 TN:416052316 FN:6164389----prec:0.795920-----Tacc:0.978390\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7959195727762252, acc = 0.9783904536422593\n",
      "2014\n",
      "learn_step= 46345\n",
      "07-26 10:29:32 | Step: 46345( 23/497) LR:0.000030 TrainLoss:0.470842 ValiLoss:0.455716 TrainAcc:0.983503\n",
      " | TP：13279682 FP:3608360 TN:415753964 FN:5793834----prec:0.786336-----Tacc:0.978555\n",
      "Best Model Found on NC16...\n",
      "prec = 0.786336391157715, acc = 0.9785551423449153\n",
      "2014\n",
      "learn_step= 48360\n",
      "07-26 11:24:41 | Step: 48360( 24/497) LR:0.000030 TrainLoss:0.466763 ValiLoss:0.452296 TrainAcc:0.983774\n",
      " | TP：12895922 FP:3372555 TN:415989769 FN:6177594----prec:0.792694-----Tacc:0.978218\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7926938704833846, acc = 0.9782176798234606\n",
      "2014\n",
      "learn_step= 50375\n",
      "07-26 12:19:59 | Step: 50375( 25/497) LR:0.000030 TrainLoss:0.462819 ValiLoss:0.446952 TrainAcc:0.983986\n",
      " | TP：13257199 FP:3315081 TN:416047243 FN:5816317----prec:0.799962-----Tacc:0.979173\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7999622864204514, acc = 0.9791727833149144\n",
      "2014\n",
      "learn_step= 52390\n",
      "07-26 13:15:02 | Step: 52390( 26/497) LR:0.000030 TrainLoss:0.459069 ValiLoss:0.442448 TrainAcc:0.984248\n",
      " | TP：13679454 FP:3747179 TN:415615145 FN:5394062----prec:0.784974-----Tacc:0.979150\n",
      "Best Model Found on NC16...\n",
      "prec = 0.7849740107569788, acc = 0.9791503310738123\n",
      "2014\n",
      "learn_step= 54405\n",
      "07-26 14:10:21 | Step: 54405( 27/497) LR:0.000030 TrainLoss:0.455482 ValiLoss:0.437750 TrainAcc:0.984467\n",
      " | TP：13988342 FP:3495197 TN:415867127 FN:5085174----prec:0.800086-----Tacc:0.980430\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8000864127108316, acc = 0.9804295833335329\n",
      "2014\n",
      "learn_step= 56420\n",
      "07-26 15:05:31 | Step: 56420( 28/497) LR:0.000030 TrainLoss:0.452014 ValiLoss:0.434230 TrainAcc:0.984659\n",
      " | TP：13532555 FP:3270683 TN:416091641 FN:5540961----prec:0.805354-----Tacc:0.979902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Found on NC16...\n",
      "prec = 0.8053540037937877, acc = 0.9799020886421204\n",
      "2014\n",
      "learn_step= 58435\n",
      "07-26 16:00:07 | Step: 58435( 29/497) LR:0.000030 TrainLoss:0.448730 ValiLoss:0.431149 TrainAcc:0.984842\n",
      " | TP：13418484 FP:3183421 TN:416178903 FN:5655032----prec:0.808250-----Tacc:0.979841\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8082496556870985, acc = 0.9798409524519882\n",
      "2014\n",
      "learn_step= 60450\n",
      "07-26 16:55:07 | Step: 60450( 30/497) LR:0.000030 TrainLoss:0.445583 ValiLoss:0.427691 TrainAcc:0.985026\n",
      " | TP：13612162 FP:3331992 TN:416030332 FN:5461354----prec:0.803354-----Tacc:0.979944\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8033544784826625, acc = 0.9799438281978727\n",
      "2014\n",
      "learn_step= 62465\n",
      "07-26 17:50:13 | Step: 62465( 31/497) LR:0.000030 TrainLoss:0.442603 ValiLoss:0.424505 TrainAcc:0.985169\n",
      " | TP：13813421 FP:3367539 TN:415994785 FN:5260095----prec:0.803996-----Tacc:0.980322\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8039958768310921, acc = 0.9803217873979577\n",
      "2014\n",
      "learn_step= 64480\n",
      "07-26 18:45:43 | Step: 64480( 32/497) LR:0.000030 TrainLoss:0.439792 ValiLoss:0.421383 TrainAcc:0.985360\n",
      " | TP：13937872 FP:3116687 TN:416245637 FN:5135644----prec:0.817252-----Tacc:0.981178\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8172519735045578, acc = 0.9811777924208364\n",
      "2014\n",
      "learn_step= 66495\n",
      "07-26 19:41:26 | Step: 66495( 33/497) LR:0.000030 TrainLoss:0.436978 ValiLoss:0.417895 TrainAcc:0.985519\n",
      " | TP：13845958 FP:3078732 TN:416283592 FN:5227558----prec:0.818092-----Tacc:0.981055\n",
      "Best Model Found on NC16...\n",
      "prec = 0.818092266387149, acc = 0.9810547179170788\n",
      "2014\n",
      "learn_step= 68510\n",
      "07-26 20:36:47 | Step: 68510( 34/497) LR:0.000030 TrainLoss:0.434362 ValiLoss:0.416338 TrainAcc:0.985691\n",
      " | TP：13216427 FP:2760944 TN:416601380 FN:5857089----prec:0.827197-----Tacc:0.980344\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8271966019941527, acc = 0.9803436844872787\n",
      "2014\n",
      "learn_step= 70525\n",
      "07-26 21:31:47 | Step: 70525( 35/497) LR:0.000030 TrainLoss:0.431900 ValiLoss:0.413802 TrainAcc:0.985827\n",
      " | TP：13686331 FP:3144529 TN:416217795 FN:5387185----prec:0.813169-----Tacc:0.980541\n",
      "Best Model Found on NC16...\n",
      "prec = 0.8131688457987244, acc = 0.9805405642419652\n",
      "1548"
     ]
    }
   ],
   "source": [
    "# batch_x=np.multiply(batch_x,1.0/mx)#####   测试集的数据忘记除以127了\n",
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 1000000\n",
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "#     vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "#         print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "            \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y})\n",
    "            train_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_x, y: rev_batch_y})\n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        \n",
    "        train_accuracy /= train_batches\n",
    "#         vali_loss = 0\n",
    "#         vali_accuracy = 0\n",
    "#         vali_batches = vx.shape[0]//batch_size\n",
    "        \n",
    "        \n",
    "#         print('vali_batches=',vali_batches)\n",
    "#         for j in range(vali_batches):\n",
    "# #             print('\\rj: %d'%j)\n",
    "#             select = next(vnb)\n",
    "#             batch_X = vx[select]\n",
    "#             batch_Y  = vy[select]\n",
    "#             batch_X1 = vx1[select]\n",
    "#             batch_X=np.multiply(batch_X,1.0/mx)\n",
    "#             rev_batch_y=np.array(conv_mask_gt(batch_Y))\n",
    "\n",
    "#             vali_loss += sess.run(loss, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "#             vali_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_X, y: rev_batch_y, freqFeat: batch_X1})\n",
    "#         vali_loss /= vali_batches\n",
    "#         vali_accuracy /= vali_batches\n",
    "        \n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        \n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        print('learn_step=',learn_step)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if learn_step % int(show_freq * train_batches)==0:\n",
    "            \n",
    "            \n",
    "            TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "            #TP1=0;FP1=0\n",
    "            num_images=batch_size\n",
    "            n_chunks=np.shape(tx)[0]//batch_size\n",
    "            tAcc=np.zeros(n_chunks)\n",
    "            vali_loss = 0\n",
    "            for chunk in range(0,n_chunks):               \n",
    "                tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx_batch=np.multiply(tx_batch,1.0/mx)\n",
    "                ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                tx1_batch=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "                ty_batch=conv_mask_gt(ty_batch)\n",
    "                \n",
    "                vali_loss += sess.run(loss, feed_dict={input_layer: tx_batch, y:ty_batch})\n",
    "        \n",
    "                \n",
    "                tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch})\n",
    "                a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "                TP+=a; FP+=b;TN+=c; FN+=d\n",
    "            vali_loss /= n_chunks\n",
    "            prec=metrics(TP,FP,TN,FN)            \n",
    "            test_accuracy=np.mean(tAcc)\n",
    "            print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TrainLoss:%.6f ValiLoss:%f TrainAcc:%.6f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, vali_loss,train_accuracy))\n",
    "\n",
    "            print(' | TP：%d FP:%d TN:%d FN:%d----prec:%.6f-----Tacc:%.6f' % (TP, FP, TN, FN, prec, test_accuracy))\n",
    "            \n",
    "            if prec > 0.55 :\n",
    "                best_prec = prec\n",
    "                save_path=saver.save(sess,'../model_s_gaijin1/final_model_nist.ckpt')\n",
    "                print (\"Best Model Found on NC16...\")\n",
    "                print ( \"prec = \"+str(prec) + \", acc = \"+ str(test_accuracy))\n",
    "\n",
    "            saver.save(sess, '../model_s_gaijin1/modelS%d.ckpt' % learn_step)\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_s_gaijin1/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIST2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T09:46:21.716166Z",
     "start_time": "2019-07-03T09:13:52.205588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img_NC16_img.shape= (798, 256, 256, 3)\n",
      "Img_NC16.shape= (798, 256, 256, 3)\n",
      "INFO:tensorflow:Restoring parameters from ../model_s_gaijin4/modelS251875.ckpt\n",
      "session starting .................!!!!\n",
      "total_epoch= 120\n",
      "24\n",
      "07-03 17:14:35 | Step:    25(  1/120) LR:0.000030 TrainLoss:0.443959 TrainAcc:0.933720\n",
      "24\n",
      "07-03 17:14:51 | Step:    50(  2/120) LR:0.000030 TrainLoss:0.424094 TrainAcc:0.938031\n",
      "24\n",
      "07-03 17:15:06 | Step:    75(  3/120) LR:0.000030 TrainLoss:0.415357 TrainAcc:0.943204\n",
      "24\n",
      "07-03 17:15:22 | Step:   100(  4/120) LR:0.000030 TrainLoss:0.410943 TrainAcc:0.946128\n",
      "24\n",
      "07-03 17:15:38 | Step:   125(  5/120) LR:0.000030 TrainLoss:0.405904 TrainAcc:0.949611\n",
      "24\n",
      "07-03 17:15:53 | Step:   150(  6/120) LR:0.000030 TrainLoss:0.401995 TrainAcc:0.955328\n",
      "24\n",
      "07-03 17:16:09 | Step:   175(  7/120) LR:0.000030 TrainLoss:0.397012 TrainAcc:0.961794\n",
      "24\n",
      "07-03 17:16:25 | Step:   200(  8/120) LR:0.000030 TrainLoss:0.395596 TrainAcc:0.964764\n",
      "24\n",
      "07-03 17:16:41 | Step:   225(  9/120) LR:0.000030 TrainLoss:0.393501 TrainAcc:0.968056\n",
      "24\n",
      "07-03 17:16:57 | Step:   250( 10/120) LR:0.000030 TrainLoss:0.390894 TrainAcc:0.970261\n",
      "24\n",
      "07-03 17:17:13 | Step:   275( 11/120) LR:0.000030 TrainLoss:0.389541 TrainAcc:0.971922\n",
      "24\n",
      "07-03 17:17:29 | Step:   300( 12/120) LR:0.000030 TrainLoss:0.388185 TrainAcc:0.974055\n",
      "24\n",
      "07-03 17:17:45 | Step:   325( 13/120) LR:0.000030 TrainLoss:0.387188 TrainAcc:0.975467\n",
      "24\n",
      "07-03 17:18:00 | Step:   350( 14/120) LR:0.000030 TrainLoss:0.387164 TrainAcc:0.976189\n",
      "24\n",
      "07-03 17:18:17 | Step:   375( 15/120) LR:0.000030 TrainLoss:0.385591 TrainAcc:0.978027\n",
      "24\n",
      "07-03 17:18:32 | Step:   400( 16/120) LR:0.000030 TrainLoss:0.385246 TrainAcc:0.979070\n",
      "24\n",
      "07-03 17:18:48 | Step:   425( 17/120) LR:0.000030 TrainLoss:0.384528 TrainAcc:0.980138\n",
      "24\n",
      "07-03 17:19:04 | Step:   450( 18/120) LR:0.000030 TrainLoss:0.384105 TrainAcc:0.980770\n",
      "24\n",
      "07-03 17:19:20 | Step:   475( 19/120) LR:0.000030 TrainLoss:0.383630 TrainAcc:0.981608\n",
      "24\n",
      "07-03 17:19:36 | Step:   500( 20/120) LR:0.000030 TrainLoss:0.383174 TrainAcc:0.982397\n",
      "24\n",
      "07-03 17:19:51 | Step:   525( 21/120) LR:0.000030 TrainLoss:0.383539 TrainAcc:0.982620\n",
      "24\n",
      "07-03 17:20:07 | Step:   550( 22/120) LR:0.000030 TrainLoss:0.381711 TrainAcc:0.983854\n",
      "24\n",
      "07-03 17:20:23 | Step:   575( 23/120) LR:0.000030 TrainLoss:0.381675 TrainAcc:0.983789\n",
      "24\n",
      "07-03 17:20:39 | Step:   600( 24/120) LR:0.000030 TrainLoss:0.380342 TrainAcc:0.985701\n",
      "24\n",
      "07-03 17:20:55 | Step:   625( 25/120) LR:0.000030 TrainLoss:0.380187 TrainAcc:0.986028\n",
      "24\n",
      "07-03 17:21:11 | Step:   650( 26/120) LR:0.000030 TrainLoss:0.381413 TrainAcc:0.984605\n",
      "24\n",
      "07-03 17:21:27 | Step:   675( 27/120) LR:0.000030 TrainLoss:0.380631 TrainAcc:0.985518\n",
      "24\n",
      "07-03 17:21:43 | Step:   700( 28/120) LR:0.000030 TrainLoss:0.381978 TrainAcc:0.983903\n",
      "24\n",
      "07-03 17:21:58 | Step:   725( 29/120) LR:0.000030 TrainLoss:0.380522 TrainAcc:0.985888\n",
      "24\n",
      "07-03 17:22:14 | Step:   750( 30/120) LR:0.000030 TrainLoss:0.379751 TrainAcc:0.986762\n",
      "24\n",
      "07-03 17:22:30 | Step:   775( 31/120) LR:0.000030 TrainLoss:0.379565 TrainAcc:0.986499\n",
      "24\n",
      "07-03 17:22:46 | Step:   800( 32/120) LR:0.000030 TrainLoss:0.379157 TrainAcc:0.986696\n",
      "24\n",
      "07-03 17:23:02 | Step:   825( 33/120) LR:0.000030 TrainLoss:0.379219 TrainAcc:0.986566\n",
      "24\n",
      "07-03 17:23:17 | Step:   850( 34/120) LR:0.000030 TrainLoss:0.379005 TrainAcc:0.987348\n",
      "24\n",
      "07-03 17:23:33 | Step:   875( 35/120) LR:0.000030 TrainLoss:0.380283 TrainAcc:0.986392\n",
      "24\n",
      "07-03 17:23:49 | Step:   900( 36/120) LR:0.000030 TrainLoss:0.378421 TrainAcc:0.987368\n",
      "24\n",
      "07-03 17:24:05 | Step:   925( 37/120) LR:0.000030 TrainLoss:0.379765 TrainAcc:0.986356\n",
      "24\n",
      "07-03 17:24:21 | Step:   950( 38/120) LR:0.000030 TrainLoss:0.378851 TrainAcc:0.987184\n",
      "24\n",
      "07-03 17:24:37 | Step:   975( 39/120) LR:0.000030 TrainLoss:0.378730 TrainAcc:0.987609\n",
      "24\n",
      "07-03 17:24:53 | Step:  1000( 40/120) LR:0.000030 TrainLoss:0.380476 TrainAcc:0.986218\n",
      "24\n",
      "07-03 17:25:09 | Step:  1025( 41/120) LR:0.000030 TrainLoss:0.378519 TrainAcc:0.987432\n",
      "24\n",
      "07-03 17:25:24 | Step:  1050( 42/120) LR:0.000030 TrainLoss:0.377830 TrainAcc:0.988323\n",
      "24\n",
      "07-03 17:25:40 | Step:  1075( 43/120) LR:0.000030 TrainLoss:0.377728 TrainAcc:0.988395\n",
      "24\n",
      "07-03 17:25:56 | Step:  1100( 44/120) LR:0.000030 TrainLoss:0.377176 TrainAcc:0.989148\n",
      "24\n",
      "07-03 17:26:12 | Step:  1125( 45/120) LR:0.000030 TrainLoss:0.379032 TrainAcc:0.987490\n",
      "24\n",
      "07-03 17:26:28 | Step:  1150( 46/120) LR:0.000030 TrainLoss:0.378403 TrainAcc:0.987825\n",
      "24\n",
      "07-03 17:26:43 | Step:  1175( 47/120) LR:0.000030 TrainLoss:0.378458 TrainAcc:0.987960\n",
      "24\n",
      "07-03 17:27:00 | Step:  1200( 48/120) LR:0.000030 TrainLoss:0.378330 TrainAcc:0.987979\n",
      "24\n",
      "07-03 17:27:15 | Step:  1225( 49/120) LR:0.000030 TrainLoss:0.378803 TrainAcc:0.987524\n",
      "24\n",
      "07-03 17:27:31 | Step:  1250( 50/120) LR:0.000030 TrainLoss:0.378367 TrainAcc:0.987496\n",
      "24\n",
      "07-03 17:27:47 | Step:  1275( 51/120) LR:0.000030 TrainLoss:0.376674 TrainAcc:0.989249\n",
      "24\n",
      "07-03 17:28:02 | Step:  1300( 52/120) LR:0.000030 TrainLoss:0.377376 TrainAcc:0.988952\n",
      "24\n",
      "07-03 17:28:19 | Step:  1325( 53/120) LR:0.000030 TrainLoss:0.377333 TrainAcc:0.988702\n",
      "24\n",
      "07-03 17:28:34 | Step:  1350( 54/120) LR:0.000030 TrainLoss:0.377890 TrainAcc:0.988311\n",
      "24\n",
      "07-03 17:28:50 | Step:  1375( 55/120) LR:0.000030 TrainLoss:0.376877 TrainAcc:0.989039\n",
      "24\n",
      "07-03 17:29:06 | Step:  1400( 56/120) LR:0.000030 TrainLoss:0.377635 TrainAcc:0.988844\n",
      "24\n",
      "07-03 17:29:22 | Step:  1425( 57/120) LR:0.000030 TrainLoss:0.377199 TrainAcc:0.989031\n",
      "24\n",
      "07-03 17:29:38 | Step:  1450( 58/120) LR:0.000030 TrainLoss:0.377204 TrainAcc:0.988807\n",
      "24\n",
      "07-03 17:29:53 | Step:  1475( 59/120) LR:0.000030 TrainLoss:0.377842 TrainAcc:0.988605\n",
      "24\n",
      "07-03 17:30:09 | Step:  1500( 60/120) LR:0.000030 TrainLoss:0.376671 TrainAcc:0.989244\n",
      "24\n",
      "07-03 17:30:25 | Step:  1525( 61/120) LR:0.000030 TrainLoss:0.378506 TrainAcc:0.988265\n",
      "24\n",
      "07-03 17:30:41 | Step:  1550( 62/120) LR:0.000030 TrainLoss:0.375575 TrainAcc:0.990434\n",
      "24\n",
      "07-03 17:30:57 | Step:  1575( 63/120) LR:0.000030 TrainLoss:0.377580 TrainAcc:0.988760\n",
      "24\n",
      "07-03 17:31:13 | Step:  1600( 64/120) LR:0.000030 TrainLoss:0.376489 TrainAcc:0.989674\n",
      "24\n",
      "07-03 17:31:29 | Step:  1625( 65/120) LR:0.000030 TrainLoss:0.376745 TrainAcc:0.989467\n",
      "24\n",
      "07-03 17:31:45 | Step:  1650( 66/120) LR:0.000030 TrainLoss:0.375902 TrainAcc:0.990025\n",
      "24\n",
      "07-03 17:32:00 | Step:  1675( 67/120) LR:0.000030 TrainLoss:0.376536 TrainAcc:0.989572\n",
      "24\n",
      "07-03 17:32:16 | Step:  1700( 68/120) LR:0.000030 TrainLoss:0.376459 TrainAcc:0.989993\n",
      "24\n",
      "07-03 17:32:32 | Step:  1725( 69/120) LR:0.000030 TrainLoss:0.375912 TrainAcc:0.990155\n",
      "24\n",
      "07-03 17:32:48 | Step:  1750( 70/120) LR:0.000030 TrainLoss:0.375382 TrainAcc:0.990831\n",
      "24\n",
      "07-03 17:33:04 | Step:  1775( 71/120) LR:0.000030 TrainLoss:0.376384 TrainAcc:0.989756\n",
      "24\n",
      "07-03 17:33:20 | Step:  1800( 72/120) LR:0.000030 TrainLoss:0.378332 TrainAcc:0.987953\n",
      "24\n",
      "07-03 17:33:36 | Step:  1825( 73/120) LR:0.000030 TrainLoss:0.375852 TrainAcc:0.990208\n",
      "24\n",
      "07-03 17:33:51 | Step:  1850( 74/120) LR:0.000030 TrainLoss:0.376280 TrainAcc:0.990038\n",
      "24\n",
      "07-03 17:34:07 | Step:  1875( 75/120) LR:0.000030 TrainLoss:0.376408 TrainAcc:0.989759\n",
      "24\n",
      "07-03 17:34:23 | Step:  1900( 76/120) LR:0.000030 TrainLoss:0.375407 TrainAcc:0.990726\n",
      "24\n",
      "07-03 17:34:39 | Step:  1925( 77/120) LR:0.000030 TrainLoss:0.375977 TrainAcc:0.990291\n",
      "24\n",
      "07-03 17:34:55 | Step:  1950( 78/120) LR:0.000030 TrainLoss:0.374881 TrainAcc:0.991163\n",
      "24\n",
      "07-03 17:35:10 | Step:  1975( 79/120) LR:0.000030 TrainLoss:0.375653 TrainAcc:0.990564\n",
      "24\n",
      "07-03 17:35:26 | Step:  2000( 80/120) LR:0.000030 TrainLoss:0.376345 TrainAcc:0.989967\n",
      "24\n",
      "07-03 17:35:42 | Step:  2025( 81/120) LR:0.000030 TrainLoss:0.375415 TrainAcc:0.990721\n",
      "24\n",
      "07-03 17:35:58 | Step:  2050( 82/120) LR:0.000030 TrainLoss:0.374898 TrainAcc:0.990908\n",
      "24\n",
      "07-03 17:36:14 | Step:  2075( 83/120) LR:0.000030 TrainLoss:0.375176 TrainAcc:0.990767\n",
      "24\n",
      "07-03 17:36:30 | Step:  2100( 84/120) LR:0.000030 TrainLoss:0.374950 TrainAcc:0.991280\n",
      "24\n",
      "07-03 17:36:45 | Step:  2125( 85/120) LR:0.000030 TrainLoss:0.375132 TrainAcc:0.991071\n",
      "24\n",
      "07-03 17:37:01 | Step:  2150( 86/120) LR:0.000030 TrainLoss:0.374695 TrainAcc:0.991258\n",
      "24\n",
      "07-03 17:37:17 | Step:  2175( 87/120) LR:0.000030 TrainLoss:0.375336 TrainAcc:0.990967\n",
      "24\n",
      "07-03 17:37:33 | Step:  2200( 88/120) LR:0.000030 TrainLoss:0.375383 TrainAcc:0.990805\n",
      "24\n",
      "07-03 17:37:49 | Step:  2225( 89/120) LR:0.000030 TrainLoss:0.375872 TrainAcc:0.990260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "07-03 17:38:05 | Step:  2250( 90/120) LR:0.000030 TrainLoss:0.374835 TrainAcc:0.991106\n",
      "24\n",
      "07-03 17:38:21 | Step:  2275( 91/120) LR:0.000030 TrainLoss:0.375841 TrainAcc:0.990501\n",
      "24\n",
      "07-03 17:38:37 | Step:  2300( 92/120) LR:0.000030 TrainLoss:0.376310 TrainAcc:0.990176\n",
      "24\n",
      "07-03 17:38:52 | Step:  2325( 93/120) LR:0.000030 TrainLoss:0.376115 TrainAcc:0.990056\n",
      "24\n",
      "07-03 17:39:08 | Step:  2350( 94/120) LR:0.000030 TrainLoss:0.375145 TrainAcc:0.990749\n",
      "24\n",
      "07-03 17:39:24 | Step:  2375( 95/120) LR:0.000030 TrainLoss:0.374466 TrainAcc:0.991417\n",
      "24\n",
      "07-03 17:39:40 | Step:  2400( 96/120) LR:0.000030 TrainLoss:0.374602 TrainAcc:0.991308\n",
      "24\n",
      "07-03 17:39:56 | Step:  2425( 97/120) LR:0.000030 TrainLoss:0.375582 TrainAcc:0.990724\n",
      "24\n",
      "07-03 17:40:12 | Step:  2450( 98/120) LR:0.000030 TrainLoss:0.375103 TrainAcc:0.990885\n",
      "24\n",
      "07-03 17:40:28 | Step:  2475( 99/120) LR:0.000030 TrainLoss:0.374799 TrainAcc:0.991201\n",
      "24\n",
      "07-03 17:40:44 | Step:  2500(100/120) LR:0.000030 TrainLoss:0.376141 TrainAcc:0.990201\n",
      "24\n",
      "07-03 17:41:00 | Step:  2525(101/120) LR:0.000030 TrainLoss:0.375909 TrainAcc:0.990121\n",
      "24\n",
      "07-03 17:41:15 | Step:  2550(102/120) LR:0.000030 TrainLoss:0.374966 TrainAcc:0.991208\n",
      "24\n",
      "07-03 17:41:31 | Step:  2575(103/120) LR:0.000030 TrainLoss:0.374272 TrainAcc:0.991393\n",
      "24\n",
      "07-03 17:41:47 | Step:  2600(104/120) LR:0.000030 TrainLoss:0.375356 TrainAcc:0.990513\n",
      "24\n",
      "07-03 17:42:03 | Step:  2625(105/120) LR:0.000030 TrainLoss:0.374996 TrainAcc:0.990892\n",
      "24\n",
      "07-03 17:42:19 | Step:  2650(106/120) LR:0.000030 TrainLoss:0.375025 TrainAcc:0.990926\n",
      "24\n",
      "07-03 17:42:35 | Step:  2675(107/120) LR:0.000030 TrainLoss:0.376354 TrainAcc:0.990136\n",
      "24\n",
      "07-03 17:42:51 | Step:  2700(108/120) LR:0.000030 TrainLoss:0.374761 TrainAcc:0.991310\n",
      "24\n",
      "07-03 17:43:07 | Step:  2725(109/120) LR:0.000030 TrainLoss:0.376254 TrainAcc:0.990295\n",
      "24\n",
      "07-03 17:43:22 | Step:  2750(110/120) LR:0.000030 TrainLoss:0.374705 TrainAcc:0.991146\n",
      "24\n",
      "07-03 17:43:38 | Step:  2775(111/120) LR:0.000030 TrainLoss:0.376401 TrainAcc:0.989762\n",
      "24\n",
      "07-03 17:43:54 | Step:  2800(112/120) LR:0.000030 TrainLoss:0.376909 TrainAcc:0.988899\n",
      "24\n",
      "07-03 17:44:10 | Step:  2825(113/120) LR:0.000030 TrainLoss:0.376150 TrainAcc:0.990330\n",
      "24\n",
      "07-03 17:44:26 | Step:  2850(114/120) LR:0.000030 TrainLoss:0.376005 TrainAcc:0.990079\n",
      "24\n",
      "07-03 17:44:42 | Step:  2875(115/120) LR:0.000030 TrainLoss:0.375988 TrainAcc:0.990119\n",
      "24\n",
      "07-03 17:44:58 | Step:  2900(116/120) LR:0.000030 TrainLoss:0.375722 TrainAcc:0.990138\n",
      "24\n",
      "07-03 17:45:14 | Step:  2925(117/120) LR:0.000030 TrainLoss:0.374341 TrainAcc:0.991637\n",
      "24\n",
      "07-03 17:45:29 | Step:  2950(118/120) LR:0.000030 TrainLoss:0.374378 TrainAcc:0.991459\n",
      "24\n",
      "07-03 17:45:46 | Step:  2975(119/120) LR:0.000030 TrainLoss:0.376849 TrainAcc:0.989953\n",
      "24\n",
      "07-03 17:46:01 | Step:  3000(120/120) LR:0.000030 TrainLoss:0.375431 TrainAcc:0.990970\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 3000\n",
    "batch_size = 30\n",
    "\n",
    "Img_NC16 = np.load('../dataset_npy/NC_16/train/NC16_train_img.npy')\n",
    "Img_NC16_img=np.shape(Img_NC16)\n",
    "print ('Img_NC16_img.shape=',Img_NC16_img)\n",
    "Lab_NC16 = np.load('../dataset_npy/NC_16/train/NC16_train_label.npy')\n",
    "# feat_nc16 = np.load('../dataset_npy/NC_16/train/NC16_train_img_feat.npy')\n",
    "feat_nc16 = np.load('../dataset_npy/NC_16/train/NC16_train_img_cohere_feat.npy')\n",
    "\n",
    "vx=Img_NC16[-40:]\n",
    "vx1=feat_nc16[-40:]\n",
    "vy=Lab_NC16[-40:]\n",
    "\n",
    "\n",
    "Img = Img_NC16[:-40]\n",
    "Lab = Lab_NC16[:-40]\n",
    "freq1 = feat_nc16[:-40]\n",
    "print('Img_NC16.shape=',Img_NC16.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'../model_s_gaijin4/modelS251875.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "#     vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "#         print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "            \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        train_accuracy /= train_batches\n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TrainLoss:%.6f TrainAcc:%.6f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, train_accuracy))\n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_s/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T02:38:51.729625Z",
     "start_time": "2019-07-05T02:09:17.251715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img_NC16.shape= (798, 256, 256, 3)\n",
      "INFO:tensorflow:Restoring parameters from ../model_s_gaijin4/modelS251875.ckpt\n",
      "session starting .................!!!!\n",
      "total_epoch= 231\n",
      "12\n",
      "07-05 10:09:51 | Step:    13(  1/231) LR:0.000030 TrainLoss:0.458275 TrainAcc:0.932608\n",
      "12\n",
      "07-05 10:09:58 | Step:    26(  2/231) LR:0.000030 TrainLoss:0.438413 TrainAcc:0.937123\n",
      "12\n",
      "07-05 10:10:05 | Step:    39(  3/231) LR:0.000030 TrainLoss:0.429661 TrainAcc:0.937196\n",
      "12\n",
      "07-05 10:10:12 | Step:    52(  4/231) LR:0.000030 TrainLoss:0.419716 TrainAcc:0.941483\n",
      "12\n",
      "07-05 10:10:20 | Step:    65(  5/231) LR:0.000030 TrainLoss:0.418053 TrainAcc:0.943267\n",
      "12\n",
      "07-05 10:10:27 | Step:    78(  6/231) LR:0.000030 TrainLoss:0.413329 TrainAcc:0.946106\n",
      "12\n",
      "07-05 10:10:34 | Step:    91(  7/231) LR:0.000030 TrainLoss:0.411844 TrainAcc:0.949005\n",
      "12\n",
      "07-05 10:10:41 | Step:   104(  8/231) LR:0.000030 TrainLoss:0.409231 TrainAcc:0.951242\n",
      "12\n",
      "07-05 10:10:49 | Step:   117(  9/231) LR:0.000030 TrainLoss:0.406971 TrainAcc:0.954734\n",
      "12\n",
      "07-05 10:10:56 | Step:   130( 10/231) LR:0.000030 TrainLoss:0.402732 TrainAcc:0.958912\n",
      "12\n",
      "07-05 10:11:03 | Step:   143( 11/231) LR:0.000030 TrainLoss:0.402567 TrainAcc:0.960688\n",
      "12\n",
      "07-05 10:11:11 | Step:   156( 12/231) LR:0.000030 TrainLoss:0.401303 TrainAcc:0.963671\n",
      "12\n",
      "07-05 10:11:19 | Step:   169( 13/231) LR:0.000030 TrainLoss:0.397695 TrainAcc:0.969543\n",
      "12\n",
      "07-05 10:11:26 | Step:   182( 14/231) LR:0.000030 TrainLoss:0.397137 TrainAcc:0.970663\n",
      "12\n",
      "07-05 10:11:33 | Step:   195( 15/231) LR:0.000030 TrainLoss:0.398131 TrainAcc:0.970551\n",
      "12\n",
      "07-05 10:11:41 | Step:   208( 16/231) LR:0.000030 TrainLoss:0.396145 TrainAcc:0.972592\n",
      "12\n",
      "07-05 10:11:48 | Step:   221( 17/231) LR:0.000030 TrainLoss:0.394839 TrainAcc:0.973344\n",
      "12\n",
      "07-05 10:11:56 | Step:   234( 18/231) LR:0.000030 TrainLoss:0.392968 TrainAcc:0.974280\n",
      "12\n",
      "07-05 10:12:03 | Step:   247( 19/231) LR:0.000030 TrainLoss:0.393933 TrainAcc:0.972707\n",
      "12\n",
      "07-05 10:12:11 | Step:   260( 20/231) LR:0.000030 TrainLoss:0.392518 TrainAcc:0.974721\n",
      "12\n",
      "07-05 10:12:18 | Step:   273( 21/231) LR:0.000030 TrainLoss:0.390651 TrainAcc:0.975573\n",
      "12\n",
      "07-05 10:12:25 | Step:   286( 22/231) LR:0.000030 TrainLoss:0.392351 TrainAcc:0.974232\n",
      "12\n",
      "07-05 10:12:33 | Step:   299( 23/231) LR:0.000030 TrainLoss:0.389007 TrainAcc:0.976612\n",
      "12\n",
      "07-05 10:12:40 | Step:   312( 24/231) LR:0.000030 TrainLoss:0.389222 TrainAcc:0.976250\n",
      "12\n",
      "07-05 10:12:48 | Step:   325( 25/231) LR:0.000030 TrainLoss:0.389536 TrainAcc:0.976164\n",
      "12\n",
      "07-05 10:12:55 | Step:   338( 26/231) LR:0.000030 TrainLoss:0.385673 TrainAcc:0.979082\n",
      "12\n",
      "07-05 10:13:03 | Step:   351( 27/231) LR:0.000030 TrainLoss:0.389731 TrainAcc:0.975709\n",
      "12\n",
      "07-05 10:13:10 | Step:   364( 28/231) LR:0.000030 TrainLoss:0.387067 TrainAcc:0.977990\n",
      "12\n",
      "07-05 10:13:18 | Step:   377( 29/231) LR:0.000030 TrainLoss:0.385082 TrainAcc:0.980420\n",
      "12\n",
      "07-05 10:13:26 | Step:   390( 30/231) LR:0.000030 TrainLoss:0.388764 TrainAcc:0.977428\n",
      "12\n",
      "07-05 10:13:33 | Step:   403( 31/231) LR:0.000030 TrainLoss:0.386582 TrainAcc:0.978697\n",
      "12\n",
      "07-05 10:13:41 | Step:   416( 32/231) LR:0.000030 TrainLoss:0.385620 TrainAcc:0.980416\n",
      "12\n",
      "07-05 10:13:48 | Step:   429( 33/231) LR:0.000030 TrainLoss:0.385029 TrainAcc:0.981391\n",
      "12\n",
      "07-05 10:13:56 | Step:   442( 34/231) LR:0.000030 TrainLoss:0.384854 TrainAcc:0.980734\n",
      "12\n",
      "07-05 10:14:03 | Step:   455( 35/231) LR:0.000030 TrainLoss:0.384452 TrainAcc:0.981637\n",
      "12\n",
      "07-05 10:14:11 | Step:   468( 36/231) LR:0.000030 TrainLoss:0.383163 TrainAcc:0.982723\n",
      "12\n",
      "07-05 10:14:18 | Step:   481( 37/231) LR:0.000030 TrainLoss:0.383922 TrainAcc:0.981886\n",
      "12\n",
      "07-05 10:14:26 | Step:   494( 38/231) LR:0.000030 TrainLoss:0.383783 TrainAcc:0.982633\n",
      "12\n",
      "07-05 10:14:33 | Step:   507( 39/231) LR:0.000030 TrainLoss:0.381730 TrainAcc:0.984180\n",
      "12\n",
      "07-05 10:14:41 | Step:   520( 40/231) LR:0.000030 TrainLoss:0.382936 TrainAcc:0.982898\n",
      "12\n",
      "07-05 10:14:48 | Step:   533( 41/231) LR:0.000030 TrainLoss:0.380861 TrainAcc:0.985621\n",
      "12\n",
      "07-05 10:14:56 | Step:   546( 42/231) LR:0.000030 TrainLoss:0.384958 TrainAcc:0.981612\n",
      "12\n",
      "07-05 10:15:04 | Step:   559( 43/231) LR:0.000030 TrainLoss:0.381198 TrainAcc:0.984874\n",
      "12\n",
      "07-05 10:15:11 | Step:   572( 44/231) LR:0.000030 TrainLoss:0.380223 TrainAcc:0.985877\n",
      "12\n",
      "07-05 10:15:19 | Step:   585( 45/231) LR:0.000030 TrainLoss:0.380973 TrainAcc:0.985269\n",
      "12\n",
      "07-05 10:15:26 | Step:   598( 46/231) LR:0.000030 TrainLoss:0.379956 TrainAcc:0.986310\n",
      "12\n",
      "07-05 10:15:34 | Step:   611( 47/231) LR:0.000030 TrainLoss:0.381413 TrainAcc:0.985150\n",
      "12\n",
      "07-05 10:15:41 | Step:   624( 48/231) LR:0.000030 TrainLoss:0.381178 TrainAcc:0.985765\n",
      "12\n",
      "07-05 10:15:49 | Step:   637( 49/231) LR:0.000030 TrainLoss:0.381711 TrainAcc:0.984660\n",
      "12\n",
      "07-05 10:15:57 | Step:   650( 50/231) LR:0.000030 TrainLoss:0.383183 TrainAcc:0.983587\n",
      "12\n",
      "07-05 10:16:04 | Step:   663( 51/231) LR:0.000030 TrainLoss:0.380280 TrainAcc:0.985508\n",
      "12\n",
      "07-05 10:16:12 | Step:   676( 52/231) LR:0.000030 TrainLoss:0.381911 TrainAcc:0.984637\n",
      "12\n",
      "07-05 10:16:19 | Step:   689( 53/231) LR:0.000030 TrainLoss:0.382046 TrainAcc:0.983980\n",
      "12\n",
      "07-05 10:16:27 | Step:   702( 54/231) LR:0.000030 TrainLoss:0.379110 TrainAcc:0.987443\n",
      "12\n",
      "07-05 10:16:35 | Step:   715( 55/231) LR:0.000030 TrainLoss:0.381921 TrainAcc:0.984285\n",
      "12\n",
      "07-05 10:16:42 | Step:   728( 56/231) LR:0.000030 TrainLoss:0.379682 TrainAcc:0.986646\n",
      "12\n",
      "07-05 10:16:50 | Step:   741( 57/231) LR:0.000030 TrainLoss:0.380927 TrainAcc:0.985702\n",
      "12\n",
      "07-05 10:16:57 | Step:   754( 58/231) LR:0.000030 TrainLoss:0.380854 TrainAcc:0.985229\n",
      "12\n",
      "07-05 10:17:05 | Step:   767( 59/231) LR:0.000030 TrainLoss:0.380529 TrainAcc:0.986011\n",
      "12\n",
      "07-05 10:17:12 | Step:   780( 60/231) LR:0.000030 TrainLoss:0.382141 TrainAcc:0.984817\n",
      "12\n",
      "07-05 10:17:20 | Step:   793( 61/231) LR:0.000030 TrainLoss:0.379528 TrainAcc:0.986499\n",
      "12\n",
      "07-05 10:17:27 | Step:   806( 62/231) LR:0.000030 TrainLoss:0.380614 TrainAcc:0.985975\n",
      "12\n",
      "07-05 10:17:35 | Step:   819( 63/231) LR:0.000030 TrainLoss:0.379941 TrainAcc:0.985966\n",
      "12\n",
      "07-05 10:17:43 | Step:   832( 64/231) LR:0.000030 TrainLoss:0.380197 TrainAcc:0.985957\n",
      "12\n",
      "07-05 10:17:50 | Step:   845( 65/231) LR:0.000030 TrainLoss:0.380747 TrainAcc:0.985276\n",
      "12\n",
      "07-05 10:17:58 | Step:   858( 66/231) LR:0.000030 TrainLoss:0.377884 TrainAcc:0.988364\n",
      "12\n",
      "07-05 10:18:06 | Step:   871( 67/231) LR:0.000030 TrainLoss:0.380134 TrainAcc:0.985965\n",
      "12\n",
      "07-05 10:18:13 | Step:   884( 68/231) LR:0.000030 TrainLoss:0.378972 TrainAcc:0.987210\n",
      "12\n",
      "07-05 10:18:21 | Step:   897( 69/231) LR:0.000030 TrainLoss:0.378287 TrainAcc:0.987905\n",
      "12\n",
      "07-05 10:18:28 | Step:   910( 70/231) LR:0.000030 TrainLoss:0.378171 TrainAcc:0.988308\n",
      "12\n",
      "07-05 10:18:36 | Step:   923( 71/231) LR:0.000030 TrainLoss:0.379702 TrainAcc:0.986972\n",
      "12\n",
      "07-05 10:18:43 | Step:   936( 72/231) LR:0.000030 TrainLoss:0.378508 TrainAcc:0.987521\n",
      "12\n",
      "07-05 10:18:51 | Step:   949( 73/231) LR:0.000030 TrainLoss:0.379625 TrainAcc:0.987276\n",
      "12\n",
      "07-05 10:18:59 | Step:   962( 74/231) LR:0.000030 TrainLoss:0.378432 TrainAcc:0.987546\n",
      "12\n",
      "07-05 10:19:06 | Step:   975( 75/231) LR:0.000030 TrainLoss:0.378877 TrainAcc:0.987317\n",
      "12\n",
      "07-05 10:19:14 | Step:   988( 76/231) LR:0.000030 TrainLoss:0.380777 TrainAcc:0.985755\n",
      "12\n",
      "07-05 10:19:21 | Step:  1001( 77/231) LR:0.000030 TrainLoss:0.380632 TrainAcc:0.986102\n",
      "12\n",
      "07-05 10:19:29 | Step:  1014( 78/231) LR:0.000030 TrainLoss:0.379156 TrainAcc:0.987490\n",
      "12\n",
      "07-05 10:19:36 | Step:  1027( 79/231) LR:0.000030 TrainLoss:0.378860 TrainAcc:0.987667\n",
      "12\n",
      "07-05 10:19:44 | Step:  1040( 80/231) LR:0.000030 TrainLoss:0.376905 TrainAcc:0.989377\n",
      "12\n",
      "07-05 10:19:52 | Step:  1053( 81/231) LR:0.000030 TrainLoss:0.377212 TrainAcc:0.989064\n",
      "12\n",
      "07-05 10:19:59 | Step:  1066( 82/231) LR:0.000030 TrainLoss:0.377398 TrainAcc:0.989171\n",
      "12\n",
      "07-05 10:20:07 | Step:  1079( 83/231) LR:0.000030 TrainLoss:0.379748 TrainAcc:0.987106\n",
      "12\n",
      "07-05 10:20:15 | Step:  1092( 84/231) LR:0.000030 TrainLoss:0.377233 TrainAcc:0.989097\n",
      "12\n",
      "07-05 10:20:22 | Step:  1105( 85/231) LR:0.000030 TrainLoss:0.378302 TrainAcc:0.988174\n",
      "12\n",
      "07-05 10:20:30 | Step:  1118( 86/231) LR:0.000030 TrainLoss:0.378213 TrainAcc:0.988200\n",
      "12\n",
      "07-05 10:20:38 | Step:  1131( 87/231) LR:0.000030 TrainLoss:0.377365 TrainAcc:0.988862\n",
      "12\n",
      "07-05 10:20:45 | Step:  1144( 88/231) LR:0.000030 TrainLoss:0.378732 TrainAcc:0.987670\n",
      "12\n",
      "07-05 10:20:53 | Step:  1157( 89/231) LR:0.000030 TrainLoss:0.377268 TrainAcc:0.988933\n",
      "12\n",
      "07-05 10:21:00 | Step:  1170( 90/231) LR:0.000030 TrainLoss:0.377350 TrainAcc:0.989240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "07-05 10:21:08 | Step:  1183( 91/231) LR:0.000030 TrainLoss:0.377427 TrainAcc:0.988920\n",
      "12\n",
      "07-05 10:21:15 | Step:  1196( 92/231) LR:0.000030 TrainLoss:0.377601 TrainAcc:0.988791\n",
      "12\n",
      "07-05 10:21:23 | Step:  1209( 93/231) LR:0.000030 TrainLoss:0.378637 TrainAcc:0.988041\n",
      "12\n",
      "07-05 10:21:31 | Step:  1222( 94/231) LR:0.000030 TrainLoss:0.378776 TrainAcc:0.988278\n",
      "12\n",
      "07-05 10:21:38 | Step:  1235( 95/231) LR:0.000030 TrainLoss:0.378810 TrainAcc:0.987615\n",
      "12\n",
      "07-05 10:21:46 | Step:  1248( 96/231) LR:0.000030 TrainLoss:0.376422 TrainAcc:0.989859\n",
      "12\n",
      "07-05 10:21:54 | Step:  1261( 97/231) LR:0.000030 TrainLoss:0.377576 TrainAcc:0.988787\n",
      "12\n",
      "07-05 10:22:01 | Step:  1274( 98/231) LR:0.000030 TrainLoss:0.377328 TrainAcc:0.988750\n",
      "12\n",
      "07-05 10:22:09 | Step:  1287( 99/231) LR:0.000030 TrainLoss:0.377773 TrainAcc:0.988791\n",
      "12\n",
      "07-05 10:22:16 | Step:  1300(100/231) LR:0.000030 TrainLoss:0.376130 TrainAcc:0.990189\n",
      "12\n",
      "07-05 10:22:24 | Step:  1313(101/231) LR:0.000030 TrainLoss:0.376031 TrainAcc:0.990379\n",
      "12\n",
      "07-05 10:22:32 | Step:  1326(102/231) LR:0.000030 TrainLoss:0.377940 TrainAcc:0.988630\n",
      "12\n",
      "07-05 10:22:39 | Step:  1339(103/231) LR:0.000030 TrainLoss:0.377734 TrainAcc:0.988819\n",
      "12\n",
      "07-05 10:22:47 | Step:  1352(104/231) LR:0.000030 TrainLoss:0.376768 TrainAcc:0.989201\n",
      "12\n",
      "07-05 10:22:55 | Step:  1365(105/231) LR:0.000030 TrainLoss:0.378002 TrainAcc:0.988342\n",
      "12\n",
      "07-05 10:23:02 | Step:  1378(106/231) LR:0.000030 TrainLoss:0.377061 TrainAcc:0.988978\n",
      "12\n",
      "07-05 10:23:10 | Step:  1391(107/231) LR:0.000030 TrainLoss:0.376919 TrainAcc:0.989603\n",
      "12\n",
      "07-05 10:23:17 | Step:  1404(108/231) LR:0.000030 TrainLoss:0.375237 TrainAcc:0.991193\n",
      "12\n",
      "07-05 10:23:25 | Step:  1417(109/231) LR:0.000030 TrainLoss:0.376967 TrainAcc:0.989183\n",
      "12\n",
      "07-05 10:23:32 | Step:  1430(110/231) LR:0.000030 TrainLoss:0.377393 TrainAcc:0.989437\n",
      "12\n",
      "07-05 10:23:40 | Step:  1443(111/231) LR:0.000030 TrainLoss:0.378121 TrainAcc:0.988537\n",
      "12\n",
      "07-05 10:23:48 | Step:  1456(112/231) LR:0.000030 TrainLoss:0.378377 TrainAcc:0.988882\n",
      "12\n",
      "07-05 10:23:55 | Step:  1469(113/231) LR:0.000030 TrainLoss:0.375607 TrainAcc:0.990537\n",
      "12\n",
      "07-05 10:24:03 | Step:  1482(114/231) LR:0.000030 TrainLoss:0.376091 TrainAcc:0.990583\n",
      "12\n",
      "07-05 10:24:11 | Step:  1495(115/231) LR:0.000030 TrainLoss:0.376640 TrainAcc:0.989662\n",
      "12\n",
      "07-05 10:24:18 | Step:  1508(116/231) LR:0.000030 TrainLoss:0.376080 TrainAcc:0.990344\n",
      "12\n",
      "07-05 10:24:26 | Step:  1521(117/231) LR:0.000030 TrainLoss:0.376496 TrainAcc:0.989729\n",
      "12\n",
      "07-05 10:24:33 | Step:  1534(118/231) LR:0.000030 TrainLoss:0.376645 TrainAcc:0.989606\n",
      "12\n",
      "07-05 10:24:41 | Step:  1547(119/231) LR:0.000030 TrainLoss:0.375999 TrainAcc:0.990211\n",
      "12\n",
      "07-05 10:24:49 | Step:  1560(120/231) LR:0.000030 TrainLoss:0.375575 TrainAcc:0.990689\n",
      "12\n",
      "07-05 10:24:56 | Step:  1573(121/231) LR:0.000030 TrainLoss:0.375978 TrainAcc:0.990508\n",
      "12\n",
      "07-05 10:25:04 | Step:  1586(122/231) LR:0.000030 TrainLoss:0.376855 TrainAcc:0.989943\n",
      "12\n",
      "07-05 10:25:11 | Step:  1599(123/231) LR:0.000030 TrainLoss:0.376631 TrainAcc:0.990021\n",
      "12\n",
      "07-05 10:25:19 | Step:  1612(124/231) LR:0.000030 TrainLoss:0.375915 TrainAcc:0.990427\n",
      "12\n",
      "07-05 10:25:27 | Step:  1625(125/231) LR:0.000030 TrainLoss:0.375523 TrainAcc:0.991102\n",
      "12\n",
      "07-05 10:25:34 | Step:  1638(126/231) LR:0.000030 TrainLoss:0.375079 TrainAcc:0.991283\n",
      "12\n",
      "07-05 10:25:42 | Step:  1651(127/231) LR:0.000030 TrainLoss:0.376574 TrainAcc:0.990262\n",
      "12\n",
      "07-05 10:25:49 | Step:  1664(128/231) LR:0.000030 TrainLoss:0.377779 TrainAcc:0.989055\n",
      "12\n",
      "07-05 10:25:57 | Step:  1677(129/231) LR:0.000030 TrainLoss:0.376669 TrainAcc:0.989542\n",
      "12\n",
      "07-05 10:26:05 | Step:  1690(130/231) LR:0.000030 TrainLoss:0.376577 TrainAcc:0.989705\n",
      "12\n",
      "07-05 10:26:12 | Step:  1703(131/231) LR:0.000030 TrainLoss:0.375456 TrainAcc:0.990869\n",
      "12\n",
      "07-05 10:26:20 | Step:  1716(132/231) LR:0.000030 TrainLoss:0.375368 TrainAcc:0.990855\n",
      "12\n",
      "07-05 10:26:27 | Step:  1729(133/231) LR:0.000030 TrainLoss:0.375231 TrainAcc:0.990979\n",
      "12\n",
      "07-05 10:26:35 | Step:  1742(134/231) LR:0.000030 TrainLoss:0.376565 TrainAcc:0.990037\n",
      "12\n",
      "07-05 10:26:43 | Step:  1755(135/231) LR:0.000030 TrainLoss:0.376749 TrainAcc:0.989647\n",
      "12\n",
      "07-05 10:26:50 | Step:  1768(136/231) LR:0.000030 TrainLoss:0.374986 TrainAcc:0.991091\n",
      "12\n",
      "07-05 10:26:58 | Step:  1781(137/231) LR:0.000030 TrainLoss:0.376180 TrainAcc:0.990632\n",
      "12\n",
      "07-05 10:27:05 | Step:  1794(138/231) LR:0.000030 TrainLoss:0.374859 TrainAcc:0.991340\n",
      "12\n",
      "07-05 10:27:13 | Step:  1807(139/231) LR:0.000030 TrainLoss:0.375563 TrainAcc:0.991014\n",
      "12\n",
      "07-05 10:27:20 | Step:  1820(140/231) LR:0.000030 TrainLoss:0.379414 TrainAcc:0.988351\n",
      "12\n",
      "07-05 10:27:28 | Step:  1833(141/231) LR:0.000030 TrainLoss:0.375360 TrainAcc:0.990891\n",
      "12\n",
      "07-05 10:27:35 | Step:  1846(142/231) LR:0.000030 TrainLoss:0.378187 TrainAcc:0.988579\n",
      "12\n",
      "07-05 10:27:43 | Step:  1859(143/231) LR:0.000030 TrainLoss:0.376886 TrainAcc:0.989928\n",
      "12\n",
      "07-05 10:27:51 | Step:  1872(144/231) LR:0.000030 TrainLoss:0.375596 TrainAcc:0.990473\n",
      "12\n",
      "07-05 10:27:58 | Step:  1885(145/231) LR:0.000030 TrainLoss:0.375273 TrainAcc:0.990680\n",
      "12\n",
      "07-05 10:28:06 | Step:  1898(146/231) LR:0.000030 TrainLoss:0.376323 TrainAcc:0.990518\n",
      "12\n",
      "07-05 10:28:13 | Step:  1911(147/231) LR:0.000030 TrainLoss:0.375377 TrainAcc:0.991058\n",
      "12\n",
      "07-05 10:28:21 | Step:  1924(148/231) LR:0.000030 TrainLoss:0.377041 TrainAcc:0.989518\n",
      "12\n",
      "07-05 10:28:28 | Step:  1937(149/231) LR:0.000030 TrainLoss:0.375128 TrainAcc:0.991155\n",
      "12\n",
      "07-05 10:28:36 | Step:  1950(150/231) LR:0.000030 TrainLoss:0.376952 TrainAcc:0.989722\n",
      "12\n",
      "07-05 10:28:44 | Step:  1963(151/231) LR:0.000030 TrainLoss:0.375790 TrainAcc:0.990892\n",
      "12\n",
      "07-05 10:28:51 | Step:  1976(152/231) LR:0.000030 TrainLoss:0.375720 TrainAcc:0.990359\n",
      "12\n",
      "07-05 10:28:58 | Step:  1989(153/231) LR:0.000030 TrainLoss:0.377322 TrainAcc:0.989125\n",
      "12\n",
      "07-05 10:29:06 | Step:  2002(154/231) LR:0.000030 TrainLoss:0.377780 TrainAcc:0.989124\n",
      "12\n",
      "07-05 10:29:14 | Step:  2015(155/231) LR:0.000030 TrainLoss:0.377173 TrainAcc:0.989565\n",
      "12\n",
      "07-05 10:29:21 | Step:  2028(156/231) LR:0.000030 TrainLoss:0.375749 TrainAcc:0.990508\n",
      "12\n",
      "07-05 10:29:29 | Step:  2041(157/231) LR:0.000030 TrainLoss:0.376356 TrainAcc:0.990302\n",
      "12\n",
      "07-05 10:29:37 | Step:  2054(158/231) LR:0.000030 TrainLoss:0.375812 TrainAcc:0.990737\n",
      "12\n",
      "07-05 10:29:44 | Step:  2067(159/231) LR:0.000030 TrainLoss:0.374446 TrainAcc:0.991573\n",
      "12\n",
      "07-05 10:29:52 | Step:  2080(160/231) LR:0.000030 TrainLoss:0.375619 TrainAcc:0.990896\n",
      "12\n",
      "07-05 10:29:59 | Step:  2093(161/231) LR:0.000030 TrainLoss:0.377057 TrainAcc:0.989542\n",
      "12\n",
      "07-05 10:30:07 | Step:  2106(162/231) LR:0.000030 TrainLoss:0.375857 TrainAcc:0.990268\n",
      "12\n",
      "07-05 10:30:14 | Step:  2119(163/231) LR:0.000030 TrainLoss:0.377112 TrainAcc:0.989165\n",
      "12\n",
      "07-05 10:30:22 | Step:  2132(164/231) LR:0.000030 TrainLoss:0.375259 TrainAcc:0.991127\n",
      "12\n",
      "07-05 10:30:29 | Step:  2145(165/231) LR:0.000030 TrainLoss:0.376715 TrainAcc:0.989878\n",
      "12\n",
      "07-05 10:30:37 | Step:  2158(166/231) LR:0.000030 TrainLoss:0.376087 TrainAcc:0.990277\n",
      "12\n",
      "07-05 10:30:44 | Step:  2171(167/231) LR:0.000030 TrainLoss:0.375268 TrainAcc:0.990852\n",
      "12\n",
      "07-05 10:30:52 | Step:  2184(168/231) LR:0.000030 TrainLoss:0.376543 TrainAcc:0.990289\n",
      "12\n",
      "07-05 10:31:00 | Step:  2197(169/231) LR:0.000030 TrainLoss:0.374043 TrainAcc:0.992039\n",
      "12\n",
      "07-05 10:31:07 | Step:  2210(170/231) LR:0.000030 TrainLoss:0.376045 TrainAcc:0.990927\n",
      "12\n",
      "07-05 10:31:15 | Step:  2223(171/231) LR:0.000030 TrainLoss:0.375354 TrainAcc:0.990749\n",
      "12\n",
      "07-05 10:31:22 | Step:  2236(172/231) LR:0.000030 TrainLoss:0.373876 TrainAcc:0.992201\n",
      "12\n",
      "07-05 10:31:30 | Step:  2249(173/231) LR:0.000030 TrainLoss:0.375649 TrainAcc:0.990663\n",
      "12\n",
      "07-05 10:31:37 | Step:  2262(174/231) LR:0.000030 TrainLoss:0.376193 TrainAcc:0.990143\n",
      "12\n",
      "07-05 10:31:45 | Step:  2275(175/231) LR:0.000030 TrainLoss:0.375069 TrainAcc:0.991306\n",
      "12\n",
      "07-05 10:31:53 | Step:  2288(176/231) LR:0.000030 TrainLoss:0.375593 TrainAcc:0.990417\n",
      "12\n",
      "07-05 10:32:00 | Step:  2301(177/231) LR:0.000030 TrainLoss:0.376716 TrainAcc:0.990147\n",
      "12\n",
      "07-05 10:32:08 | Step:  2314(178/231) LR:0.000030 TrainLoss:0.375096 TrainAcc:0.991087\n",
      "12\n",
      "07-05 10:32:15 | Step:  2327(179/231) LR:0.000030 TrainLoss:0.374501 TrainAcc:0.991719\n",
      "12\n",
      "07-05 10:32:23 | Step:  2340(180/231) LR:0.000030 TrainLoss:0.375320 TrainAcc:0.991102\n",
      "12\n",
      "07-05 10:32:30 | Step:  2353(181/231) LR:0.000030 TrainLoss:0.374491 TrainAcc:0.991505\n",
      "12\n",
      "07-05 10:32:38 | Step:  2366(182/231) LR:0.000030 TrainLoss:0.375953 TrainAcc:0.990361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "07-05 10:32:45 | Step:  2379(183/231) LR:0.000030 TrainLoss:0.375250 TrainAcc:0.991083\n",
      "12\n",
      "07-05 10:32:53 | Step:  2392(184/231) LR:0.000030 TrainLoss:0.376472 TrainAcc:0.990146\n",
      "12\n",
      "07-05 10:33:01 | Step:  2405(185/231) LR:0.000030 TrainLoss:0.374889 TrainAcc:0.991322\n",
      "12\n",
      "07-05 10:33:08 | Step:  2418(186/231) LR:0.000030 TrainLoss:0.376530 TrainAcc:0.989685\n",
      "12\n",
      "07-05 10:33:16 | Step:  2431(187/231) LR:0.000030 TrainLoss:0.375359 TrainAcc:0.991005\n",
      "12\n",
      "07-05 10:33:23 | Step:  2444(188/231) LR:0.000030 TrainLoss:0.373830 TrainAcc:0.992353\n",
      "12\n",
      "07-05 10:33:31 | Step:  2457(189/231) LR:0.000030 TrainLoss:0.375129 TrainAcc:0.991189\n",
      "12\n",
      "07-05 10:33:38 | Step:  2470(190/231) LR:0.000030 TrainLoss:0.374455 TrainAcc:0.991667\n",
      "12\n",
      "07-05 10:33:46 | Step:  2483(191/231) LR:0.000030 TrainLoss:0.375189 TrainAcc:0.991273\n",
      "12\n",
      "07-05 10:33:54 | Step:  2496(192/231) LR:0.000030 TrainLoss:0.375265 TrainAcc:0.991295\n",
      "12\n",
      "07-05 10:34:01 | Step:  2509(193/231) LR:0.000030 TrainLoss:0.374105 TrainAcc:0.992078\n",
      "12\n",
      "07-05 10:34:09 | Step:  2522(194/231) LR:0.000030 TrainLoss:0.377286 TrainAcc:0.989986\n",
      "12\n",
      "07-05 10:34:17 | Step:  2535(195/231) LR:0.000030 TrainLoss:0.374483 TrainAcc:0.991752\n",
      "12\n",
      "07-05 10:34:24 | Step:  2548(196/231) LR:0.000030 TrainLoss:0.374904 TrainAcc:0.991263\n",
      "12\n",
      "07-05 10:34:32 | Step:  2561(197/231) LR:0.000030 TrainLoss:0.374443 TrainAcc:0.991705\n",
      "12\n",
      "07-05 10:34:39 | Step:  2574(198/231) LR:0.000030 TrainLoss:0.375318 TrainAcc:0.991180\n",
      "12\n",
      "07-05 10:34:47 | Step:  2587(199/231) LR:0.000030 TrainLoss:0.373999 TrainAcc:0.992011\n",
      "12\n",
      "07-05 10:34:54 | Step:  2600(200/231) LR:0.000030 TrainLoss:0.376278 TrainAcc:0.990660\n",
      "12\n",
      "07-05 10:35:02 | Step:  2613(201/231) LR:0.000030 TrainLoss:0.375432 TrainAcc:0.990785\n",
      "12\n",
      "07-05 10:35:10 | Step:  2626(202/231) LR:0.000030 TrainLoss:0.374926 TrainAcc:0.991280\n",
      "12\n",
      "07-05 10:35:17 | Step:  2639(203/231) LR:0.000030 TrainLoss:0.373994 TrainAcc:0.992218\n",
      "12\n",
      "07-05 10:35:25 | Step:  2652(204/231) LR:0.000030 TrainLoss:0.377511 TrainAcc:0.989540\n",
      "12\n",
      "07-05 10:35:32 | Step:  2665(205/231) LR:0.000030 TrainLoss:0.374532 TrainAcc:0.991824\n",
      "12\n",
      "07-05 10:35:40 | Step:  2678(206/231) LR:0.000030 TrainLoss:0.373849 TrainAcc:0.992311\n",
      "12\n",
      "07-05 10:35:47 | Step:  2691(207/231) LR:0.000030 TrainLoss:0.373918 TrainAcc:0.992168\n",
      "12\n",
      "07-05 10:35:55 | Step:  2704(208/231) LR:0.000030 TrainLoss:0.375493 TrainAcc:0.990788\n",
      "12\n",
      "07-05 10:36:03 | Step:  2717(209/231) LR:0.000030 TrainLoss:0.376007 TrainAcc:0.990708\n",
      "12\n",
      "07-05 10:36:10 | Step:  2730(210/231) LR:0.000030 TrainLoss:0.375143 TrainAcc:0.991388\n",
      "12\n",
      "07-05 10:36:18 | Step:  2743(211/231) LR:0.000030 TrainLoss:0.374056 TrainAcc:0.992113\n",
      "12\n",
      "07-05 10:36:25 | Step:  2756(212/231) LR:0.000030 TrainLoss:0.376296 TrainAcc:0.990601\n",
      "12\n",
      "07-05 10:36:33 | Step:  2769(213/231) LR:0.000030 TrainLoss:0.374127 TrainAcc:0.992034\n",
      "12\n",
      "07-05 10:36:40 | Step:  2782(214/231) LR:0.000030 TrainLoss:0.374611 TrainAcc:0.991970\n",
      "12\n",
      "07-05 10:36:48 | Step:  2795(215/231) LR:0.000030 TrainLoss:0.374035 TrainAcc:0.992089\n",
      "12\n",
      "07-05 10:36:56 | Step:  2808(216/231) LR:0.000030 TrainLoss:0.374426 TrainAcc:0.991736\n",
      "12\n",
      "07-05 10:37:03 | Step:  2821(217/231) LR:0.000030 TrainLoss:0.374920 TrainAcc:0.991497\n",
      "12\n",
      "07-05 10:37:10 | Step:  2834(218/231) LR:0.000030 TrainLoss:0.374846 TrainAcc:0.991651\n",
      "12\n",
      "07-05 10:37:18 | Step:  2847(219/231) LR:0.000030 TrainLoss:0.375828 TrainAcc:0.991035\n",
      "12\n",
      "07-05 10:37:26 | Step:  2860(220/231) LR:0.000030 TrainLoss:0.373870 TrainAcc:0.992345\n",
      "12\n",
      "07-05 10:37:33 | Step:  2873(221/231) LR:0.000030 TrainLoss:0.375294 TrainAcc:0.991162\n",
      "12\n",
      "07-05 10:37:41 | Step:  2886(222/231) LR:0.000030 TrainLoss:0.373457 TrainAcc:0.992630\n",
      "12\n",
      "07-05 10:37:48 | Step:  2899(223/231) LR:0.000030 TrainLoss:0.374770 TrainAcc:0.991562\n",
      "12\n",
      "07-05 10:37:56 | Step:  2912(224/231) LR:0.000030 TrainLoss:0.374628 TrainAcc:0.992017\n",
      "12\n",
      "07-05 10:38:03 | Step:  2925(225/231) LR:0.000030 TrainLoss:0.376256 TrainAcc:0.990295\n",
      "12\n",
      "07-05 10:38:11 | Step:  2938(226/231) LR:0.000030 TrainLoss:0.375647 TrainAcc:0.990870\n",
      "12\n",
      "07-05 10:38:18 | Step:  2951(227/231) LR:0.000030 TrainLoss:0.376076 TrainAcc:0.990436\n",
      "12\n",
      "07-05 10:38:26 | Step:  2964(228/231) LR:0.000030 TrainLoss:0.373783 TrainAcc:0.991929\n",
      "12\n",
      "07-05 10:38:34 | Step:  2977(229/231) LR:0.000030 TrainLoss:0.373986 TrainAcc:0.992219\n",
      "12\n",
      "07-05 10:38:41 | Step:  2990(230/231) LR:0.000030 TrainLoss:0.375153 TrainAcc:0.991092\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 3000\n",
    "batch_size = 30\n",
    "\n",
    "Img = np.load('../dataset_npy/NC_16/mani/NC16_mani_train_img.npy')\n",
    "Lab = np.load('../dataset_npy/NC_16/mani/NC16_mani_train_label.npy')\n",
    "freq1 = np.load('../dataset_npy/NC_16/mani/NC16_mani_train_imgS_feat.npy')\n",
    "print('Img_NC16.shape=',Img_NC16.shape)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'../model_s_gaijin4/modelS251875.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "#     vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "#         print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "            \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        train_accuracy /= train_batches\n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TrainLoss:%.6f TrainAcc:%.6f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, train_accuracy))\n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_s_gaijin6/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T10:28:10.674076Z",
     "start_time": "2019-07-27T09:56:40.788495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img.shape= (441, 256, 256, 3)\n",
      "INFO:tensorflow:Restoring parameters from ../model_s_gaijin4/modelS251875.ckpt\n",
      "session starting .................!!!!\n",
      "total_epoch= 215\n",
      "13\n",
      "07-27 17:57:08 | Step:    14(  1/215) LR:0.000030 TrainLoss:0.454782 TrainAcc:0.947981\n",
      "13\n",
      "07-27 17:57:17 | Step:    28(  2/215) LR:0.000030 TrainLoss:0.436918 TrainAcc:0.953111\n",
      "13\n",
      "07-27 17:57:26 | Step:    42(  3/215) LR:0.000030 TrainLoss:0.425745 TrainAcc:0.954204\n",
      "13\n",
      "07-27 17:57:34 | Step:    56(  4/215) LR:0.000030 TrainLoss:0.418862 TrainAcc:0.954022\n",
      "13\n",
      "07-27 17:57:43 | Step:    70(  5/215) LR:0.000030 TrainLoss:0.413757 TrainAcc:0.955117\n",
      "13\n",
      "07-27 17:57:51 | Step:    84(  6/215) LR:0.000030 TrainLoss:0.410306 TrainAcc:0.956543\n",
      "13\n",
      "07-27 17:58:00 | Step:    98(  7/215) LR:0.000030 TrainLoss:0.407649 TrainAcc:0.957021\n",
      "13\n",
      "07-27 17:58:08 | Step:   112(  8/215) LR:0.000030 TrainLoss:0.405518 TrainAcc:0.957812\n",
      "13\n",
      "07-27 17:58:16 | Step:   126(  9/215) LR:0.000030 TrainLoss:0.403720 TrainAcc:0.957918\n",
      "13\n",
      "07-27 17:58:25 | Step:   140( 10/215) LR:0.000030 TrainLoss:0.401966 TrainAcc:0.960035\n",
      "13\n",
      "07-27 17:58:33 | Step:   154( 11/215) LR:0.000030 TrainLoss:0.401573 TrainAcc:0.958341\n",
      "13\n",
      "07-27 17:58:42 | Step:   168( 12/215) LR:0.000030 TrainLoss:0.399924 TrainAcc:0.960217\n",
      "13\n",
      "07-27 17:58:50 | Step:   182( 13/215) LR:0.000030 TrainLoss:0.398869 TrainAcc:0.961340\n",
      "13\n",
      "07-27 17:58:59 | Step:   196( 14/215) LR:0.000030 TrainLoss:0.397563 TrainAcc:0.961984\n",
      "13\n",
      "07-27 17:59:08 | Step:   210( 15/215) LR:0.000030 TrainLoss:0.397036 TrainAcc:0.962130\n",
      "13\n",
      "07-27 17:59:16 | Step:   224( 16/215) LR:0.000030 TrainLoss:0.396944 TrainAcc:0.962846\n",
      "13\n",
      "07-27 17:59:25 | Step:   238( 17/215) LR:0.000030 TrainLoss:0.395192 TrainAcc:0.965191\n",
      "13\n",
      "07-27 17:59:34 | Step:   252( 18/215) LR:0.000030 TrainLoss:0.393854 TrainAcc:0.966603\n",
      "13\n",
      "07-27 17:59:42 | Step:   266( 19/215) LR:0.000030 TrainLoss:0.395235 TrainAcc:0.965582\n",
      "13\n",
      "07-27 17:59:51 | Step:   280( 20/215) LR:0.000030 TrainLoss:0.394196 TrainAcc:0.966851\n",
      "13\n",
      "07-27 18:00:00 | Step:   294( 21/215) LR:0.000030 TrainLoss:0.393511 TrainAcc:0.968896\n",
      "13\n",
      "07-27 18:00:08 | Step:   308( 22/215) LR:0.000030 TrainLoss:0.391629 TrainAcc:0.971740\n",
      "13\n",
      "07-27 18:00:17 | Step:   322( 23/215) LR:0.000030 TrainLoss:0.391323 TrainAcc:0.971588\n",
      "13\n",
      "07-27 18:00:26 | Step:   336( 24/215) LR:0.000030 TrainLoss:0.390897 TrainAcc:0.973413\n",
      "13\n",
      "07-27 18:00:35 | Step:   350( 25/215) LR:0.000030 TrainLoss:0.389993 TrainAcc:0.975381\n",
      "13\n",
      "07-27 18:00:43 | Step:   364( 26/215) LR:0.000030 TrainLoss:0.390310 TrainAcc:0.975829\n",
      "13\n",
      "07-27 18:00:52 | Step:   378( 27/215) LR:0.000030 TrainLoss:0.389898 TrainAcc:0.976768\n",
      "13\n",
      "07-27 18:01:01 | Step:   392( 28/215) LR:0.000030 TrainLoss:0.389461 TrainAcc:0.977393\n",
      "13\n",
      "07-27 18:01:09 | Step:   406( 29/215) LR:0.000030 TrainLoss:0.387926 TrainAcc:0.979093\n",
      "13\n",
      "07-27 18:01:18 | Step:   420( 30/215) LR:0.000030 TrainLoss:0.387971 TrainAcc:0.978699\n",
      "13\n",
      "07-27 18:01:27 | Step:   434( 31/215) LR:0.000030 TrainLoss:0.387176 TrainAcc:0.979458\n",
      "13\n",
      "07-27 18:01:36 | Step:   448( 32/215) LR:0.000030 TrainLoss:0.387379 TrainAcc:0.979240\n",
      "13\n",
      "07-27 18:01:44 | Step:   462( 33/215) LR:0.000030 TrainLoss:0.387645 TrainAcc:0.978906\n",
      "13\n",
      "07-27 18:01:53 | Step:   476( 34/215) LR:0.000030 TrainLoss:0.387375 TrainAcc:0.979360\n",
      "13\n",
      "07-27 18:02:02 | Step:   490( 35/215) LR:0.000030 TrainLoss:0.386778 TrainAcc:0.980029\n",
      "13\n",
      "07-27 18:02:10 | Step:   504( 36/215) LR:0.000030 TrainLoss:0.386541 TrainAcc:0.980075\n",
      "13\n",
      "07-27 18:02:19 | Step:   518( 37/215) LR:0.000030 TrainLoss:0.386194 TrainAcc:0.980547\n",
      "13\n",
      "07-27 18:02:28 | Step:   532( 38/215) LR:0.000030 TrainLoss:0.385599 TrainAcc:0.981046\n",
      "13\n",
      "07-27 18:02:37 | Step:   546( 39/215) LR:0.000030 TrainLoss:0.385708 TrainAcc:0.980532\n",
      "13\n",
      "07-27 18:02:45 | Step:   560( 40/215) LR:0.000030 TrainLoss:0.385865 TrainAcc:0.980403\n",
      "13\n",
      "07-27 18:02:54 | Step:   574( 41/215) LR:0.000030 TrainLoss:0.385457 TrainAcc:0.981084\n",
      "13\n",
      "07-27 18:03:03 | Step:   588( 42/215) LR:0.000030 TrainLoss:0.385129 TrainAcc:0.981140\n",
      "13\n",
      "07-27 18:03:12 | Step:   602( 43/215) LR:0.000030 TrainLoss:0.385439 TrainAcc:0.980970\n",
      "13\n",
      "07-27 18:03:20 | Step:   616( 44/215) LR:0.000030 TrainLoss:0.384514 TrainAcc:0.981713\n",
      "13\n",
      "07-27 18:03:29 | Step:   630( 45/215) LR:0.000030 TrainLoss:0.384829 TrainAcc:0.981412\n",
      "13\n",
      "07-27 18:03:38 | Step:   644( 46/215) LR:0.000030 TrainLoss:0.384145 TrainAcc:0.981775\n",
      "13\n",
      "07-27 18:03:47 | Step:   658( 47/215) LR:0.000030 TrainLoss:0.384279 TrainAcc:0.981928\n",
      "13\n",
      "07-27 18:03:55 | Step:   672( 48/215) LR:0.000030 TrainLoss:0.384215 TrainAcc:0.981689\n",
      "13\n",
      "07-27 18:04:04 | Step:   686( 49/215) LR:0.000030 TrainLoss:0.384202 TrainAcc:0.981587\n",
      "13\n",
      "07-27 18:04:13 | Step:   700( 50/215) LR:0.000030 TrainLoss:0.384070 TrainAcc:0.982108\n",
      "13\n",
      "07-27 18:04:22 | Step:   714( 51/215) LR:0.000030 TrainLoss:0.383438 TrainAcc:0.982484\n",
      "13\n",
      "07-27 18:04:31 | Step:   728( 52/215) LR:0.000030 TrainLoss:0.383827 TrainAcc:0.982109\n",
      "13\n",
      "07-27 18:04:39 | Step:   742( 53/215) LR:0.000030 TrainLoss:0.383522 TrainAcc:0.982418\n",
      "13\n",
      "07-27 18:04:48 | Step:   756( 54/215) LR:0.000030 TrainLoss:0.383434 TrainAcc:0.982289\n",
      "13\n",
      "07-27 18:04:57 | Step:   770( 55/215) LR:0.000030 TrainLoss:0.383140 TrainAcc:0.983044\n",
      "13\n",
      "07-27 18:05:06 | Step:   784( 56/215) LR:0.000030 TrainLoss:0.383204 TrainAcc:0.982535\n",
      "13\n",
      "07-27 18:05:14 | Step:   798( 57/215) LR:0.000030 TrainLoss:0.383087 TrainAcc:0.982775\n",
      "13\n",
      "07-27 18:05:23 | Step:   812( 58/215) LR:0.000030 TrainLoss:0.383254 TrainAcc:0.982555\n",
      "13\n",
      "07-27 18:05:31 | Step:   826( 59/215) LR:0.000030 TrainLoss:0.383172 TrainAcc:0.982518\n",
      "13\n",
      "07-27 18:05:40 | Step:   840( 60/215) LR:0.000030 TrainLoss:0.382795 TrainAcc:0.983082\n",
      "13\n",
      "07-27 18:05:49 | Step:   854( 61/215) LR:0.000030 TrainLoss:0.382526 TrainAcc:0.983169\n",
      "13\n",
      "07-27 18:05:58 | Step:   868( 62/215) LR:0.000030 TrainLoss:0.382634 TrainAcc:0.983093\n",
      "13\n",
      "07-27 18:06:06 | Step:   882( 63/215) LR:0.000030 TrainLoss:0.382493 TrainAcc:0.983170\n",
      "13\n",
      "07-27 18:06:15 | Step:   896( 64/215) LR:0.000030 TrainLoss:0.382369 TrainAcc:0.983073\n",
      "13\n",
      "07-27 18:06:24 | Step:   910( 65/215) LR:0.000030 TrainLoss:0.382585 TrainAcc:0.983035\n",
      "13\n",
      "07-27 18:06:33 | Step:   924( 66/215) LR:0.000030 TrainLoss:0.382089 TrainAcc:0.983581\n",
      "13\n",
      "07-27 18:06:41 | Step:   938( 67/215) LR:0.000030 TrainLoss:0.382393 TrainAcc:0.983216\n",
      "13\n",
      "07-27 18:06:50 | Step:   952( 68/215) LR:0.000030 TrainLoss:0.381912 TrainAcc:0.983595\n",
      "13\n",
      "07-27 18:06:59 | Step:   966( 69/215) LR:0.000030 TrainLoss:0.381710 TrainAcc:0.983788\n",
      "13\n",
      "07-27 18:07:08 | Step:   980( 70/215) LR:0.000030 TrainLoss:0.382162 TrainAcc:0.983380\n",
      "13\n",
      "07-27 18:07:16 | Step:   994( 71/215) LR:0.000030 TrainLoss:0.381414 TrainAcc:0.984121\n",
      "13\n",
      "07-27 18:07:25 | Step:  1008( 72/215) LR:0.000030 TrainLoss:0.381491 TrainAcc:0.984109\n",
      "13\n",
      "07-27 18:07:34 | Step:  1022( 73/215) LR:0.000030 TrainLoss:0.382257 TrainAcc:0.983239\n",
      "13\n",
      "07-27 18:07:43 | Step:  1036( 74/215) LR:0.000030 TrainLoss:0.382105 TrainAcc:0.983382\n",
      "13\n",
      "07-27 18:07:51 | Step:  1050( 75/215) LR:0.000030 TrainLoss:0.381392 TrainAcc:0.984005\n",
      "13\n",
      "07-27 18:08:00 | Step:  1064( 76/215) LR:0.000030 TrainLoss:0.381768 TrainAcc:0.983666\n",
      "13\n",
      "07-27 18:08:09 | Step:  1078( 77/215) LR:0.000030 TrainLoss:0.381237 TrainAcc:0.984081\n",
      "13\n",
      "07-27 18:08:18 | Step:  1092( 78/215) LR:0.000030 TrainLoss:0.381487 TrainAcc:0.983741\n",
      "13\n",
      "07-27 18:08:26 | Step:  1106( 79/215) LR:0.000030 TrainLoss:0.381947 TrainAcc:0.983431\n",
      "13\n",
      "07-27 18:08:35 | Step:  1120( 80/215) LR:0.000030 TrainLoss:0.382044 TrainAcc:0.983312\n",
      "13\n",
      "07-27 18:08:44 | Step:  1134( 81/215) LR:0.000030 TrainLoss:0.381330 TrainAcc:0.983878\n",
      "13\n",
      "07-27 18:08:52 | Step:  1148( 82/215) LR:0.000030 TrainLoss:0.381311 TrainAcc:0.983810\n",
      "13\n",
      "07-27 18:09:01 | Step:  1162( 83/215) LR:0.000030 TrainLoss:0.380969 TrainAcc:0.984221\n",
      "13\n",
      "07-27 18:09:10 | Step:  1176( 84/215) LR:0.000030 TrainLoss:0.381507 TrainAcc:0.983869\n",
      "13\n",
      "07-27 18:09:19 | Step:  1190( 85/215) LR:0.000030 TrainLoss:0.380740 TrainAcc:0.984385\n",
      "13\n",
      "07-27 18:09:27 | Step:  1204( 86/215) LR:0.000030 TrainLoss:0.380603 TrainAcc:0.984458\n",
      "13\n",
      "07-27 18:09:36 | Step:  1218( 87/215) LR:0.000030 TrainLoss:0.380679 TrainAcc:0.984443\n",
      "13\n",
      "07-27 18:09:45 | Step:  1232( 88/215) LR:0.000030 TrainLoss:0.380662 TrainAcc:0.984395\n",
      "13\n",
      "07-27 18:09:53 | Step:  1246( 89/215) LR:0.000030 TrainLoss:0.380967 TrainAcc:0.984050\n",
      "13\n",
      "07-27 18:10:02 | Step:  1260( 90/215) LR:0.000030 TrainLoss:0.380872 TrainAcc:0.984174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "07-27 18:10:11 | Step:  1274( 91/215) LR:0.000030 TrainLoss:0.380245 TrainAcc:0.984665\n",
      "13\n",
      "07-27 18:10:19 | Step:  1288( 92/215) LR:0.000030 TrainLoss:0.380913 TrainAcc:0.984036\n",
      "13\n",
      "07-27 18:10:28 | Step:  1302( 93/215) LR:0.000030 TrainLoss:0.380688 TrainAcc:0.984328\n",
      "13\n",
      "07-27 18:10:37 | Step:  1316( 94/215) LR:0.000030 TrainLoss:0.380671 TrainAcc:0.984402\n",
      "13\n",
      "07-27 18:10:46 | Step:  1330( 95/215) LR:0.000030 TrainLoss:0.380653 TrainAcc:0.984204\n",
      "13\n",
      "07-27 18:10:54 | Step:  1344( 96/215) LR:0.000030 TrainLoss:0.380804 TrainAcc:0.984095\n",
      "13\n",
      "07-27 18:11:03 | Step:  1358( 97/215) LR:0.000030 TrainLoss:0.380273 TrainAcc:0.984641\n",
      "13\n",
      "07-27 18:11:11 | Step:  1372( 98/215) LR:0.000030 TrainLoss:0.380330 TrainAcc:0.984601\n",
      "13\n",
      "07-27 18:11:20 | Step:  1386( 99/215) LR:0.000030 TrainLoss:0.380645 TrainAcc:0.984157\n",
      "13\n",
      "07-27 18:11:29 | Step:  1400(100/215) LR:0.000030 TrainLoss:0.380462 TrainAcc:0.984624\n",
      "13\n",
      "07-27 18:11:37 | Step:  1414(101/215) LR:0.000030 TrainLoss:0.380525 TrainAcc:0.984294\n",
      "13\n",
      "07-27 18:11:46 | Step:  1428(102/215) LR:0.000030 TrainLoss:0.379758 TrainAcc:0.985026\n",
      "13\n",
      "07-27 18:11:55 | Step:  1442(103/215) LR:0.000030 TrainLoss:0.380586 TrainAcc:0.984193\n",
      "13\n",
      "07-27 18:12:04 | Step:  1456(104/215) LR:0.000030 TrainLoss:0.379980 TrainAcc:0.985095\n",
      "13\n",
      "07-27 18:12:12 | Step:  1470(105/215) LR:0.000030 TrainLoss:0.380261 TrainAcc:0.984589\n",
      "13\n",
      "07-27 18:12:21 | Step:  1484(106/215) LR:0.000030 TrainLoss:0.380086 TrainAcc:0.984671\n",
      "13\n",
      "07-27 18:12:29 | Step:  1498(107/215) LR:0.000030 TrainLoss:0.379882 TrainAcc:0.984856\n",
      "13\n",
      "07-27 18:12:38 | Step:  1512(108/215) LR:0.000030 TrainLoss:0.379587 TrainAcc:0.985267\n",
      "13\n",
      "07-27 18:12:47 | Step:  1526(109/215) LR:0.000030 TrainLoss:0.380204 TrainAcc:0.984528\n",
      "13\n",
      "07-27 18:12:55 | Step:  1540(110/215) LR:0.000030 TrainLoss:0.379312 TrainAcc:0.985558\n",
      "13\n",
      "07-27 18:13:04 | Step:  1554(111/215) LR:0.000030 TrainLoss:0.379667 TrainAcc:0.984985\n",
      "13\n",
      "07-27 18:13:13 | Step:  1568(112/215) LR:0.000030 TrainLoss:0.379975 TrainAcc:0.984548\n",
      "13\n",
      "07-27 18:13:22 | Step:  1582(113/215) LR:0.000030 TrainLoss:0.379793 TrainAcc:0.984912\n",
      "13\n",
      "07-27 18:13:30 | Step:  1596(114/215) LR:0.000030 TrainLoss:0.380353 TrainAcc:0.984310\n",
      "13\n",
      "07-27 18:13:39 | Step:  1610(115/215) LR:0.000030 TrainLoss:0.379301 TrainAcc:0.985236\n",
      "13\n",
      "07-27 18:13:48 | Step:  1624(116/215) LR:0.000030 TrainLoss:0.379573 TrainAcc:0.985028\n",
      "13\n",
      "07-27 18:13:56 | Step:  1638(117/215) LR:0.000030 TrainLoss:0.380352 TrainAcc:0.984424\n",
      "13\n",
      "07-27 18:14:05 | Step:  1652(118/215) LR:0.000030 TrainLoss:0.379114 TrainAcc:0.985413\n",
      "13\n",
      "07-27 18:14:14 | Step:  1666(119/215) LR:0.000030 TrainLoss:0.379314 TrainAcc:0.985316\n",
      "13\n",
      "07-27 18:14:23 | Step:  1680(120/215) LR:0.000030 TrainLoss:0.380606 TrainAcc:0.984237\n",
      "13\n",
      "07-27 18:14:31 | Step:  1694(121/215) LR:0.000030 TrainLoss:0.379751 TrainAcc:0.984673\n",
      "13\n",
      "07-27 18:14:40 | Step:  1708(122/215) LR:0.000030 TrainLoss:0.379733 TrainAcc:0.984867\n",
      "13\n",
      "07-27 18:14:49 | Step:  1722(123/215) LR:0.000030 TrainLoss:0.379424 TrainAcc:0.984971\n",
      "13\n",
      "07-27 18:14:57 | Step:  1736(124/215) LR:0.000030 TrainLoss:0.380419 TrainAcc:0.984281\n",
      "13\n",
      "07-27 18:15:06 | Step:  1750(125/215) LR:0.000030 TrainLoss:0.379088 TrainAcc:0.985479\n",
      "13\n",
      "07-27 18:15:15 | Step:  1764(126/215) LR:0.000030 TrainLoss:0.379662 TrainAcc:0.984875\n",
      "13\n",
      "07-27 18:15:23 | Step:  1778(127/215) LR:0.000030 TrainLoss:0.379708 TrainAcc:0.984884\n",
      "13\n",
      "07-27 18:15:32 | Step:  1792(128/215) LR:0.000030 TrainLoss:0.379270 TrainAcc:0.985162\n",
      "13\n",
      "07-27 18:15:41 | Step:  1806(129/215) LR:0.000030 TrainLoss:0.379312 TrainAcc:0.985275\n",
      "13\n",
      "07-27 18:15:50 | Step:  1820(130/215) LR:0.000030 TrainLoss:0.379221 TrainAcc:0.985203\n",
      "13\n",
      "07-27 18:15:58 | Step:  1834(131/215) LR:0.000030 TrainLoss:0.379454 TrainAcc:0.984919\n",
      "13\n",
      "07-27 18:16:07 | Step:  1848(132/215) LR:0.000030 TrainLoss:0.378521 TrainAcc:0.985830\n",
      "13\n",
      "07-27 18:16:15 | Step:  1862(133/215) LR:0.000030 TrainLoss:0.378657 TrainAcc:0.985791\n",
      "13\n",
      "07-27 18:16:24 | Step:  1876(134/215) LR:0.000030 TrainLoss:0.379194 TrainAcc:0.985149\n",
      "13\n",
      "07-27 18:16:33 | Step:  1890(135/215) LR:0.000030 TrainLoss:0.379150 TrainAcc:0.985218\n",
      "13\n",
      "07-27 18:16:41 | Step:  1904(136/215) LR:0.000030 TrainLoss:0.379007 TrainAcc:0.985386\n",
      "13\n",
      "07-27 18:16:50 | Step:  1918(137/215) LR:0.000030 TrainLoss:0.378898 TrainAcc:0.985380\n",
      "13\n",
      "07-27 18:16:59 | Step:  1932(138/215) LR:0.000030 TrainLoss:0.378692 TrainAcc:0.985850\n",
      "13\n",
      "07-27 18:17:07 | Step:  1946(139/215) LR:0.000030 TrainLoss:0.378897 TrainAcc:0.985409\n",
      "13\n",
      "07-27 18:17:16 | Step:  1960(140/215) LR:0.000030 TrainLoss:0.379158 TrainAcc:0.985158\n",
      "13\n",
      "07-27 18:17:25 | Step:  1974(141/215) LR:0.000030 TrainLoss:0.378418 TrainAcc:0.985907\n",
      "13\n",
      "07-27 18:17:33 | Step:  1988(142/215) LR:0.000030 TrainLoss:0.378847 TrainAcc:0.985495\n",
      "13\n",
      "07-27 18:17:42 | Step:  2002(143/215) LR:0.000030 TrainLoss:0.379060 TrainAcc:0.985151\n",
      "13\n",
      "07-27 18:17:51 | Step:  2016(144/215) LR:0.000030 TrainLoss:0.379354 TrainAcc:0.985217\n",
      "13\n",
      "07-27 18:17:59 | Step:  2030(145/215) LR:0.000030 TrainLoss:0.378949 TrainAcc:0.985229\n",
      "13\n",
      "07-27 18:18:08 | Step:  2044(146/215) LR:0.000030 TrainLoss:0.378681 TrainAcc:0.985498\n",
      "13\n",
      "07-27 18:18:17 | Step:  2058(147/215) LR:0.000030 TrainLoss:0.378304 TrainAcc:0.986035\n",
      "13\n",
      "07-27 18:18:26 | Step:  2072(148/215) LR:0.000030 TrainLoss:0.378584 TrainAcc:0.985594\n",
      "13\n",
      "07-27 18:18:34 | Step:  2086(149/215) LR:0.000030 TrainLoss:0.379092 TrainAcc:0.985126\n",
      "13\n",
      "07-27 18:18:43 | Step:  2100(150/215) LR:0.000030 TrainLoss:0.378466 TrainAcc:0.985727\n",
      "13\n",
      "07-27 18:18:52 | Step:  2114(151/215) LR:0.000030 TrainLoss:0.378652 TrainAcc:0.985607\n",
      "13\n",
      "07-27 18:19:00 | Step:  2128(152/215) LR:0.000030 TrainLoss:0.377997 TrainAcc:0.986304\n",
      "13\n",
      "07-27 18:19:09 | Step:  2142(153/215) LR:0.000030 TrainLoss:0.378898 TrainAcc:0.985330\n",
      "13\n",
      "07-27 18:19:18 | Step:  2156(154/215) LR:0.000030 TrainLoss:0.380656 TrainAcc:0.983984\n",
      "13\n",
      "07-27 18:19:26 | Step:  2170(155/215) LR:0.000030 TrainLoss:0.378391 TrainAcc:0.985722\n",
      "13\n",
      "07-27 18:19:35 | Step:  2184(156/215) LR:0.000030 TrainLoss:0.378830 TrainAcc:0.985441\n",
      "13\n",
      "07-27 18:19:44 | Step:  2198(157/215) LR:0.000030 TrainLoss:0.379492 TrainAcc:0.984931\n",
      "13\n",
      "07-27 18:19:52 | Step:  2212(158/215) LR:0.000030 TrainLoss:0.378468 TrainAcc:0.985704\n",
      "13\n",
      "07-27 18:20:01 | Step:  2226(159/215) LR:0.000030 TrainLoss:0.378358 TrainAcc:0.985875\n",
      "13\n",
      "07-27 18:20:10 | Step:  2240(160/215) LR:0.000030 TrainLoss:0.378494 TrainAcc:0.985705\n",
      "13\n",
      "07-27 18:20:18 | Step:  2254(161/215) LR:0.000030 TrainLoss:0.378705 TrainAcc:0.985290\n",
      "13\n",
      "07-27 18:20:27 | Step:  2268(162/215) LR:0.000030 TrainLoss:0.378560 TrainAcc:0.985466\n",
      "13\n",
      "07-27 18:20:36 | Step:  2282(163/215) LR:0.000030 TrainLoss:0.378852 TrainAcc:0.985296\n",
      "13\n",
      "07-27 18:20:44 | Step:  2296(164/215) LR:0.000030 TrainLoss:0.378682 TrainAcc:0.985662\n",
      "13\n",
      "07-27 18:20:53 | Step:  2310(165/215) LR:0.000030 TrainLoss:0.378505 TrainAcc:0.985403\n",
      "13\n",
      "07-27 18:21:02 | Step:  2324(166/215) LR:0.000030 TrainLoss:0.379489 TrainAcc:0.985019\n",
      "13\n",
      "07-27 18:21:10 | Step:  2338(167/215) LR:0.000030 TrainLoss:0.378114 TrainAcc:0.986013\n",
      "13\n",
      "07-27 18:21:19 | Step:  2352(168/215) LR:0.000030 TrainLoss:0.378334 TrainAcc:0.985720\n",
      "13\n",
      "07-27 18:21:28 | Step:  2366(169/215) LR:0.000030 TrainLoss:0.378111 TrainAcc:0.985885\n",
      "13\n",
      "07-27 18:21:36 | Step:  2380(170/215) LR:0.000030 TrainLoss:0.378087 TrainAcc:0.985974\n",
      "13\n",
      "07-27 18:21:45 | Step:  2394(171/215) LR:0.000030 TrainLoss:0.378464 TrainAcc:0.985572\n",
      "13\n",
      "07-27 18:21:54 | Step:  2408(172/215) LR:0.000030 TrainLoss:0.378346 TrainAcc:0.985863\n",
      "13\n",
      "07-27 18:22:03 | Step:  2422(173/215) LR:0.000030 TrainLoss:0.378219 TrainAcc:0.985825\n",
      "13\n",
      "07-27 18:22:11 | Step:  2436(174/215) LR:0.000030 TrainLoss:0.377864 TrainAcc:0.986056\n",
      "13\n",
      "07-27 18:22:20 | Step:  2450(175/215) LR:0.000030 TrainLoss:0.378904 TrainAcc:0.985035\n",
      "13\n",
      "07-27 18:22:29 | Step:  2464(176/215) LR:0.000030 TrainLoss:0.378409 TrainAcc:0.985704\n",
      "13\n",
      "07-27 18:22:37 | Step:  2478(177/215) LR:0.000030 TrainLoss:0.378066 TrainAcc:0.985824\n",
      "13\n",
      "07-27 18:22:46 | Step:  2492(178/215) LR:0.000030 TrainLoss:0.378027 TrainAcc:0.985879\n",
      "13\n",
      "07-27 18:22:55 | Step:  2506(179/215) LR:0.000030 TrainLoss:0.377629 TrainAcc:0.986305\n",
      "13\n",
      "07-27 18:23:03 | Step:  2520(180/215) LR:0.000030 TrainLoss:0.378799 TrainAcc:0.985203\n",
      "13\n",
      "07-27 18:23:12 | Step:  2534(181/215) LR:0.000030 TrainLoss:0.378171 TrainAcc:0.985702\n",
      "13\n",
      "07-27 18:23:21 | Step:  2548(182/215) LR:0.000030 TrainLoss:0.378224 TrainAcc:0.985756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "07-27 18:23:30 | Step:  2562(183/215) LR:0.000030 TrainLoss:0.377897 TrainAcc:0.986012\n",
      "13\n",
      "07-27 18:23:38 | Step:  2576(184/215) LR:0.000030 TrainLoss:0.377937 TrainAcc:0.986031\n",
      "13\n",
      "07-27 18:23:47 | Step:  2590(185/215) LR:0.000030 TrainLoss:0.378347 TrainAcc:0.985452\n",
      "13\n",
      "07-27 18:23:56 | Step:  2604(186/215) LR:0.000030 TrainLoss:0.377605 TrainAcc:0.986409\n",
      "13\n",
      "07-27 18:24:04 | Step:  2618(187/215) LR:0.000030 TrainLoss:0.378180 TrainAcc:0.985594\n",
      "13\n",
      "07-27 18:24:13 | Step:  2632(188/215) LR:0.000030 TrainLoss:0.378662 TrainAcc:0.985345\n",
      "13\n",
      "07-27 18:24:22 | Step:  2646(189/215) LR:0.000030 TrainLoss:0.378212 TrainAcc:0.985534\n",
      "13\n",
      "07-27 18:24:30 | Step:  2660(190/215) LR:0.000030 TrainLoss:0.377815 TrainAcc:0.985933\n",
      "13\n",
      "07-27 18:24:39 | Step:  2674(191/215) LR:0.000030 TrainLoss:0.378072 TrainAcc:0.985749\n",
      "13\n",
      "07-27 18:24:48 | Step:  2688(192/215) LR:0.000030 TrainLoss:0.377873 TrainAcc:0.985834\n",
      "13\n",
      "07-27 18:24:56 | Step:  2702(193/215) LR:0.000030 TrainLoss:0.377792 TrainAcc:0.985973\n",
      "13\n",
      "07-27 18:25:05 | Step:  2716(194/215) LR:0.000030 TrainLoss:0.377850 TrainAcc:0.985963\n",
      "13\n",
      "07-27 18:25:14 | Step:  2730(195/215) LR:0.000030 TrainLoss:0.378048 TrainAcc:0.985594\n",
      "13\n",
      "07-27 18:25:23 | Step:  2744(196/215) LR:0.000030 TrainLoss:0.377279 TrainAcc:0.986637\n",
      "13\n",
      "07-27 18:25:31 | Step:  2758(197/215) LR:0.000030 TrainLoss:0.377230 TrainAcc:0.986718\n",
      "13\n",
      "07-27 18:25:40 | Step:  2772(198/215) LR:0.000030 TrainLoss:0.378118 TrainAcc:0.985572\n",
      "13\n",
      "07-27 18:25:49 | Step:  2786(199/215) LR:0.000030 TrainLoss:0.377404 TrainAcc:0.986487\n",
      "13\n",
      "07-27 18:25:57 | Step:  2800(200/215) LR:0.000030 TrainLoss:0.378476 TrainAcc:0.985550\n",
      "13\n",
      "07-27 18:26:06 | Step:  2814(201/215) LR:0.000030 TrainLoss:0.378202 TrainAcc:0.985507\n",
      "13\n",
      "07-27 18:26:15 | Step:  2828(202/215) LR:0.000030 TrainLoss:0.377528 TrainAcc:0.986205\n",
      "13\n",
      "07-27 18:26:23 | Step:  2842(203/215) LR:0.000030 TrainLoss:0.377744 TrainAcc:0.985945\n",
      "13\n",
      "07-27 18:26:32 | Step:  2856(204/215) LR:0.000030 TrainLoss:0.377454 TrainAcc:0.986367\n",
      "13\n",
      "07-27 18:26:41 | Step:  2870(205/215) LR:0.000030 TrainLoss:0.377625 TrainAcc:0.986001\n",
      "13\n",
      "07-27 18:26:49 | Step:  2884(206/215) LR:0.000030 TrainLoss:0.377906 TrainAcc:0.985710\n",
      "13\n",
      "07-27 18:26:58 | Step:  2898(207/215) LR:0.000030 TrainLoss:0.377370 TrainAcc:0.986304\n",
      "13\n",
      "07-27 18:27:07 | Step:  2912(208/215) LR:0.000030 TrainLoss:0.377714 TrainAcc:0.986034\n",
      "13\n",
      "07-27 18:27:15 | Step:  2926(209/215) LR:0.000030 TrainLoss:0.377884 TrainAcc:0.985752\n",
      "13\n",
      "07-27 18:27:24 | Step:  2940(210/215) LR:0.000030 TrainLoss:0.377183 TrainAcc:0.986497\n",
      "13\n",
      "07-27 18:27:33 | Step:  2954(211/215) LR:0.000030 TrainLoss:0.377732 TrainAcc:0.985871\n",
      "13\n",
      "07-27 18:27:42 | Step:  2968(212/215) LR:0.000030 TrainLoss:0.377489 TrainAcc:0.986107\n",
      "13\n",
      "07-27 18:27:50 | Step:  2982(213/215) LR:0.000030 TrainLoss:0.377753 TrainAcc:0.985835\n",
      "13\n",
      "07-27 18:27:59 | Step:  2996(214/215) LR:0.000030 TrainLoss:0.377441 TrainAcc:0.986116\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "show_freq =1\n",
    "learn_step =0\n",
    "learn_rate = 0.00003\n",
    "train_step = 3000\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "Img = np.load('../dataset_npy/IFS/IFS_imgS.npy')\n",
    "Lab = np.load('../dataset_npy/IFS/IFS_labelS.npy')\n",
    "freq1 = np.load('../dataset_npy/IFS/IFS_img_feat.npy')\n",
    "print('Img.shape=',Img.shape)\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'../model_s_gaijin4/modelS251875.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "    \n",
    "    tnb = next_batch(Lab.shape[0], batch_size)\n",
    "#     vnb = next_batch(vx.shape[0], batch_size)\n",
    "    total_epoch = math.ceil(train_step / (Lab.shape[0] // batch_size))\n",
    "    print('total_epoch=',total_epoch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        train_batches = int(show_freq * Lab.shape[0] // batch_size)\n",
    "#         print('train_batches=',train_batches)\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "#             print('\\ri: %d'%i)\n",
    "            sys.stdout.write('\\r' + str(i))\n",
    "            sys.stdout.flush()\n",
    "#             print('\\ri: %d'%i,  end= flush=True)\n",
    "            select = next(tnb)\n",
    "            \n",
    "            batch_x = Img[select]\n",
    "            batch_y = Lab[select]\n",
    "            batch_x1 = freq1[select]\n",
    "           \n",
    "            rev_batch_y=np.array(conv_mask_gt(batch_y))\n",
    "#             print('batch_y',rev_batch_y.max())\n",
    "#             print('batch_y',rev_batch_y.min())\n",
    "            batch_x=np.multiply(batch_x,1.0/mx)\n",
    "            sess.run(update, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_loss += sess.run(loss, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            train_accuracy += sess.run(accuracy, feed_dict={input_layer: batch_x, y: rev_batch_y, freqFeat: batch_x1})\n",
    "            learn_step +=1  \n",
    "        train_loss /= train_batches\n",
    "        epoch = math.ceil(learn_step / (Lab.shape[0] // batch_size)) \n",
    "        print()\n",
    "        if learn_step > train_step:\n",
    "                break\n",
    "        train_accuracy /= train_batches\n",
    "        print(datetime.datetime.now( ).strftime('%m-%d %H:%M:%S') + \n",
    "                  ' | Step:%6d(%3d/%3d) LR:%.6f TrainLoss:%.6f TrainAcc:%.6f' % (\n",
    "                      learn_step, epoch, total_epoch, learn_rate, train_loss, train_accuracy))\n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "    saver.save(sess, '../model_finetune_IFC/modelS.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T10:37:46.381066Z",
     "start_time": "2019-07-27T10:34:27.167325Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "#     saver.restore(sess,'../model_s_gaijin6/modelS.ckpt')\n",
    "    saver.restore(sess,'../model_finetune_IFC/modelS.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "            \n",
    "    TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "    #TP1=0;FP1=0\n",
    "    num_images=batch_size\n",
    "    \n",
    "    tx = np.load('../dataset_npy/IFS/IFS_imgS.npy')\n",
    "    tx= np.multiply(tx,1.0/mx)\n",
    "    ty = np.load('../dataset_npy/IFS/IFS_labelS.npy')\n",
    "    freq4 = np.load('../dataset_npy/IFS/IFS_img_feat.npy')\n",
    "    \n",
    "    print('ty.shape:',ty.shape)\n",
    "    nTx=np.zeros((batch_size,256,256,3))\n",
    "    nTy=np.zeros((batch_size,256,256))\n",
    "#     nTx1=np.zeros((batch_size,64,240))\n",
    "    nTx1=np.zeros((batch_size,248,248,3))\n",
    "    \n",
    "    n_chunks=np.shape(tx)[0]//batch_size\n",
    "    tAcc=np.zeros(n_chunks)\n",
    "    \n",
    "    n1=0;n2=len(tx)\n",
    "    pred = np.zeros([441,256,256])\n",
    "    prob = np.zeros([256*256*441,2])\n",
    "    for chunk in range(n1,n_chunks):\n",
    "#         nTx[imNb-n1]=tx[imNb] \n",
    "#         nTy[imNb-n1]=ty[imNb]\n",
    "#         nTx1[imNb-n1]=freq4[imNb]\n",
    "        print('chunk=',chunk)\n",
    "        nTx=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "        \n",
    "        nTy=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "#         print(nTy.shape)\n",
    "        nTx1=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "       \n",
    "        \n",
    "#         print ('nTx = ',np.shape(nTx))\n",
    "#         print ('nTy = ',np.shape(nTy))\n",
    "#         print ('nTx1 = ',np.shape(nTx1))\n",
    "        ty_prime=np.array(conv_mask_gt(nTy))\n",
    "            \n",
    "        final_predictions, final_probabilities,y2=sess.run([mask_pred,probabilities,mask_actual], feed_dict={input_layer: nTx, y:ty_prime, freqFeat: nTx1})\n",
    "    \n",
    "        print ('final_predictions_shape:',np.shape(final_predictions))\n",
    "        print ('final_probabilities_shape;',np.shape(final_probabilities))\n",
    "        pred[((chunk)*num_images):((chunk+1)*num_images),...] =  final_predictions\n",
    "        prob[((chunk)*num_images)*256*256:((chunk+1)*num_images)*256*256,...] = final_probabilities\n",
    "        \n",
    "        print ('pred_shape:',np.shape(pred))\n",
    "        print ('prob_shape:',np.shape(prob))\n",
    "    print('-----------------------------------------------------------')\n",
    "    print ('pred_shape:',np.shape(pred))\n",
    "    print ('prob_shape:',np.shape(prob))\n",
    "    final_predictions1 =pred \n",
    "    final_probabilities1 =prob\n",
    "    print ('final_predictions1_shape:',np.shape(final_predictions1))\n",
    "    print ('final_probabilities1_shape;',np.shape(final_probabilities1))\n",
    "    \n",
    "    #sio.savemat('pred_res.mat',{'img':nTx,'labels':nTy,'pred':final_predictions,'prob':final_probabilities,'gT':y2})\n",
    "    nb = 0\n",
    "    F1_sum =0\n",
    "#     for i in range(n1,n2):\n",
    "    for i in range(n1,441):\n",
    "        print('i=',i)\n",
    "        cmap = plt.get_cmap('bwr')\n",
    "        f,(ax,ax1,ax2)=plt.subplots(1,3,sharey=True)\n",
    "#         f,(ax,ax1,ax2,ax3)=plt.subplots(1,4,sharey=True)\n",
    "        ax.imshow(tx[i])\n",
    "        ax1.imshow(ty[i])\n",
    "        ax2.imshow(final_predictions1[i])\n",
    "        plt.imsave(fname='/home/shizenan/IFS_result/tx_%s.png'%i,\n",
    "                       arr=tx[i], cmap=plt.cm.gray)\n",
    "        plt.imsave(fname='/home/shizenan/IFS_result/ty_%s.png'%i,\n",
    "                       arr=ty[i], cmap=plt.cm.gray)\n",
    "        plt.imsave(fname='/home/shizenan/IFS_result/final_predictions_%s.png'%i,\n",
    "                       arr=final_predictions1[i], cmap=plt.cm.gray)\n",
    "        F1 = F1_score(final_predictions1[i],ty[i])\n",
    "        \n",
    "        \n",
    "        #ax3.set_title('Final Argmax')\n",
    "#         probability_graph = ax3.imshow(final_probabilities.squeeze()[nb, :,:, 0])\n",
    "        nb += 1\n",
    "        #ax3.set_title('Final Probability of the Class')\n",
    "#         plt.colorbar(probability_graph)\n",
    "        plt.show()\n",
    "        print('F1:,i',F1,i)\n",
    "        F1_sum += F1\n",
    "    avg_F1 = F1_sum/(n2-n1)\n",
    "    print('F1_sum:',F1_sum)\n",
    "    print('avg_F1:',avg_F1)\n",
    "    print('ty.shape:',ty.shape)\n",
    "    print('final_predictions1.shape:',final_predictions1.shape)\n",
    "    print('final_probabilities1.shape:',final_probabilities1.shape)\n",
    "    print('-----------------------------------------------------------')\n",
    "    \n",
    "    prob_new1=np.zeros(len(final_probabilities1))\n",
    "# prob_new1 = prob_new[:,0]\n",
    "\n",
    "# prob_new=np.zeros(len(final_probabilities1))\n",
    "#     for i in range(0,150):\n",
    "#         if final_probabilities1[i,0]>= final_probabilities1[i,1]:\n",
    "#             prob_new[i]= final_probabilities1[i,0]\n",
    "#         if final_probabilities1[i,0]<= final_probabilities1[i,1]:\n",
    "#               prob_new[i]= final_probabilities1[i,1]\n",
    "    ty = ty[0:150,]\n",
    "    ty= ty.flatten()\n",
    "    for i in range(0,len(final_probabilities1)):\n",
    "        if ty[i]==0.:\n",
    "            prob_new1[i]= final_probabilities1[i,1]\n",
    "        if ty[i]==1.:\n",
    "            prob_new1[i]= final_probabilities1[i,0]\n",
    "            \n",
    "    print(prob_new1.shape)\n",
    "    print(prob_new1.shape)\n",
    "\n",
    "\n",
    "    ccc= check_fit(ty, prob_new1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T02:48:52.163820Z",
     "start_time": "2019-07-05T02:48:21.458339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model_s_gaijin6/modelS.ckpt\n",
      "session starting .................!!!!\n",
      "(158, 256, 256)\n",
      "4 | TP：560038 FP:43299 TN:9034868 FN:192195----prec:0.928234-----Tacc:0.976044\n",
      "Best Model Found on NC16...\n",
      "prec = 0.9282341378034286, acc = 0.9760443091392517\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'../model_s_gaijin6/modelS.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "            \n",
    "    TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "    #TP1=0;FP1=0\n",
    "    num_images=batch_size\n",
    "    \n",
    "    tx = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_img.npy')\n",
    "    ty = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_label.npy')\n",
    "    freq4 = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_imgS_feat.npy')\n",
    "    print(ty.shape)\n",
    "    \n",
    "    \n",
    "    n_chunks=np.shape(tx)[0]//batch_size\n",
    "    tAcc=np.zeros(n_chunks)\n",
    "    vali_loss = 0\n",
    "    for chunk in range(0,n_chunks):\n",
    "        \n",
    "        sys.stdout.write('\\r' + str(chunk))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tx_batch=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "        tx_batch=np.multiply(tx_batch,1.0/mx)\n",
    "        ty_batch=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "        tx1_batch=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "        ty_batch=conv_mask_gt(ty_batch)\n",
    "\n",
    "        vali_loss += sess.run(loss, feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "\n",
    "\n",
    "        tAcc[chunk],y2,p2=sess.run([accuracy,y_actual,y_pred], feed_dict={input_layer: tx_batch, y:ty_batch, freqFeat: tx1_batch})\n",
    "        a,b,c,d=compute_pos_neg(y2,p2)\n",
    "\n",
    "        TP+=a; FP+=b;TN+=c; FN+=d\n",
    "    vali_loss /= n_chunks\n",
    "    prec=metrics(TP,FP,TN,FN)            \n",
    "    test_accuracy=np.mean(tAcc)\n",
    "\n",
    "    print(' | TP：%d FP:%d TN:%d FN:%d----prec:%.6f-----Tacc:%.6f' % (TP, FP, TN, FN, prec, test_accuracy))\n",
    "\n",
    "    if prec > 0.70 :\n",
    "        best_prec = prec\n",
    "        print (\"Best Model Found on NC16...\")\n",
    "        print ( \"prec = \"+str(prec) + \", acc = \"+ str(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test F1  NC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T05:22:47.743635Z",
     "start_time": "2019-07-06T05:21:08.638554Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess,'../model_s_gaijin6/modelS.ckpt')\n",
    "    print ('session starting .................!!!!')\n",
    "            \n",
    "    TP = 0; FP = 0;TN = 0; FN = 0 \n",
    "    #TP1=0;FP1=0\n",
    "    num_images=batch_size\n",
    "    \n",
    "#     tx = np.load('../dataset_npy/NC_16/NC16_mani_imgS.npy')\n",
    "#     tx= np.multiply(tx,1.0/mx)\n",
    "#     ty = np.load('../dataset_npy/NC_16/NC16_mani_labelS.npy')\n",
    "#     freq4 = np.load('../dataset_npy/NC_16/NC16_mani_imgS_feat.npy')\n",
    "    tx = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_img.npy')\n",
    "    tx= np.multiply(tx,1.0/mx)\n",
    "    ty = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_label.npy')\n",
    "    freq4 = np.load('../dataset_npy/NC_16/mani/NC16_mani_test_imgS_feat.npy')\n",
    "    \n",
    "    \n",
    "    print('ty.shape:',ty.shape)\n",
    "    nTx=np.zeros((batch_size,256,256,3))\n",
    "    nTy=np.zeros((batch_size,256,256))\n",
    "#     nTx1=np.zeros((batch_size,64,240))\n",
    "    nTx1=np.zeros((batch_size,248,248,3))\n",
    "    \n",
    "    n_chunks=np.shape(tx)[0]//batch_size\n",
    "    tAcc=np.zeros(n_chunks)\n",
    "    \n",
    "    n1=0;n2=len(tx)\n",
    "    pred = np.zeros([150,256,256])\n",
    "    prob = np.zeros([256*256*150,2])\n",
    "    for chunk in range(n1,n_chunks):\n",
    "#         nTx[imNb-n1]=tx[imNb] \n",
    "#         nTy[imNb-n1]=ty[imNb]\n",
    "#         nTx1[imNb-n1]=freq4[imNb]\n",
    "        print('chunk=',chunk)\n",
    "        nTx=tx[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "        \n",
    "        nTy=ty[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "#         print(nTy.shape)\n",
    "        nTx1=freq4[((chunk)*num_images):((chunk+1)*num_images),...]\n",
    "       \n",
    "        \n",
    "#         print ('nTx = ',np.shape(nTx))\n",
    "#         print ('nTy = ',np.shape(nTy))\n",
    "#         print ('nTx1 = ',np.shape(nTx1))\n",
    "        ty_prime=conv_mask_gt(nTy)\n",
    "        final_predictions, final_probabilities,y2=sess.run([mask_pred,probabilities,mask_actual], feed_dict={input_layer: nTx, y:ty_prime, freqFeat: nTx1})\n",
    "    \n",
    "        print ('final_predictions_shape:',np.shape(final_predictions))\n",
    "        print ('final_probabilities_shape;',np.shape(final_probabilities))\n",
    "        pred[((chunk)*num_images):((chunk+1)*num_images),...] =  final_predictions\n",
    "        prob[((chunk)*num_images)*256*256:((chunk+1)*num_images)*256*256,...] = final_probabilities\n",
    "        \n",
    "        print ('pred_shape:',np.shape(pred))\n",
    "        print ('prob_shape:',np.shape(prob))\n",
    "    print('-----------------------------------------------------------')\n",
    "    print ('pred_shape:',np.shape(pred))\n",
    "    print ('prob_shape:',np.shape(prob))\n",
    "    final_predictions1 =pred \n",
    "    final_probabilities1 =prob\n",
    "    print ('final_predictions1_shape:',np.shape(final_predictions1))\n",
    "    print ('final_probabilities1_shape;',np.shape(final_probabilities1))\n",
    "    \n",
    "    #sio.savemat('pred_res.mat',{'img':nTx,'labels':nTy,'pred':final_predictions,'prob':final_probabilities,'gT':y2})\n",
    "    nb = 0\n",
    "    F1_sum =0\n",
    "#     for i in range(n1,n2):\n",
    "    for i in range(n1,150):\n",
    "        print('i=',i)\n",
    "        cmap = plt.get_cmap('bwr')\n",
    "        f,(ax,ax1,ax2)=plt.subplots(1,3,sharey=True)\n",
    "#         f,(ax,ax1,ax2,ax3)=plt.subplots(1,4,sharey=True)\n",
    "        ax.imshow(tx[i])\n",
    "        ax1.imshow(ty[i])\n",
    "        ax2.imshow(final_predictions1[i])\n",
    "        plt.imsave(fname='/home/shizenan/shi_tmp/tx_%s.png'%i,\n",
    "                       arr=tx[i], cmap=plt.cm.gray)\n",
    "        plt.imsave(fname='/home/shizenan/shi_tmp/ty_%s.png'%i,\n",
    "                       arr=ty[i], cmap=plt.cm.gray)\n",
    "        plt.imsave(fname='/home/shizenan/shi_tmp/final_predictions_%s.png'%i,\n",
    "                       arr=final_predictions1[i], cmap=plt.cm.gray)\n",
    "        F1 = F1_score(final_predictions1[i],ty[i])\n",
    "        \n",
    "        \n",
    "        #ax3.set_title('Final Argmax')\n",
    "#         probability_graph = ax3.imshow(final_probabilities.squeeze()[nb, :,:, 0])\n",
    "        nb += 1\n",
    "        #ax3.set_title('Final Probability of the Class')\n",
    "#         plt.colorbar(probability_graph)\n",
    "        plt.show()\n",
    "        print('F1:,i',F1,i)\n",
    "        F1_sum += F1\n",
    "    avg_F1 = F1_sum/(n2-n1)\n",
    "    print('F1_sum:',F1_sum)\n",
    "    print('avg_F1:',avg_F1)\n",
    "    print('ty.shape:',ty.shape)\n",
    "    print('final_predictions1.shape:',final_predictions1.shape)\n",
    "    print('final_probabilities1.shape:',final_probabilities1.shape)\n",
    "    print('-----------------------------------------------------------')\n",
    "    \n",
    "    prob_new1=np.zeros(len(final_probabilities1))\n",
    "# prob_new1 = prob_new[:,0]\n",
    "\n",
    "# prob_new=np.zeros(len(final_probabilities1))\n",
    "#     for i in range(0,150):\n",
    "#         if final_probabilities1[i,0]>= final_probabilities1[i,1]:\n",
    "#             prob_new[i]= final_probabilities1[i,0]\n",
    "#         if final_probabilities1[i,0]<= final_probabilities1[i,1]:\n",
    "#               prob_new[i]= final_probabilities1[i,1]\n",
    "    ty = ty[0:150,]\n",
    "    ty= ty.flatten()\n",
    "    for i in range(0,len(final_probabilities1)):\n",
    "        if ty[i]==0.:\n",
    "            prob_new1[i]= final_probabilities1[i,1]\n",
    "        if ty[i]==1.:\n",
    "            prob_new1[i]= final_probabilities1[i,0]\n",
    "            \n",
    "    print(prob_new1.shape)\n",
    "    print(prob_new1.shape)\n",
    "\n",
    "\n",
    "    ccc= check_fit(ty, prob_new1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "477.455px",
    "left": "46px",
    "top": "130.364px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
